\documentclass[12pt,a4paper]{article}

\input{preamble.tex}

\title{Better-Than-Chance Classification for Signal Detection}


\begin{document}
%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?


\maketitle
%\linenumbers

\begin{abstract}
We show that using a classifier's accuracy as a test statistic, is an underpowered strategy for the purpose of finding a difference between populations, compared to a bona-fide statistical test.
It is also more complicated to implement. 
For the cases that the purposes of the analysis is not the mere existence of a difference between populations, but rather the performance of a particular classifier, we suggest several improvements to increase power. 
\end{abstract}


%%%% Introduction %%%
\section{Introduction}
\label{sec:introduction}

A common workflow in neuroimaging and genetics consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Examples in the neuroscientific literature include \citet{golland_permutation_2003,pereira_machine_2009,varoquaux_assessing_2016}, and especially the recently popularized \emph{multivariate pattern analysis} (MVPA) framework of \citet{kriegeskorte_information-based_2006}.
For examples in the genetics literature see for example
\citet{golub_molecular_1999,slonim_class_2000,radmacher_paradigm_2002,mukherjee_estimating_2003,juan_prediction_2004,jiang_calculating_2008}.


To fix ideas, we will adhere to a concrete example.
In \cite{gilron_quantifying_2016}, the authors seek to detect brain regions which encode differences between vocal and non-vocal stimuli. 
Following the MVPA workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the brain's activation pattern significantly better than chance, then a region is declared to encode vocal/non-vocal information. 
We call this an \emph{accuracy test}, because it uses the prediction accuracy as a test statistic. 

This same signal detection task can be also approached as a multivariate test.
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
As put in \cite{pereira_machine_2009}: 
\begin{quote}
... the problem of deciding whether the classifier learned to discriminate the classes can be subsumed into the more general question as to whether there is evidence that the underlying distributions of each class are equal or not.
\end{quote}
A practitioner may thus approach the signal detection problem with a two-group location test such as Hotelling's $T^2$ \citep{anderson_introduction_2003}.
Alternatively, if the size of the brain's region of interest is large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of Hotelling's test can be called upon.
Examples of high dimensional multivariate tests include \cite{schafer_shrinkage_2005}, \cite{goeman2006testing}, or \cite{srivastava_multivariate_2007} .
For brevity, and in contrast to \emph{accuracy tests}, we will call these \emph{location tests}, because they test for the equality of location of a multivariate distribution. 


At this point, it becomes unclear which is preferable: a location test or an accuracy test?
The former with a heritage dating back to \cite{hotelling_generalization_1931}, and the latter being extremely popular, as the $1,170$ citations\footnote{GoogleScholar. Accessed Aug 2017.} of \cite{kriegeskorte_information-based_2006} suggest. 

The comparison between location and accuracy tests was precisely the goal of \cite{ramdas_classification_2016}, who compared Hotelling's $T^2$ location test to \emph{Fisher's linear discriminant analysis} (LDA) accuracy test. 
By comparing the rates of convergence of the power of each statistic to $1$, \cite{ramdas_classification_2016} concluded that accuracy and location tests are rate equivalent. 
Rates, however, are only a first stage when comparing test statistics. 

Asymptotic relative efficiency measures (ARE) are typically used by statisticians to compare between rate-equivalent test statistics \citep{vaart_asymptotic_1998}.
ARE is the limiting ratio of the samples sizes required by two statistics to achieve similar power. 
\cite{ramdas_classification_2016} derive the asymptotic power functions of the two test statistics, which allows to compute the ARE between Hotelling's $T^2$ (location) test and Fisher's LDA (accuracy) test.
Theorem~14.7 of \cite{vaart_asymptotic_1998} relates asymptotic power functions to ARE.
Using this theorem and the results of \cite{ramdas_classification_2016} we deduce that the ARE is lower bounded by $2 \pi \approx 6.3$. 
This means that Fisher's LDA requires at least $6.3$ more samples to achieve the same (asymptotic) power as the $T^2$ test. 
In this light, the accuracy test is remarkably inefficient compared to the location test.  
For comparison, the t-test is only $1.04$ more (asymptotically) efficient than Wilcoxon's rank-sum test \citep{lehmann_parametric_2009}, so that an ARE of $6.3$ is strong evidence in favor of the location test. 

Before discarding accuracy tests as inefficient, we recall that \cite{ramdas_classification_2016} analyzed a \emph{half-sample} holdout. 
The authors conjectured that a leave-one-out approach, which makes more efficient use of the data, may have better performance. 
Also, the analysis in \cite{ramdas_classification_2016} is asymptotic. 
This eschews the discrete nature of the accuracy statistic, which we will show to have  crucial impact. 
Since typical sample sizes in neuroscience are not large, we seek to study which test is to be preferred in finite samples, and not only asymptotically.
Our conclusion will be quite simple: {\em location tests typically have more power than accuracy tests, and are easier to implement.}

Our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Permutation testing with discrete test statistics are known to be conservative \citep{hemerik_exact_2014}, since they are insensitive to mild perturbations of the data, and they cannot exhaust the permissible false positive rate. 
As put by Prof. Frank Harrell in \textsf{CrossValidated\footnote{A Q\&A website for statistical questions: \url{http://stats.stackexchange.com/questions/17408/how-to-assess-statistical-significance-of-the-accuracy-of-a-classifier}}} post back in $2011$:
\begin{quote}
	... your use of proportion classified correctly as your accuracy score. This is a discontinuous improper scoring rule that can be easily manipulated because it is arbitrary and insensitive.
\end{quote}


The degree of discretization is governed by the number of samples. 
In our example from \citet{gilron_quantifying_2016}, the classification accuracy is computed using $40$ examples, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of trial-repeats, or the number of subjects, in an neuroimaging study. 

The discretization effect is aggravated if the test statistic is highly concentrated. 
For an intuition consider the usage of the \emph{resubstitution accuracy} as a test statistic. 
This statistic simply means that the accuracy is not cross validated, but rather evaluated on the training data.
If the data is high dimensional, the resubstitution accuracy will be very high due to over fitting. 
In a very high dimensional regime, the resubstitution accuracy will be $1$ for the observed data \cite[Theorem 1]{mclachlan_bias_1976}, but also for any permutation.
The concentration of resubstitution accuracy near $1$, and its discreteness, render this test completely useless, with power tending to $0$ for any (fixed) effect size, as the dimension of the model grows. 


To compare the power of accuracy tests and location tests in finite samples, we study a battery of test statistics by means of simulation. 
We start with formalizing the problem in Section~\ref{sec:problem_setup}.
The main findings are reported in Sections~\ref{sec:results}, and \ref{sec:example}.
A discussion follows. 



%%%% Section %%%%
\section{Problem setup}
\label{sec:problem_setup}


\subsection{Multivariate Testing}

Let $y \in \mathcal{Y}$ be a class encoding. 
Let $x \in \mathcal{X}$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example we have $\mathcal{Y}=\set{0,1}$ and $p$, the number of voxels in a brain region so that $\mathcal{X}=\reals^{27}$. 

Denoting a dataset by $\data:=\{(x_i,y_i)\}_{i=1}^n$, a multivariate test amounts to testing whether the distribution of $x$ given $y=1$ is the same as $x$ given $y=0$. 
I.e., we test if the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 
The comparison metric between statistics is their power, i.e., the probability to infer that $x|y=1$ is not distributed like $x|y=0$.


\subsection{Location Tests and Hotelling's $T^2$}
The most prevalent interpretation of ``$x|y=1$ is not distributed like $x|y=0$'' is to assume they differ in means. 
In his seminal work, \citet{hotelling_generalization_1931} has proposed the $T^2$ test statistic for testing the equality in means of two multivariate distributions. 
Using our notations this statistic is proportional to the difference between group means, measured with the Mahalanobis norm: 
\begin{align}
	T^2 \propto \; (\bar{x}_{y=1}-\bar{x}_{y=0})'\; \hat{\Sigma}^{-1} \;(\bar{x}_{y=1}-\bar{x}_{y=0}), 
\end{align}
where $\bar{x}_{y=j}$ is the $p$-vector of means in the $y=j$ group, and $\hat{\Sigma}$ is a pooled covariance estimator.
Perhaps more intuitively, it can be shown that $T^2$ equals the Euclidean norm of the mean difference vector, after decorrelating the coordinates of $x$. 
For more background see, for example, \cite{anderson_introduction_2003}.

The major difficulty with these multivariate tests is that $\Sigma$ has $p(p+1)/2$ free parameters, so that $n$ has to be very large to apply these tests.
If $n$ is not much larger than $p$, or in low signal-to-noise regimes, the test is very low powered, as argued by \cite{bai1996effect}. 
In these cases, high dimensional versions of the $T^2$ may be applied, which essentially regularize the estimator of $\Sigma$, thus reducing the dimensionality of the problem and improving the SNR.



\subsection{Prediction Accuracy as a Test Statistic}
An accuracy test amounts to setting a test statistic to be some measure prediction accuracy.  

A predictor\footnote{Known as a \emph{hypothesis} in the machine learning literature.}, $\hyp:\mathcal{X} \to \mathcal{Y}$, is the output of a learning algorithm $\algo$ when applied to the dataset $\data$. 
The accuracy of predictor\footnote{Known as (the complement of) the \emph{test error} in \cite{hastie_elements_2003}}, $\acc_{\hyp}$, is defined as the probability of $\hyp$ making a correct prediction. 
The accuracy of an algorithm\footnote{Known as (the complement of) the \emph{expected test error} in \cite{hastie_elements_2003}}, $\acc_{\algo}$, is defined as the expected accuracy over all possible data sets $\data$. 
Formalizing, we denote by $\measure$ the probability measure of $(x, y)$, and by $\measuren$ the joint probability measure of the sample $\data$. 
We can then write 
\begin{align}
	\acc_{\hyp}:=\int_{(x,y)} \indicator{\hyp(x)=y} \; d\measure(x,y),
\end{align}
and
\begin{align}
	\acc_{\algo}:=\int_\data \acc_{\algo_\data} \; d\measuren,
\end{align}
where $\indicator{A}$ is the indicator function of the set $A$. 

Denoting an estimate of $\acc_{\hyp}$ by $\accEstim_{\hyp}$, and $\acc_{\algo}$ by $\accEstim_{\algo}$, a statistically significant ``better than chance'' estimate of either, is evidence that the classes are distinct. 

Two popular estimates of $\accEstim_{\algo}$ are the \emph{resubstitution estimate}\footnote{Known as the \emph{train error} in \cite{hastie_elements_2003}.}, and the V-fold Cross Validation (CV) estimate.
\begin{definition}[Resubstitution estimate]
\label{def:resubstitution}
The resubstitution accuracy estimator of a learning algorithm $\algo$, denoted $\accEstim_{\algo}^{Resub}$,  is defined as
\begin{align}
	\accEstim_{\algo}^{Resub} := \frac 1n \sum_{i=1}^{n} \indicator{\hypFun{\data}{x_i}=y_i}.
\end{align}
\end{definition}


\begin{definition}[V-fold CV estimate]
\label{def:v-fold}
Denoting by $\data^{v}$ the $v$'th partition, or \emph{fold}, of the dataset, and by $\data^{(v)}$ its complement, so that $\data^{v} \union \data^{(v)}=\union_{v=1}^V \data^{v}=\data$, the V-fold CV accuracy estimator, denoted $\accEstim_{\algo}^{Vfold}$, is defined as 	
\begin{align}
	\accEstim_{\algo}^{Vfold} := 
	\frac 1V \sum_{v=1}^{V} \frac{1}{|\data^v|} \sum_{i \in \data^{v}} \indicator{\hypFun{\data^{(v)}}{x_i}=y_i},
\end{align}
where $|A|$ denotes the cardinality of a set $A$.
\end{definition}




\subsection{How to Estimate Accuracies?}
\label{sec:considerations}

Estimating $\accEstim_{\algo}$ requires the following design choices: 
(a) Should it be resampled and how? 
(b) If resampling using V-fold cross validation then How many folds? Should the data be refolded after each permutation? Should the folding be balanced? 

We will now address these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of $\acc_{\algo}$, but rather in the detection of its departure from chance level. 

\paragraph{Cross validate or not?}
Given our goal, a biased estimate of $\accEstim_{\algo}$ is not a problem provided that bias is consistent over all permutations. 
The underlying intuition is that a permutation test will be unbiased, provided that the exact same computation is performed over all permutations. 
We will thus be considering both cross validated accuracies, and resubstitution accuracies.


\paragraph{Balanced folding?}
The standard practice in V-fold CV is to constrain the data folds to be balanced, i.e. stratified \citep[e.g.][]{ojala_permutation_2010}.
This means that each fold has the same number of examples from each class. 
We will report results with both balanced and unbalanced data foldings. 


\paragraph{Refolding?}
The standard practice in neuroimaging is to permute labels and refold the data after each permutation, so that the balance of the classes in each fold is preserved.
We will adhere to this practice due to its popularity, even though it is more computaionally efficient to permute features instead of labels, as done by \citet{golland_permutation_2005}.


\paragraph{How many folds?}
Different authors suggest different rules for the number of folds. 
We will look into the effect of the number of folds. 

\bigskip

Table~\ref{tab:collected} collects an initial battery of tests we will be comparing. 
\begin{tcolorbox}
\centering
\begin{tabular}{l|c|c|c}
Name & Algorithm & Resampling & Parameters\\ 
\hline
\hline
Hotelling & Hotelling & Resubstitution & -- \\ 
Oracle & Hotelling & Resubstitution & -- \\ 
Hotelling.shrink & Hotelling & Resubstitution & -- \\ 
SD & Hotelling & Resubstitution & -- \\ 
LDA.CV.1 	& LDA & V-fold 			&  -- \\ 
LDA.noCV.1 	& LDA & Resubstitution 	&  --\\ 
SVM.CV.1 	& SVM & V-fold 		    & cost=$10$ \\ 
SVM.CV.2 	& SVM & V-fold 		    & cost=$0.1$ \\ 
SVM.noCV.1 	& SVM & Resubstitution  & cost=$10$ \\ 
SVM.noCV.2 	& SVM & Resubstitution  & cost=$0.1$ \\ 
\end{tabular} 
\captionsetup{type=table}
\caption{\footnotesize
This table collects the various test statistics we will be studying. 
Location tests include: \textit{Oracle}, \textit{Hotelling}, \textit{Hotelling.shrink}, and \textit{SD}.
\textit{Hotelling} is the classical two-group $T^2$ statistic \citep{anderson_introduction_2003}. 
\textit{Oracle} is the same as Hotelling's $T^2$, only using the generative covariance, and not an estimated one.
\textit{Hotelling.shrink} is a high dimensional version of $T^2$, with the regularized covariance from \citet{schafer_shrinkage_2005}. 
\textit{SD} is another high dimensional version of the $T^2$, from \citet{srivastava_two_2013}.
The rest of the tests are accuracy tests, with details given in the table. 
For example, \textit{SVM.CV.2} is a linear SVM, with V-fold cross validated accuracy, and cost parameter set at $0.1$ \citep{meyer_e1071:_2015}.
Another example is \textit{LDA.noCV.1}, which is Fisher's LDA, with a resubstituted accuracy estimate.}
\label{tab:collected}
\end{tcolorbox}


\subsection{From a Test Statistic to a Permutation Test}

The various test statistics in Table~\ref{tab:collected} will be compared using their power. 
Because our problems of interest are typically high-dimensional, i.e. $n \gg p$ does not hold, them central limit laws will not apply and we recur to permutation tests.
Because we focus on two-group testing under an independent sampling assumption, we know that a label-switching permutation test is valid in that it has a false positive rate no larger than the desired level. 

\noindent
The sketch of our permutation test is the following:\newline
(a) Fix a test statistic.\newline
(b) Permute labels and recompute the statistic to recover its permutation distribution. \newline
(c) Declare classes differ if the observed $\statistic$ is beyond its $95\%$'th permutation percentile.  \newline





%%%% Section %%%%
\section{Results}
\label{sec:results}
We now compare the power of our various statistics in various configurations. 
We do so via simulation, which details are now described.
We start by presenting the most basic simulation setup, and then its modifications. 


\subsection{Basic Simulation Setup}
\label{sec:simulation_details}

The following details are common to all the reported simulations, unless stated otherwise in a figure's caption. 
The \R code for the simulations can be found in \url{http://www.john-ros.com/permuting_accuracy/}.

Each simulation is based on $1,000$ replications. 
In each replication, we generate $n$ i.i.d. samples from a shift class 
\begin{align}
\label{eq:distribution}
	\x_i = \mu \y_i + \eta_i,
\end{align}
where $\y_i \in \mathcal{Y}=\set{0,1}$ encodes the class of subject $i$, the noise is distributed as $\eta_i \sim \gaussp{p}{0,\Sigma}$, the sample size $n=40$, and the dimension of the data is $p=23$. 
The covariance $\Sigma=I$. 
In this basic setup, effects are of shift type and captured by $\mu$. 
Shifts are equal in all $p$ coordinates of $\mu$, with magnitude varying over $\set{0,1/4,1/2}$.

Having generated the data, we compute each of the test statistics in Table~\ref{tab:collected}.
For test statistics that require data folding, we used $4$ folds. 
We then compute a permutation p-value by permuting the class labels, and recomputing each test statistic. 
We perform $300$ such permutations. 
We then reject the $\mu_i=0$ null hypothesis against a two sided alternative if the permutation p-value is smaller than $0.05$.
The reported power is the proportion of replication where the permutation p-value fell below $0.05$.



\subsection{False Positive Rate}
\label{sec:type_i}

We start with a sanity check. 
Theory suggests that all test statistics should control their false positive rate. 
Our simulations confirm this.
In all our results, such as Figure~\ref{fig:simulation_1}, we encode the null case, where no signal is present and $x|y=1$ has the same distribution as $x|y=0$, by a red circle. 
Since the red circles are always below the desired $0.05$ error rate, then the false positive rate of all test statistics, in all simulations is controlled. 
We may thus proceed and compare the power of each test statistic. 






%%%% Section %%%%
\subsection{Power}
\label{sec:power}

Having established that all of the tests in our battery control the false positive rate, it remains to be seen if they have similar power-- especially when comparing location tests to accuracy tests. 

From Figure~\ref{fig:simulation_1} we learn that location tests are more powerful than accuracy tests.
This is particularly visible for intermediate signal strength (green triangle), and location tests \emph{SD} and \emph{Hotelling.shrink} defined in Table~\ref{tab:collected}.



\begin{figure}[h]
	\centering
	\caption{
		The power of the permutation test with various test statistics. 
		The power on the $x$ axis. 
		Effects are color and shape coded. 
		The various statistics on the $y$ axis. 
		Their details are given in Table~\ref{tab:collected}. 
		Effects vary over $\mu=(0,\dots,0)$ (red circle), $\mu=(0.25,\dots,0.25)$ (green triangle), and $\mu=(0.5,\dots,0.5)$ (blue square). 
		Simulation details in Section~\ref{sec:simulation_details}.
		Cross-validation was performed with balanced and unbalanced data folding. See sub-captions.}	
	\label{fig:simulation_1}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{"art/file3"}
		\caption{\textbf{Unbalanced.}} 
		\label{fig:simulation_11}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{"art/file2"}
		\caption{\textbf{Balanced.}} 
		\label{fig:simulation_12}
	\end{subfigure}
\end{figure}






\subsection{Departure From Gaussianity}
The Neyman-Pearson Lemma (NPL) type reasoning that favors the location test over accuracy tests may fail when the data is not multivariate Gaussian, and Hotelling's $T^2$ statistic no longer a generalized-likelihood-ratio test. 

To check this, we replaced the multivariate Gaussian distribution of $\eta$ in Eq.(\ref{eq:distribution}) with a heavy-tailed multivariate-$t$ distribution. 
The dominance of the location tests was preserved even under a multivariate heavy-tailed distribution, but this does reduce the advantage of the location tests  over accuracy tests (Figure~\ref{fig:t_null}).




\begin{figure}[h]
	\centering
	\caption{\mycaption}	
	%\label{fig:simulation_1}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{"art/file8"}
		\caption{\textbf{Scale Change.} 
			$Var[\x_i|y_i=0] = I; Var[\x_i|y_i=1] = \Sigma$ where $\Sigma$ is diagonal with $\Sigma_{jj}=exp((j-p/2)/4*effect)$.}  
		\label{fig:scale_change}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{"art/file9"}
		\caption{\textbf{Heavytailed.} $\eta_i$ is $p$-variate t, with $df=3$ .  } 
		\label{fig:t_null}
	\end{subfigure}
\end{figure}





\subsection{Departure from Independence}
\label{sec:dependence}

We now test the robustness of our results to the correlations in $x$. 
In terms of Eq.(\ref{eq:distribution}), $\Sigma$ will no longer be the identity matrix. 
Intuitively- both location tests and accuracy tests include the estimation of $\Sigma$, so that correlations should be accounted for. 
The question remains- which test has more power?
We address this matter in two setups. 
The first is a ``short memory'' dependence, for which we use an $AR(1)$ covariance.
The second is a ``long range'' dependence, for which we use the Brownian motion covariance. 

We would expect \emph{Oracle} to be the best performer within the location tests. 
We would also expect \emph{SD} to be the worst performer, particularly in the long memory setup, since its assumed diagonal covariance is far from the true covariance. 
From the long memory setup in Figure~\ref{fig:pink_noise} we see that the Oracle is indeed the best performed, but we are surprised that:
(a) An SVM with small regularization (\emph{SVM.CV.2}) is the best test statistic.
(b) \emph{SD} outperforms \emph{Hotelling.shrink} even though is assumes a diagonal covariance, which is far from the true Brownian motion covariance. 
[TODO: why??]


\begin{figure}[h]
	\centering
	\caption{\mycaption}	
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{"art/file11"}
		\caption{TODO: Oracle looks bad. 
			Short memory AR(1) dependence: $\Sigma_{k,l}=\rho^{|k-l|}; \rho=0.8$. } 
		\label{fig:ar_1}
	\end{subfigure}
	\begin{subfigure}{.4\textwidth}
		\centering
		  \includegraphics[width=1\linewidth]{"art/file16"}
		\caption{Long-memory Brownian motion dependence: $\Sigma_{k,l}=\rho \min\{k,l\}; \rho=0.2$.} 
		\label{fig:pink_noise}
	\end{subfigure}
\end{figure}





\subsection{Departure from Shift Alternatives}
Is it possible for an accuracy test to have more power than a location test?
Yes. 
We already saw one example in the long memory setup in Section~\ref{sec:dependence}.
\emph{Scale alternatives}, where signal is carried in the covariance and not in means, provide one such example. 
Formally, $\mathbb{E}[x|y=1]=\mathbb{E}[x|y=0]$, but $Var[x|y=1]\neq Var[x|y=0]$.

In Figure~\ref{fig:scale_change} we report a simulation designed to show the possibility of an accuracy test being more powerful then a location test. 
This result is merely a proof of existence, and is not to be interpreted as ``accuracy tests have more power than location tests against scale alternatives''.
If the practitioner were expecting a change in scatter, then a Neyman-Pearson type reasoning would lead them to abandon both location and accuracy tests, and adopt a bona-fide covariance test \citep[e.g.][]{ley2015high}.
Our empirical example in Section~\ref{sec:example}, and experience, actually support the opposite conclusion: that location tests have more power than accuracy tests. 





\subsection{Departure from V-fold CV}
\label{sec:bootstrap}

Intuition suggests we may alleviate the discretization by cross-validating with replacement, instead of the V-fold CV.
The discreteness of the accuracy statistic is governed by the number of examples in the union of holdout test sets.
For V-fold CV, for instance, the accuracy may assume as many values as the sample size. 
This suggests that the accuracy can be ``smoothed'' by allowing the test sample to be drawn with replacement. 
An algorithm that samples test sets with replacement is the \emph{leave-one-out bootstrap estimator},  and its derivatives, such as the \emph{0.632 bootstrap}, and \emph{0.632+ bootstrap} \citep[Sec 7.11]{hastie_elements_2003}.
\begin{definition}[bLOO]
	\label{def:bloo}
	The \emph{leave-one-out bootstrap} estimate, bLOO, is the average accuracy of the holdout observations, over all bootstrap samples. 
	Denote by $\data^b$, a bootstrap sample $b$ of size $n$, sampled with replacement from $\data$. 
	Also denote by $C^{(i)}$ the index set of bootstrap samples not containing observation $i$.
	The leave-one-out bootstrap estimate, $\accEstim_{\algo}^{bLOO}$,  is defined as:
	\begin{align}
	\accEstim_{\algo}^{bLOO}:= \frac 1n \sum_{i=1}^{n} \frac{1}{|C^{(i)}|} \sum_{b \in C^{(i)}} \indicator{\hypFun{\data^b}{x_i}=y_i}.
	\end{align}
	An equivalent formulation, which stresses the Bootstrap nature of the algorithm is the following. 
	Denoting by $S^{(b)}$ the indexes of observations that are \emph{not} in the bootstrap sample $b$ and are not empty, 
	\begin{align}
	\accEstim_{\algo}^{bLOO} = \frac 1B \sum_{b=1}^{B} \frac{1}{|S^{(b)}|} \sum_{i \in S^{(b)}} \indicator{\hypFun{\data^b}{x_i}=y_i}.
	\end{align}
\end{definition}

\begin{definition}[b$0.632$]
	\label{def:b0632}
	The \emph{0.632 bootstrap} accuracy estimate, b$0.632$, is a weighted average of the resubstitution error and the bLOO.
	Formally:
	\begin{align}
	\accEstim_{\algo}^{0.632} := 0.368 \; \accEstim_{\algo}^{Resub}  + 0.632 \; \accEstim_{\algo}^{bLOO}.
	\end{align}
\end{definition}
Simulation results are reported in Figure~\ref{fig:bootstrap} with naming conventions in Table~\ref{tab:collected_2}.
It can be seen that selecting test sets with replacement does increase the power of accuracy tests, when compared to V-fold cross validation, but still falls short from the power of location tests. 
It can also be seen that power increases with the number of bootstrap replications, as was to be expected, since more replications reduce the level of discretization.
The type of bootstrap, bLOO versus b$0.632$, does not change the power. 

\bigskip

%TODO: change cost in simulation to 0.1
\begin{tcolorbox}
	\centering
	\begin{tabular}{l|c|c|c|c|c}
		Name & Algorithm & Resampling & B & Z-scored & Parameters\\ 
		\hline
		\hline
		LDA.Boot.1 & LDA & b$0.632$ & $10$ & FALSE &  -- \\ 
		LDA.Boot.2 & LDA & bLOO 	& $10$ & FALSE &  -- \\ 
		SVM.Boot.1 & SVM & b$0.632$ & $10$ & FALSE & cost=10 \\ 
		SVM.Boot.2 & SVM & bLOO 	& $10$ & FALSE & cost=10 \\ 
		SVM.Boot.3 & SVM & b$0.632$ & $50$ & FALSE & cost=10 \\ 
		SVM.Boot.4 & SVM & bLOO 	& $50$ & FALSE & cost=10 \\ 
	\end{tabular} 
	\captionsetup{type=table}
	\caption{
		The same as Table~\ref{tab:collected} for bootstraped accuracy estimates. 
		bLOO and b$0.632$ are defined in definitions~\ref{def:bloo} and \ref{def:b0632} respectively.
		$B$ denotes the number of Bootstrap samples. } 
	\label{tab:collected_2}
\end{tcolorbox}


\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{"art/file13"}
	\caption{
		\textbf{Bootstrap--}
		The power of a permutation test with various test statistics. 
		The power on the $x$ axis. 
		Effects are color and shape coded. 
		The various statistics on the $y$ axis. 
		Their details are given in tables~\ref{tab:collected} and \ref{tab:collected_2}. 
		Effects vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
		Simulation details in Appendix~\ref{sec:simulation_details}.
	} 
	\label{fig:bootstrap}
\end{figure}



\subsection{The Effect of High Dimension}
\label{sec:highdim}
Inspecting Figure~\ref{fig:simulation_11} (for instance), it can be seen that Hotelling's unregularized  $T^2$ test has similar power as accuracy tests. 
It could thus be argued that the real advantage of \emph{SD} or \emph{Hotelling.shrink} is not in their sensitivity to location, but rather, in their adaptation to high dimension\footnote{By \emph{high-dimension} it is typically meant that $p/n$ is not too small, regardless of $p$ and $n$ themselves. } by regularization.
To study this, we call upon several \emph{covariance regularized classifiers}, designed for high dimensional problems. 
We try an $l_2$ regularized SVM \citep{friedman_regularization_2010}, and a shrinkage based LDA \citep{pang_shrinkage-based_2009,ramey_high-dimensional_2016}, which are similar in spirit to the \emph{Hotelling.shrink} test. 
We also try we try a diagonalized LDA\footnote{Known as \emph{Gaussian Na\"ive Bayes}.} \citep{dudoit_comparison_2002}, which is similar in spirit to the \emph{SD} location test. 


Simulation results are reported in Figure~\ref{fig:highdim} with naming conventions in Table~\ref{tab:collected_3}.
It can be seen that regularizing a classifier in high dimension, just like a location test, improves power. 
It can also be seen that (regularized) location tests are still more powerful than (regularized) accuracy tests. 
This was to be expected, since we already saw in (e.g.) Figure~\ref{fig:simulation_11} that the unregularized location test, \emph{Hotelling}, is slightly more powerful than unregularized accuracy tests such as (e.g.) \emph{SVM.CV.1}; a phenomenon we attribute to the discretization effect.



\bigskip

%TODO: change cost in simulation to 0.1
\begin{tcolorbox}
	\centering
	\begin{tabular}{l|c|c|c|c}
		Name & Algorithm & Resampling & Z-scored & Parameters\\ 
		\hline
		\hline
		SVM.highdim.1 & SVM & V-fold & FALSE & cost=10, V=4 \\ 
		SVM.highdim.2 & SVM & b$0.632$ & FALSE & cost=10, B=50 \\ 
		LDA.highdim.1 & LDA & V-fold & FALSE & V=4 \\ 
		LDA.highdim.2 & LDA & V-fold & FALSE & V=4 \\ 
		LDA.highdim.3 & LDA & V-fold & FALSE & V=4 \\ 
		LDA.highdim.4 & LDA & b$0.632$ & FALSE & B=50 \\ 
	\end{tabular} 
	\captionsetup{type=table}
	\caption{
		The same as Table~\ref{tab:collected} for regularized (high dimensional) predictors. 
		\emph{SVM.highdim.1} is an $l_2$ regularized SVM \citep{friedman_regularization_2010}. 
		\emph{SVM.highdim.2} is the same with b$0.632$ instead of V-fold cross validation. 
		\emph{LDA.highdim.1} is the Diagonal Linear Discriminant Analysis of \cite{dudoit_comparison_2002}.
		\emph{LDA.highdim.2} is the High-Dimensional Regularized Discriminant Analysis of \cite{ramey_high-dimensional_2016}.
		\emph{LDA.highdim.3} is the Shrinkage-based Diagonal Linear Discriminant Analysis of \cite{pang_shrinkage-based_2009}.
		\emph{LDA.highdim.4} is the same with b$0.632$.
	} 
	\label{tab:collected_3}
\end{tcolorbox}


\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{"art/file14"}
	\caption{
		\textbf{HighDim Classifier--} 
		The power of a permutation test with various test statistics. 
		The power on the $x$ axis. 
		Effects are color and shape coded. 
		The various statistics on the $y$ axis. 
		Their details are given in tables~\ref{tab:collected} and \ref{tab:collected_3}. 
		Effects vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
		Simulation details in Appendix~\ref{sec:simulation_details}.
	} 
	\label{fig:highdim}
\end{figure}








%%%%%%%   Neuroimaging
\section{Neuroimaging Example}
\label{sec:example}

Figure~\ref{fig:read_data} is an application of both a location and an accuracy test to the data of \cite{pernet_human_2015}. 
The authors of \cite{pernet_human_2015} collected fMRI data while subjects were exposed to the sounds of human speech (vocal), and other non-vocal sounds. 
Each subject was exposed to $20$ sounds of each type, totaling in $n=40$ trials.
The study was rather large and consisted of about $200$ subjects.
The data was kindly made available by the authors at the OpenfMRI website\footnote{\url{https://openfmri.org/}}.

We perform group inference using within-subject permutations along the analysis pipeline of \cite{stelzer_statistical_2013}, which was also reported in \cite{gilron_quantifying_2016}. 
To demonstrate our point, we compare the \emph{SD} location test with the \emph{SVM.cv.1} accuracy test. 

In agreement with our simulation results, the location test (\emph{sd}) discovers more brain regions of interest when compared to an accuracy test (\emph{svm.cv.1}).
The former discovers $1,232$ regions, while the latter only $441$, as depicted in Figure~\ref{fig:read_data}.
We emphasize that both test statistics were compared with the same permutation scheme, and the same error controls, so that any difference in detections is due to their different power.


\begin{figure}[th]
\centering
\includegraphics[width=0.7\linewidth]{"art/svm_vs_SD"}
\caption{\footnotesize
Brain regions encoding information discriminating between vocal and non-vocal stimuli.
Map reports the centers of $27$-voxel sized spherical regions, as discovered by an accuracy test (\emph{svm.cv.1}), and a location test (\emph{sd}). 
\emph{svm.cv.1} was computed using $5$-fold cross validation, and a cost parameter of $1$. 
Region-wise significance was determined using the permutation scheme of \cite{stelzer_statistical_2013}, followed by region-wise $FDR \leq 0.05$ control using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}.
Number of permutations equals $400$.
The location test detect $1,232$ regions, and the accuracy test $441$, $399$ of which are common to both.
For the details of the analysis see \cite{gilron_quantifying_2016}.  
  }
\label{fig:read_data}
\end{figure}








%%%% Section %%%%
\section{Discussion}
\label{sec:discussion}

We have set out to understand which of the tests is more powerful: accuracy tests or location tests. 
Our practical advice for the practitioner, is that accuracy tests are never optimal. 
There is always a multivariate test, possibly a location test, that dominates in power. 
The class of location tests we examined, in particular their regularized versions, are good performers in a wide range of simulation setups and empirically. 
They are also typically easier to implement, and faster to run, since no cross validation will be involved. 
Their high-dimensional versions, such as \cite{schafer_shrinkage_2005}, \cite{goeman2006testing}, and \cite{srivastava_multivariate_2007}, are particularly well suited for empirical problems such as neuroimaging and genetics.




\subsection{Where do Accuracy Tests Lose Power?}
The low power of the accuracy tests compared to location tests can be attributed to the following causes:\newline
(a) A Neyman-Pearson type argument, favoring location tests for the shift alternatives we simulated (even if not maximal power). \newline
(b) The discrete nature of accuracy tests. \newline
(c) Inefficient use of the data when validating with a holdout set. \newline
(d) Inappropriate regularization in high SNR regimes: testing requires less regularization than predicting. \newline



\paragraph{Shift Alternatives}
We focused on shift alternatives. 
One may argue that empirical alternatives are rarely of shift type, thus limiting our conclusions. 
We reply that our empirical evidence in Section~\ref{sec:example}, also favors location tests. 
More generally: discretization alone suffice to state that there will always be a test statistic with more power than an accuracy test. 
When looking for a ``one-size fits all'' strategy, it is our experience that location tests are a better candidate than accuracy tests. 



\paragraph{Discreteness}
The degree of discretization is governed by the sample size. 
For this reason, an asymptotic analysis such as \cite{ramdas_classification_2016}, or \cite{golland_permutation_2005}, will not capture power loss due to discretization\footnote{This actually holds for all power analyses relying on a \emph{contiguity} argument \cite[Ch.6]{vaart_asymptotic_1998}.}.
These studies have other limitations as well.
\citet{ramdas_classification_2016} assume a split-sample holdout, without cross-validation.
This hurts the power of accuracy tests. 
The asymptotic analysis of the resubstitution accuracy statistics in \citet{golland_permutation_2005} not only conceals the discreteness of the test statistic, but also the effects of its concentration. 
This analysis renders resubstitution accuracy estimates a legitimate asymptotic test, while they are a terrible finite sample test. 




\subsection{Interpretation}
Multivariate tests, and location tests in particular, are easier to interpret. 
To do so we typically use a Neyman-Pearson type argument, and think of the type of signal a test is sensitive to. 
Accuracy tests are seen as ``black boxes'', even though they can be analyzed in the same way. 
\citet{gilron2017s} for instance, demonstrate that the type of signal captured by accuracy tests is less obvious to neuroimaging practitioners than location tests. 



\subsection{Testing in Augmented Spaces}
It may be argued that accuracy tests permits the separation between classes in high dimensions, such as in \emph{reproducing kernel Hilbert spaces} (RKHS) by using non-linear predictors while location tests do not. 
This is a false argument-- accuracy tests do not have any more flexibility than location tests. 
Indeed, it is possible to test for location in the same space the classifier is learned. 
For independence tests in high dimensional spaces see for example \cite{szekely_brownian_2009} or \citet{gretton_kernel_2012-1}.













\subsection{A Good Accuracy Test}
There are cases where an accuracy test cannot be replaced by some location, or other, statistical test. 
Brain-computer interfaces and clinical diagnostics \citep[e.g.][]{olivetti_induction_2012,wager_fmri-based_2013} are examples where we want to know not only if information is encoded in a region, but rather, that a particular decoder can extract it. 
For these cases, we collect some conclusions and best practices. 


\paragraph{Sample size.} The conservativeness of accuracy tests decrease with sample size. 


\paragraph{Regularize.}
Regularization proves crucial to detection power in low SNR regimes, which may be does to strong correlations or high-dimension.
We find that the Shrinkage-based Diagonal Linear Discriminant Analysis of \cite{pang_shrinkage-based_2009} is a particularly good performer, but more research is required on this matter. 


\paragraph{Smooth accuracy.}
Smooth accuracy estimate by cross validating with replacement. The bLOO estimator, in particular, is preferable over V-fold.

We can compound regularization with bootstrapping to improve the power of the accuracy tests. 
This is done in the \emph{SVM.highdim.2} and \emph{LDA.highdim.4} tests. 
The latter being one of the very few accuracy tests that achieve the same power as location tests. 
This is exciting since it shows how to design powerful new high-powered accuracy tests: by sampling test sets with replacement, and by regularizing the classifiers. 


\paragraph{Permute features.} Permuting features, such as in \cite{golland_permutation_2005}, is easier than permuting labels. 
It allows to preserve the balance of folds after a permutation, without refolding.

\paragraph{Resubstitution accuracy in low dimension.} Resubstitution accuracy is useful in low SNR regimes, such as low dimensional problems, because it avoids cross validation without compromising power. 
In high dimension, the power loss is considerable compared to a cross validated approach. 
We attribute this to the compounding of discretization and concentration effects: the difference between the sampling distribution of the resubstitution accuracy is simply indistinguishable under the null and under the alternative. 
In low dimensional problems, the discretization is less impactful, and the computational burden of cross validation can be avoided by using the resubstitution accuracy. 
There is a fundamental difference between V-folding and resubstitution. The latter should not be thought of as the limit of the former. 
 


















\subsection{Related Literature}
\cite{ojala_permutation_2010} study the power of two accuracy tests differing in the permutation scheme:
One testing the ``no signal'' null hypothesis, and the other testing the ``independent features'' null hypothesis. 
They perform an asymptotic analysis, and a simulation study. 
They also apply various classifiers to various data sets. 
Their emphasis is the effect of the underlying classifier on the power, and the potential of the ``independent features'' test for feature selection.
This is a very different emphasis from our own.


\cite{olivetti_induction_2012} and \cite{olivetti_statistical_2014} looked into the problem of choosing a good accuracy test. 
They propose a new test they call an \emph{independence test}, and demonstrate by simulation that it has more power than other accuracy tests, and can deal with non-balanced data sets. 
We did not include this test in the battery we compared, but we note that the independence test of \cite{olivetti_induction_2012} relies on a discrete test statistic. 
It may thus be improved by regularizing and resampling with replacement, before the application of \cite{olivetti_induction_2012}'s test statistic. 


\cite{golland_permutation_2003} and \cite{golland_permutation_2005} study accuracy tests using simulation, neuroimaging data, genetic data, and analytically.
Their analytic results formalize our intuition from Section~\ref{sec:introduction} on the effect of concentration of the accuracy statistic:
The finite Vapnik–Chervonenkis dimension requirement \citep[Sec 4.3]{golland_permutation_2005} prevents the permutation p-value from (asymptotically) concentrating near $1$. 
Like us, they find that the power increases with the size of the test set. 
This is seen in Fig.4 of \citet{golland_permutation_2005}, where the size of the test-set, $K$, governs the discretization. 
Since they permute features, not labels, then all their permutation samples are balanced, and there is no issue of refolding. 

\cite{golland_permutation_2005} simulate the power of accuracy tests by sampling from a Gaussian mixture family of models, and not from a location family as our own simulations. 
Under their model (with some abuse of notation)
$$(x_i|y_i=1) \sim p \gauss{\mu_1,I}+ (1-p) \gauss{\mu_2,I}$$ 
and 
$$(x_i|y_i=0) \sim (1-p) \gauss{\mu_1,I}+ p \gauss{\mu_2,I}.$$
Varying $p$ interpolates between the null distribution $(p=0.5)$ and a location shift model $(p=0)$. 
We now perform the same simulation as \cite{golland_permutation_2005}, after re-parameterizing so that $p=0$ corresponds to the null model, and in the same dimensionality as our previous simulations.
We find that also in this mixture class of models a location test has more power than an accuracy test (Figure~\ref{fig:golland}).



\begin{figure}[ht]
\centering
	  \includegraphics[width=0.7\linewidth]{"art/file12"}
	  \caption{TODO: verify!
	  	\textbf{Mixture--} $\x_i = \chi_i \mu + \eta_i; \chi_i = \set{-1,1}$ and $\prob{\chi_i=1}=(1/2-p)^{\y_i}  (1/2+p)^{1-\y_i}$. $\mu$ is a $p$-vector with $3/\sqrt{p}$ in all coordinates.
	  The effect, $p$, is color and shape coded and varies over $0$ (red circle), $1/4$ (green triangle) and $1/2$ (blue square). }
	\label{fig:golland}
\end{figure}










\subsection{Epilogue}
Given all the above, we find the popularity of accuracy tests for signal detection quite puzzling. 
We believe this is due to a reversal of the inference cascade. 
Researchers first fit a classifier, and then ask if the classes are any different.
Were they to start by asking if classes are any different, and only then try to classify, then location tests would naturally arise as the preferred method. 
As put by \cite{ramdas_classification_2016}:
\begin{quote}
The recent popularity of machine learning has resulted in the extensive teaching and use
of prediction in theoretical and applied communities and the relative lack of awareness or
popularity of the topic of Neyman-Pearson style hypothesis testing in the computer science
and related ``data science'' communities.
\end{quote}






%\section{Acknowledgments}
%TODO
% isf 900/60, Jelle, Jesse B.A. Hemerik, Yakir Brechenko, Omer Shamir, Joshua Vogelstein, Gilles Blanchard, Jason 




\newpage
\bibliographystyle{abbrvnat}
\bibliography{Permuting_Accuracy.bib}

\appendix





\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Analysis pipeline}
%\label{apx:analysis}
%
%Here is the analysis pipeline of \cite{stelzer_statistical_2013} we for the auditory data in \cite{gilron_quantifying_2016}.
%Denoting by 
%$i=1,\dots,I$ the subject index, 
%$v=1,\dots,V$ the voxel index, and 
%$s = 1,\dots,S$ the permutation index. 
%Since regions\footnote{\emph{searchlight} or \emph{sphere} in the MVPA parlance} are centered around a unique voxel, the voxel index $v$ also serves as a unique region index.
%Algorithm~\ref{algo:statistic} computes a region-wise test statistic, which is compared to its permutation null distribution computed by Algorithm~\ref{algo:permutation}.
%
%
%\begin{algorithm}[H]
%\caption{Compute a group parametric map.}
%\label{algo:statistic}
%
% \KwData{fMRI scans, and experimental design.}
% \KwResult{Brain map of group statistics: $\{\bar{T}_v\}_{v=1}^V$}
%	 \For{$v \in 1,\dots,V$}{
%		 \For{$i \in 1,\dots,I$}{
%			 $T_{i,v} \leftarrow$ test statistic for subject $i$ in a region centered at $v$.
%			 } 	  
%	  	 $\bar{T}_{v} \leftarrow \frac{1}{I}\sum_{i=1}^I T_{i,v}$. 
% 	 }
%\end{algorithm}
%
%
%\begin{algorithm}[H]
%\caption{Compute a permutation p-value map.} 
%\label{algo:permutation}
%
% \KwData{fMRI scans of $20$ subjects, experimental design.}
% \KwResult{Brain map of permutation p-values: $\{p_v\}_{v=1}^V$}
%  \For{$s \in 1,\dots\,S$}{
%    	    permute labels\;
%    	    $\bar{T}_{v}^s \leftarrow$ parametric map 
%  
%  	}
%\end{algorithm}
%
%










%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Simulation Results}
%\label{apx:simulations}
%
%
%\begin{figure}[h]
%\centering
%\caption{\mycaption}	
%\label{fig:n_folds}
%	\begin{subfigure}{.5\textwidth}
%	  \centering
%	  \includegraphics[width=1\linewidth]{"art/file4"}
%	  \caption{\textbf{2-fold} cross validation. Balanced folding.}  
%	\label{fig:n_folds_1}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%	  \centering
%	  \includegraphics[width=1\linewidth]{"art/file6"}
%	  \caption{\textbf{20-fold} cross validation. Balanced folding} 
%	\label{fig:n_folds_2}
%	\end{subfigure}
%\end{figure}



%\begin{figure}[h]
%\centering
%\caption{\mycaption}	
%\label{fig:n_folds_unbalanced}
%	\begin{subfigure}{.5\textwidth}
%	  \centering
%	  \includegraphics[width=1\linewidth]{"art/file5"}
%	  \caption{\textbf{2-fold} cross validation. Unbalanced folding.} 
%	\label{fig:n_folds_unbalanced_1}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%	  \centering
%	  \includegraphics[width=1\linewidth]{"art/file7"}
%	  \caption{\textbf{20-fold} cross validation. Unbalanced folding.} 
%	\label{fig:n_folds_unbalanced_2}
%	\end{subfigure}
%\end{figure}




\end{document}
