
@article{benjamini_controlling_1995,
  title = {Controlling the false discovery rate: a practical and powerful approach to multiple testing},
  volume = {57},
  shorttitle = {Controlling the false discovery rate},
  timestamp = {2013-04-28T13:10:53Z},
  journal = {JOURNAL-ROYAL STATISTICAL SOCIETY SERIES B},
  author = {Benjamini, Y. and Hochberg, Y.},
  year = {1995},
  keywords = {FDR,Seminal,Statistics},
  pages = {289--289},
  file = {Benjamini_Hochberg_1995_Controlling the false discovery rate.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/D5FV8QSI/Benjamini_Hochberg_1995_Controlling the false discovery rate.pdf:application/pdf;Controlling the False Discovery Rate\: A Practical and Powerful Approach to Multiple Testing:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/DDUCV4K7/2346101.html:text/html}
}

@article{schafer_shrinkage_2005,
  title = {A {{Shrinkage Approach}} to {{Large-Scale Covariance Matrix Estimation}} and {{Implications}} for {{Functional Genomics}}},
  volume = {4},
  issn = {1544-6115},
  doi = {10.2202/1544-6115.1175},
  timestamp = {2016-08-02T19:07:24Z},
  number = {1},
  urldate = {2013-09-15},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  author = {Sch{\"a}fer, Juliane and Strimmer, Korbinian},
  month = jan,
  year = {2005},
  file = {Schäfer et al_2005_A shrinkage approach to large-scale covariance matrix estimation and.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/AXE5W4FW/Schäfer et al_2005_A shrinkage approach to large-scale covariance matrix estimation and.pdf:application/pdf;Schäfer_Strimmer_2005_A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/Q9GFBFS7/Schäfer_Strimmer_2005_A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and.pdf:application/pdf;A shrinkage approach to large-scale covariance matrix estimation and implications for... - Abstract - Europe PubMed Central:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/JKX3G4SS/16646851.html:text/html}
}

@article{fan_liblinear:_2008,
  title = {{{LIBLINEAR}}: {{A Library}} for {{Large Linear Classification}}},
  volume = {9},
  issn = {1532-4435},
  shorttitle = {{{LIBLINEAR}}},
  abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
  timestamp = {2016-08-19T13:51:12Z},
  urldate = {2014-03-10},
  journal = {J. Mach. Learn. Res.},
  author = {Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  month = jun,
  year = {2008},
  pages = {1871--1874},
  file = {Fan et al_2008_LIBLINEAR.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/RAU6THKQ/Fan et al_2008_LIBLINEAR.pdf:application/pdf}
}

@book{vaart_asymptotic_1998,
  address = {Cambridge, UK ; New York, NY, USA},
  title = {Asymptotic {{Statistics}}},
  isbn = {978-0-521-49603-2},
  abstract = {Here is a practical and mathematically rigorous introduction to the field of asymptotic statistics. In addition to most of the standard topics of an asymptotics course--likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures--the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, one of the book's unifying themes that mainly entails the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation.},
  language = {English},
  timestamp = {2014-05-12T07:27:16Z},
  publisher = {{Cambridge University Press}},
  author = {{van der Vaart}, A. W.},
  month = oct,
  year = {1998}
}

@book{devroye_probabilistic_1997,
  address = {New York},
  edition = {Corrected edition},
  title = {A {{Probabilistic Theory}} of {{Pattern Recognition}}},
  isbn = {978-0-387-94618-4},
  abstract = {A self-contained and coherent account of probabilistic techniques, covering: distance measures, kernel rules, nearest neighbour rules, Vapnik-Chervonenkis theory, parametric classification, and feature extraction. Each chapter concludes with problems and exercises to further the readers understanding. Both research workers and graduate students will benefit from this wide-ranging and up-to-date account of a fast- moving field.},
  language = {English},
  timestamp = {2016-08-05T06:04:21Z},
  publisher = {{Springer}},
  author = {Devroye, Luc and Gy{\"o}rfi, L{\'a}szl{\'o} and Lugosi, Gabor},
  month = mar,
  year = {1997},
  keywords = {Computers / Computer Vision \& Pattern Recognition,Computers / Optical Data Processing,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Psychology / Cognitive Psychology \& Cognition}
}

@article{lehmann_parametric_2009,
  title = {Parametric versus nonparametrics: two alternative methodologies},
  volume = {21},
  issn = {1048-5252},
  shorttitle = {Parametric versus nonparametrics},
  doi = {10.1080/10485250902842727},
  abstract = {This article compares parametric and nonparametric approaches to statistical inference. It considers their advantages and disadvantages, and their areas of applicability. Although there is no clear comprehensive conclusion, the article finds that in simple problems in which Wilcoxon type tests and estimators apply, they may be recommended as the methods of choice.},
  timestamp = {2014-07-29T19:18:14Z},
  number = {4},
  urldate = {2012-06-11},
  journal = {Journal of Nonparametric Statistics},
  author = {Lehmann, Erich L.},
  year = {2009},
  pages = {397--405},
  file = {Lehmann_2009_Parametric versus nonparametrics.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TUM867X9/Lehmann_2009_Parametric versus nonparametrics.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/VZ6JRE7S/10485250902842727.html:text/html}
}

@article{jimura_analyses_2012,
  series = {Multivoxel pattern analysis and cognitive theories},
  title = {Analyses of regional-average activation and multivoxel pattern information tell complementary stories},
  volume = {50},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2011.11.007},
  abstract = {Multivariate pattern analysis (MVPA) has recently received increasing attention in functional neuroimaging due to its ability to decode mental states from fMRI signals. However, questions remain regarding both the empirical and conceptual relationships between results from MVPA and standard univariate analyses. In the current study, whole-brain univariate and searchlight MVPAs of parametric manipulations of monetary gain and loss in a decision making task (Tom et al., 2007) were compared to identify the differences in the results across these methods and the implications for understanding the underlying mental processes. The MVPA and univariate results did identify some overlapping regions in whole brain analyses. However, an analysis of consistency revealed that in many regions the effect size estimates obtained from MVPA and univariate analysis were uncorrelated. Moreover, comparison of sensitivity showed a general trend towards greater sensitivity to task manipulations by MVPA compared to univariate analysis. These results demonstrate that MVPA methods may provide a different view of the functional organization of mental processing compared to univariate analysis, wherein MVPA is more sensitive to distributed coding of information whereas univariate analysis is more sensitive to global engagement in ongoing tasks. The results also highlight the need for better ways to integrate these methods.},
  timestamp = {2015-03-19T08:14:51Z},
  number = {4},
  urldate = {2015-03-19},
  journal = {Neuropsychologia},
  author = {Jimura, Koji and Poldrack, Russell A.},
  month = mar,
  year = {2012},
  keywords = {decision-making,fMRI,MVPA,Support vector regression,Univariate analysis},
  pages = {544--552},
  file = {Jimura_Poldrack_2012_Analyses of regional-average activation and multivoxel pattern information tell.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/XHK76TX4/Jimura_Poldrack_2012_Analyses of regional-average activation and multivoxel pattern information tell.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TUQNCDIP/S0028393211005070.html:text/html}
}

@article{kriegeskorte_information-based_2006,
  title = {Information-based functional brain mapping},
  volume = {103},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0600244103},
  abstract = {The development of high-resolution neuroimaging and multielectrode electrophysiological recording provides neuroscientists with huge amounts of multivariate data. The complexity of the data creates a need for statistical summary, but the local averaging standardly applied to this end may obscure the effects of greatest neuroscientific interest. In neuroimaging, for example, brain mapping analysis has focused on the discovery of activation, i.e., of extended brain regions whose average activity changes across experimental conditions. Here we propose to ask a more general question of the data: Where in the brain does the activity pattern contain information about the experimental condition? To address this question, we propose scanning the imaged volume with a ``searchlight,'' whose contents are analyzed multivariately at each location in the brain.},
  language = {en},
  timestamp = {2015-03-31T16:52:56Z},
  number = {10},
  urldate = {2015-03-31},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  author = {Kriegeskorte, Nikolaus and Goebel, Rainer and Bandettini, Peter},
  month = jul,
  year = {2006},
  keywords = {Functional Magnetic Resonance Imaging,Neuroimaging,Statistical analysis},
  pages = {3863--3868},
  file = {Kriegeskorte et al_2006_Information-based functional brain mapping.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SGBTJT8B/Kriegeskorte et al_2006_Information-based functional brain mapping.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/P9ADDW3Q/3863.html:text/html},
  pmid = {16537458}
}

@book{anderson_introduction_2003,
  address = {Hoboken, NJ},
  edition = {3 edition},
  title = {An {{Introduction}} to {{Multivariate Statistical Analysis}}},
  isbn = {978-0-471-36091-9},
  abstract = {Perfected over three editions and more than forty years, this field- and classroom-tested reference: * Uses the method of maximum likelihood to a large extent to ensure reasonable, and in some cases optimal procedures. * Treats all the basic and important topics in multivariate statistics. * Adds two new chapters, along with a number of new sections. * Provides the most methodical, up-to-date information on MV statistics available.},
  language = {English},
  timestamp = {2015-05-04T06:37:32Z},
  publisher = {{Wiley-Interscience}},
  author = {Anderson, T. W.},
  month = jul,
  year = {2003}
}

@article{efron_improvements_1997,
  title = {Improvements on {{Cross-Validation}}: {{The}} .632+ {{Bootstrap Method}}},
  volume = {92},
  copyright = {Copyright \textcopyright{} 1997 American Statistical Association},
  issn = {0162-1459},
  shorttitle = {Improvements on {{Cross-Validation}}},
  doi = {10.2307/2965703},
  abstract = {A training set of data has been used to construct a rule for predicting future responses. What is the error rate of this rule? This is an important question both for comparing models and for assessing a final selected model. The traditional answer to this question is given by cross-validation. The cross-validation estimate of prediction error is nearly unbiased but can be highly variable. Here we discuss bootstrap estimates of prediction error, which can be thought of as smoothed versions of cross-validation. We show that a particular bootstrap method, the .632+ rule, substantially outperforms cross-validation in a catalog of 24 simulation experiments. Besides providing point estimates, we also consider estimating the variability of an error rate estimate. All of the results here are nonparametric and apply to any possible prediction rule; however, we study only classification problems with 0-1 loss in detail. Our simulations include "smooth" prediction rules like Fisher's linear discriminant function and unsmooth ones like nearest neighbors.},
  timestamp = {2015-05-27T07:17:14Z},
  number = {438},
  urldate = {2015-05-27},
  journal = {Journal of the American Statistical Association},
  author = {Efron, Bradley and Tibshirani, Robert},
  month = jun,
  year = {1997},
  pages = {548--560},
  file = {Efron_Tibshirani_1997_Improvements on Cross-Validation.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/QR5QUGQ9/Efron_Tibshirani_1997_Improvements on Cross-Validation.pdf:application/pdf}
}

@article{srivastava_two_2013,
  title = {A two sample test in high dimensional data},
  volume = {114},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2012.08.014},
  abstract = {In this paper we propose a test for testing the equality of the mean vectors of two groups with unequal covariance matrices based on N 1 and N 2 independently distributed p -dimensional observation vectors. It will be assumed that N 1 observation vectors from the first group are normally distributed with mean vector \ensuremath{\mu} 1 and covariance matrix \ensuremath{\Sigma} 1 . Similarly, the N 2 observation vectors from the second group are normally distributed with mean vector \ensuremath{\mu} 2 and covariance matrix \ensuremath{\Sigma} 2 . We propose a test for testing the hypothesis that \ensuremath{\mu} 1 = \ensuremath{\mu} 2 . This test is invariant under the group of p \texttimes{} p nonsingular diagonal matrices. The asymptotic distribution is obtained as ( N 1 , N 2 , p ) \ensuremath{\rightarrow} \ensuremath{\infty} and N 1 / ( N 1 + N 2 ) \ensuremath{\rightarrow} k \ensuremath{\in} ( 0 , 1 ) but N 1 / p and N 2 / p may go to zero or infinity. It is compared with a recently proposed non-invariant test. It is shown that the proposed test performs the best.},
  timestamp = {2015-05-31T14:04:32Z},
  urldate = {2015-05-31},
  journal = {Journal of Multivariate Analysis},
  author = {Srivastava, Muni S. and Katayama, Shota and Kano, Yutaka},
  month = feb,
  year = {2013},
  keywords = {Asymptotic theory,Behrens–Fisher problem,High-dimensional data,Hypothesis testing},
  pages = {349--358},
  file = {Srivastava et al_2013_A two sample test in high dimensional data.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/HITJM4F3/Srivastava et al_2013_A two sample test in high dimensional data.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/T8KIS5U9/S0047259X12002126.html:text/html}
}

@article{srivastava_test_2008,
  title = {A test for the mean vector with fewer observations than the dimension},
  volume = {99},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2006.11.002},
  abstract = {In this paper, we consider a test for the mean vector of independent and identically distributed multivariate normal random vectors where the dimension p is larger than or equal to the number of observations N. This test is invariant under scalar transformations of each component of the random vector. Theories and simulation results show that the proposed test is superior to other two tests available in the literature. Interest in such significance test for high-dimensional data is motivated by DNA microarrays. However, the methodology is valid for any application which involves high-dimensional data.},
  timestamp = {2015-06-01T11:56:51Z},
  number = {3},
  urldate = {2015-06-01},
  journal = {Journal of Multivariate Analysis},
  author = {Srivastava, Muni S. and Du, Meng},
  month = mar,
  year = {2008},
  keywords = {Asymptotic distribution,DNA microarray,Multivariate normal,Power comparison,Significance test},
  pages = {386--402},
  file = {Srivastava_Du_2008_A test for the mean vector with fewer observations than the dimension.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/7GXHRAJV/Srivastava_Du_2008_A test for the mean vector with fewer observations than the dimension.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/ZM7JH2QS/S0047259X06001990.html:text/html}
}

@article{stelzer_statistical_2013,
  title = {Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis ({{MVPA}}): {{Random}} permutations and cluster size control},
  volume = {65},
  issn = {1053-8119},
  shorttitle = {Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis ({{MVPA}})},
  doi = {10.1016/j.neuroimage.2012.09.063},
  abstract = {An ever-increasing number of functional magnetic resonance imaging (fMRI) studies are now using information-based multi-voxel pattern analysis (MVPA) techniques to decode mental states. In doing so, they achieve a significantly greater sensitivity compared to when they use univariate frameworks. However, the new brain-decoding methods have also posed new challenges for analysis and statistical inference on the group level. We discuss why the usual procedure of performing t-tests on accuracy maps across subjects in order to produce a group statistic is inappropriate. We propose a solution to this problem for local MVPA approaches, which achieves higher sensitivity than other procedures. Our method uses random permutation tests on the single-subject level, and then combines the results on the group level with a bootstrap method. To preserve the spatial dependency induced by local MVPA methods, we generate a random permutation set and keep it fixed across all locations. This enables us to later apply a cluster size control for the multiple testing problem. More specifically, we explicitly compute the distribution of cluster sizes and use this to determine the p-values for each cluster. Using a volumetric searchlight decoding procedure, we demonstrate the validity and sensitivity of our approach using both simulated and real fMRI data sets. In comparison to the standard t-test procedure implemented in SPM8, our results showed a higher sensitivity. We discuss the theoretical applicability and the practical advantages of our approach, and outline its generalization to other local MVPA methods, such as surface decoding techniques.},
  timestamp = {2015-08-26T20:55:19Z},
  urldate = {2013-08-30},
  journal = {NeuroImage},
  author = {Stelzer, Johannes and Chen, Yi and Turner, Robert},
  month = jan,
  year = {2013},
  keywords = {Cluster size control,fMRI,Multiple testing,MVPA,Second level analysis,Statistics},
  pages = {69--82},
  file = {Stelzer et al_2013_Statistical inference and multiple testing correction in classification-based.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/C9BKWQSA/Stelzer et al_2013_Statistical inference and multiple testing correction in classification-based.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/2NGS4Q2G/S1053811912009810.html:text/html;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/BX4T8DK8/S1053811912009810.html:text/html}
}

@article{molinaro_prediction_2005,
  title = {Prediction error estimation: a comparison of resampling methods},
  volume = {21},
  issn = {1367-4803, 1460-2059},
  shorttitle = {Prediction error estimation},
  doi = {10.1093/bioinformatics/bti499},
  abstract = {Motivation: In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the `true' prediction error of a prediction model in the presence of feature selection.
Results: For small studies where features are selected from thousands of candidates, the resubstitution and simple split-sample estimates are seriously biased. In these small samples, leave-one-out cross-validation (LOOCV), 10-fold cross-validation (CV) and the .632+ bootstrap have the smallest bias for diagonal discriminant analysis, nearest neighbor and classification trees. LOOCV and 10-fold CV have the smallest bias for linear discriminant analysis. Additionally, LOOCV, 5- and 10-fold CV, and the .632+ bootstrap have the lowest mean square error. The .632+ bootstrap is quite biased in small sample sizes with strong signal-to-noise ratios. Differences in performance among resampling methods are reduced as the number of specimens available increase.
Contact: annette.molinaro@yale.edu
Supplementary Information: A complete compilation of results and R code for simulations and analyses are available in Molinaro et al. (2005) (http://linus.nci.nih.gov/brb/TechReport.htm).},
  language = {en},
  timestamp = {2016-08-19T13:50:52Z},
  number = {15},
  urldate = {2015-12-27},
  journal = {Bioinformatics},
  author = {Molinaro, Annette M. and Simon, Richard and Pfeiffer, Ruth M.},
  month = jan,
  year = {2005},
  pages = {3301--3307},
  file = {Molinaro et al_2005_Prediction error estimation.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/K7KIACIA/Molinaro et al_2005_Prediction error estimation.pdf:application/pdf;Molinaro et al_2005_Prediction error estimation.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/UFQXIRFW/Molinaro et al_2005_Prediction error estimation.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/B3EV448D/3301.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/ZH2Z4QVF/3301.html:text/html},
  pmid = {15905277}
}

@article{arlot_survey_2010,
  title = {A survey of cross-validation procedures for model selection},
  volume = {4},
  issn = {1935-7516},
  doi = {10.1214/09-SS054},
  abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
  language = {EN},
  timestamp = {2015-12-27T10:07:01Z},
  urldate = {2015-12-21},
  journal = {Statistics Surveys},
  author = {Arlot, Sylvain and Celisse, Alain},
  year = {2010},
  keywords = {cross-validation,leave-one-out,Model selection},
  pages = {40--79},
  file = {Arlot_Celisse_2010_A survey of cross-validation procedures for model selection.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/X5N3KUDP/Arlot_Celisse_2010_A survey of cross-validation procedures for model selection.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/AHQ72DX9/1268143839.html:text/html},
  mrnumber = {MR2602303},
  zmnumber = {1190.62080}
}

@article{nadler_finite_2008,
  title = {Finite sample approximation results for principal component analysis: {{A}} matrix perturbation approach},
  volume = {36},
  issn = {0090-5364, 2168-8966},
  shorttitle = {Finite sample approximation results for principal component analysis},
  doi = {10.1214/08-AOS618},
  abstract = {Principal component analysis (PCA) is a standard tool for dimensional reduction of a set of n observations (samples), each with p variables. In this paper, using a matrix perturbation approach, we study the nonasymptotic relation between the eigenvalues and eigenvectors of PCA computed on a finite sample of size n, and those of the limiting population PCA as n\ensuremath{\rightarrow{}\infty}. As in machine learning, we present a finite sample theorem which holds with high probability for the closeness between the leading eigenvalue and eigenvector of sample PCA and population PCA under a spiked covariance model. In addition, we also consider the relation between finite sample PCA and the asymptotic results in the joint limit p, n\ensuremath{\rightarrow{}\infty}, with p/n=c. We present a matrix perturbation view of the ``phase transition phenomenon,'' and a simple linear-algebra based derivation of the eigenvalue and eigenvector overlap in this asymptotic limit. Moreover, our analysis also applies for finite p, n where we show that although there is no sharp phase transition as in the infinite case, either as a function of noise level or as a function of sample size n, the eigenvector of sample PCA may exhibit a sharp ``loss of tracking,'' suddenly losing its relation to the (true) eigenvector of the population PCA matrix. This occurs due to a crossover between the eigenvalue due to the signal and the largest eigenvalue due to noise, whose eigenvector points in a random direction.},
  language = {EN},
  timestamp = {2016-01-12T20:00:57Z},
  number = {6},
  urldate = {2016-01-12},
  journal = {The Annals of Statistics},
  author = {Nadler, Boaz},
  month = dec,
  year = {2008},
  keywords = {matrix perturbation,phase transition,principal component analysis,random matrix theory,spiked covariance model},
  pages = {2791--2817},
  file = {Nadler_2008_Finite sample approximation results for principal component analysis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/K8SXK8TZ/Nadler_2008_Finite sample approximation results for principal component analysis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/ZV3UZPQ7/1231165185.html:text/html},
  mrnumber = {MR2485013},
  zmnumber = {05503376}
}

@article{besag_sequential_1991,
  title = {Sequential {{Monte Carlo}} p-values},
  volume = {78},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/78.2.301},
  abstract = {The assessment of statistical significance by Monte Carlo simulation may be costly in computer time. This paper looks at a number of ways of calculating exact Monte Carlo p-values by sequential sampling. Such p-values are shown to have properties similar to those obtained by sampling with a fixed sample size. Both standard and generalized Monte Carlo procedures are discussed and, in particular, a sequential method is proposed for dealing with situations in which values can only be conveniently generated using a Markov chain, conditioned to pass through the observed data.},
  language = {en},
  timestamp = {2016-08-19T13:51:18Z},
  number = {2},
  urldate = {2016-01-28},
  journal = {Biometrika},
  author = {Besag, Julian and Clifford, Peter},
  month = jan,
  year = {1991},
  keywords = {Markov Chain,Monte Carlo testing,p-value,Sequential estimation,Sequential test,Significance test},
  pages = {301--304},
  file = {Besag_Clifford_1991_Sequential Monte Carlo p-values.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/GRPAZN2F/Besag_Clifford_1991_Sequential Monte Carlo p-values.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/74PTK7ZA/301.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/T9S4S4RK/301.html:text/html}
}

@article{srivastava_testing_2013,
  title = {On testing the equality of mean vectors in high dimension},
  volume = {17},
  issn = {2228-4699},
  doi = {10.12697/ACUTM.2013.17.03},
  abstract = {In this article, we review various tests that have been proposed in the literature for testing the equality of several mean vectors. In particular, it includes testing the equality of two mean vectors, the so-called two-sample problem as well as that of testing the equality of several mean vectors, the so-called multivariate analysis of variance or MANOVA problem. The total sample size, however, may be less than the dimension of the mean vectors, and so usual tests cannot be used. Powers of these tests are compared using simulation.},
  language = {en},
  timestamp = {2016-02-01T12:13:48Z},
  number = {1},
  urldate = {2015-06-01},
  journal = {Acta et Commentationes Universitatis Tartuensis de Mathematica},
  author = {Srivastava, Muni S.},
  month = jun,
  year = {2013},
  keywords = {Equality of two mean vectors,high dimensional,inequality of two covariance matrices,Multivariate analysis of variance,sample smaller than dimension},
  pages = {31--56},
  file = {Srivastava_2013_On testing the equality of mean vectors in high dimension.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/CES3WQ63/Srivastava_2013_On testing the equality of mean vectors in high dimension.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/52BRBUCE/ACUTM.2013.17.html:text/html}
}

@article{blair_study_1994,
  title = {A {{Study}} of {{Multivariate Permutation Tests Which May Replace Hotelling}}'s {{T2 Test}} in {{Prescribed Circumstances}}},
  volume = {29},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr2902_2},
  abstract = {Multivariate permutation tests are described and studied which may be profitably substituted for Hotelling's one-sample P test in situations commonly arising in behavioral science research. These tests (a) may be computed even when the number of variables exceeds the number of subjects, (b) are distribution-free, (c) may be tailored for sensitivity to specific treatment alternatives, and (d) provide one-sided as well as two-sided tests of hypotheses. Power comparisons were made between the permutation tests and Hotelling's T(2) test under a variety of treatment effect model, correlation structure and number of variables combinations. Results show that the permutation tests have significant power advantages over the T(2) in a variety of circumstances, but may have considerably less power in others.},
  language = {eng},
  timestamp = {2016-02-06T10:27:37Z},
  number = {2},
  journal = {Multivariate Behavioral Research},
  author = {Blair, R. C. and Higgins, J. J. and Karniski, W. and Kromrey, J. D.},
  month = apr,
  year = {1994},
  pages = {141--163},
  file = {Blair et al_1994_A Study of Multivariate Permutation Tests Which May Replace Hotelling's T2 Test.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/J6QZ69DU/Blair et al_1994_A Study of Multivariate Permutation Tests Which May Replace Hotelling's T2 Test.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/XGSVCHPR/s15327906mbr2902_2.html:text/html},
  pmid = {26745025}
}

@incollection{olivetti_induction_2012,
  series = {Lecture Notes in Computer Science},
  title = {Induction in {{Neuroscience}} with {{Classification}}: {{Issues}} and {{Solutions}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-34712-2 978-3-642-34713-9},
  shorttitle = {Induction in {{Neuroscience}} with {{Classification}}},
  abstract = {Machine learning and pattern recognition techniques are increasingly adopted in neuroimaging-based neuroscience research. In many applications a classifier is trained on brain data in order to predict a variable of interest. Two leading examples are brain decoding and clinical diagnosis. Brain decoding consists of predicting stimuli or mental states from concurrent functional brain data. In clinical diagnosis it is the presence or absence of a given medical condition that is predicted from brain data. Observing accurate classification is considered to support the hypothesis of variable-related information within brain data. In this work we briefly review the literature on statistical tests for this kind of hypothesis testing problem. We claim that the current approaches to this hypothesis testing problem are suboptimal, do not cover all useful settings, and that they could lead to wrong conclusions. We present a more accurate statistical test and provide examples of its superiority.},
  language = {en},
  timestamp = {2016-08-02T19:07:56Z},
  number = {7263},
  urldate = {2016-02-15},
  booktitle = {Machine {{Learning}} and {{Interpretation}} in {{Neuroimaging}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Olivetti, Emanuele and Greiner, Susanne and Avesani, Paolo},
  editor = {Langs, Georg and Rish, Irina and Grosse-Wentrup, Moritz and Murphy, Brian},
  year = {2012},
  keywords = {Computer Applications,Computer Imaging; Vision; Pattern Recognition and Graphics,Data Mining and Knowledge Discovery,Image Processing and Computer Vision,pattern recognition,Probability and Statistics in Computer Science},
  pages = {42--50},
  file = {Olivetti et al_2012_Induction in Neuroscience with Classification.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/DNCZRCNJ/Olivetti et al_2012_Induction in Neuroscience with Classification.pdf:application/pdf;Olivetti et al_2012_Induction in Neuroscience with Classification.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/P89G3D73/Olivetti et al_2012_Induction in Neuroscience with Classification.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/BBENTVTF/978-3-642-34713-9_6.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TF8AQ2N9/978-3-642-34713-9_6.html:text/html},
  doi = {10.1007/978-3-642-34713-9_6}
}

@article{gretton_kernel_2012-1,
  title = {A {{Kernel Two}}-sample {{Test}}},
  volume = {13},
  issn = {1532-4435},
  abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  timestamp = {2016-08-02T19:07:12Z},
  urldate = {2016-02-15},
  journal = {J. Mach. Learn. Res.},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  month = mar,
  year = {2012},
  keywords = {Hypothesis testing,integral probability metric,kernel methods,schema matching,two-sample test,uniform convergence bounds},
  pages = {723--773},
  file = {Gretton et al_2012_A Kernel Two-Sample Test.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/GSTU3PBK/Gretton et al_2012_A Kernel Two-Sample Test.pdf:application/pdf;Gretton et al_2012_A Kernel Two-sample Test.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/QEIC73GI/Gretton et al_2012_A Kernel Two-sample Test.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/FE27P35Z/gretton12a.html:text/html}
}

@article{ramdas_classification_2016,
  title = {Classification {{Accuracy}} as a {{Proxy}} for {{Two Sample Testing}}},
  abstract = {When data analysts train a classifier and check if its accuracy is significantly different from random guessing, they are implicitly and indirectly performing a hypothesis test (two sample testing) and it is of importance to ask whether this indirect method for testing is statistically optimal or not. Given that hypothesis tests attempt to maximize statistical power subject to a bound on the allowable false positive rate, while prediction attempts to minimize statistical risk on future predictions on unseen data, we wish to study whether a predictive approach for an ultimate aim of testing is prudent. We formalize this problem by considering the two-sample mean-testing setting where one must determine if the means of two Gaussians (with known and equal covariance) are the same or not, but the analyst indirectly does so by checking whether the accuracy achieved by Fisher's LDA classifier is significantly different from chance or not. Unexpectedly, we find that the asymptotic power of LDA's sample-splitting classification accuracy is actually minimax rate-optimal in terms of problem-dependent parameters. Since prediction is commonly thought to be harder than testing, it might come as a surprise to some that solving a harder problem does not create a information-theoretic bottleneck for the easier one. On the flip side, even though the power is rate-optimal, our derivation suggests that it may be worse by a small constant factor; hence practitioners must be wary of using (admittedly flexible) prediction methods on disguised testing problems.},
  timestamp = {2016-02-15T08:29:52Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02210},
  primaryClass = {cs, math, stat},
  urldate = {2016-02-09},
  journal = {arXiv:1602.02210 [cs, math, stat]},
  author = {Ramdas, Aaditya and Singh, Aarti and Wasserman, Larry},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {Ramdas et al_2016_Classification Accuracy as a Proxy for Two Sample Testing.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/MD57BQS4/Ramdas et al_2016_Classification Accuracy as a Proxy for Two Sample Testing.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/ANXGBH2J/1602.html:text/html}
}

@article{srivastava_multivariate_2007,
  title = {Multivariate {{Theory}} for {{Analyzing High Dimensional Data}}},
  volume = {37},
  doi = {10.14490/jjss.37.53},
  abstract = {In this article, we develop a multivariate theory for analyzing multivariate datasets that have fewer observations than dimensions. More specifically, we consider the problem of testing the hypothesis that the mean vector \ensuremath{\mu} of a p-dimensional random vector x is a zero vector where N, the number of independent observations on x, is less than the dimension p. It is assumed that x is normally distributed with mean vector \ensuremath{\mu} and unknown nonsingular covariance matrix \ensuremath{\sum}. We propose the test statistic F+ = n-2 (p - n + 1) N \textasciimacron{}x\ensuremath{'}S+\textasciimacron{}x, where n = N - 1 \ensuremath{<} p, \textasciimacron{}x and S are the sample mean vector and the sample covariance matrix respectively, and S+ is the Moore-Penrose inverse of S. It is shown that a suitably normalized version of the F+ statistic is asymptotically normally distributed under the hypothesis. The asymptotic non-null distribution in one sample case is given. The case when the covariance matrix \ensuremath{\sum} is singular of rank r but the sample size N is larger than r is also considered. The corresponding results for the case of two-samples and k samples, known as MANOVA, are given.},
  timestamp = {2016-02-17T04:59:33Z},
  number = {1},
  journal = {Journal of the Japan Statistical Society},
  author = {Srivastava, M. S.},
  year = {2007},
  keywords = {Distribution of test statistics,DNA microarray data,Fewer observations than dimension,Multivariate analysis of variance,Singular Wishart},
  pages = {53--86},
  file = {Srivastava_2007_Multivariate Theory for Analyzing High Dimensional Data.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/RN68APQJ/Srivastava_2007_Multivariate Theory for Analyzing High Dimensional Data.pdf:application/pdf}
}

@article{mumford_deconvolving_2012,
  title = {Deconvolving {{BOLD}} activation in event-related designs for multivoxel pattern classification analyses},
  volume = {59},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2011.08.076},
  abstract = {Use of multivoxel pattern analysis (MVPA) to predict the cognitive state of a subject during task performance has become a popular focus of fMRI studies. The input to these analyses consists of activation patterns corresponding to different tasks or stimulus types. These activation patterns are fairly straightforward to calculate for blocked trials or slow event-related designs, but for rapid event-related designs the evoked BOLD signal for adjacent trials will overlap in time, complicating the identification of signal unique to specific trials. Rapid event-related designs are often preferred because they allow for more stimuli to be presented and subjects tend to be more focused on the task, and thus it would be beneficial to be able to use these types of designs in MVPA analyses. The present work compares 8 different models for estimating trial-by-trial activation patterns for a range of rapid event-related designs varying by interstimulus interval and signal-to-noise ratio. The most effective approach obtains each trial's estimate through a general linear model including a regressor for that trial as well as another regressor for all other trials. Through the analysis of both simulated and real data we have found that this model shows some improvement over the standard approaches for obtaining activation patterns. The resulting trial-by-trial estimates are more representative of the true activation magnitudes, leading to a boost in classification accuracy in fast event-related designs with higher signal-to-noise. This provides the potential for fMRI studies that allow simultaneous optimization of both univariate and MVPA approaches.},
  timestamp = {2016-03-10T15:50:34Z},
  number = {3},
  urldate = {2016-03-10},
  journal = {NeuroImage},
  author = {Mumford, Jeanette A. and Turner, Benjamin O. and Ashby, F. Gregory and Poldrack, Russell A.},
  month = feb,
  year = {2012},
  keywords = {Beta series estimation,classification analysis,Functional Magnetic Resonance Imaging,MVPA,Rapid event-related design},
  pages = {2636--2643},
  file = {Mumford et al_2012_Deconvolving BOLD activation in event-related designs for multivoxel pattern.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/6F6XSHTW/Mumford et al_2012_Deconvolving BOLD activation in event-related designs for multivoxel pattern.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/2W45BQQJ/S1053811911010081.html:text/html}
}

@article{mumford_impact_2014,
  title = {The impact of study design on pattern estimation for single-trial multivariate pattern analysis},
  volume = {103},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.09.026},
  abstract = {A prerequisite for a pattern analysis using functional magnetic resonance imaging (fMRI) data is estimating the patterns from time series data, which then are input into the pattern analysis. Here we focus on how the combination of study design (order and spacing of trials) with pattern estimator impacts the Type I error rate of the subsequent pattern analysis. When Type I errors are inflated, the results are no longer valid, so this work serves as a guide for designing and analyzing MVPA studies with controlled false positive rates. The MVPA strategies examined are pattern classification and similarity, utilizing single trial activation patterns from the same functional run. Primarily focusing on the Least Squares Single and Least Square All pattern estimators, we show that collinearities in the models, along with temporal autocorrelation, can cause false positive correlations between activation pattern estimates that adversely impact the false positive rates of pattern similarity and classification analyses. It may seem intuitive that increasing the interstimulus interval (ISI) would alleviate this issue, but remaining weak correlations between activation patterns persist and have a strong influence in pattern similarity analyses. Pattern similarity analyses using only activation patterns estimated from the same functional run of data are susceptible to inflated false positives unless trials are randomly ordered, with a different randomization for each subject. In other cases, where there is any structure to trial order, valid pattern similarity analysis results can only be obtained if similarity computations are restricted to pairs of activation patterns from independent runs. Likewise, for pattern classification, false positives are minimized when the testing and training sets in cross validation do not contain patterns estimated from the same run.},
  timestamp = {2016-03-20T21:16:42Z},
  urldate = {2016-03-20},
  journal = {NeuroImage},
  author = {Mumford, Jeanette A. and Davis, Tyler and Poldrack, Russell A.},
  month = dec,
  year = {2014},
  keywords = {False positive rate,fMRI,MVPA,pattern classification,Pattern similarity},
  pages = {130--138},
  file = {Mumford et al_2014_The impact of study design on pattern estimation for single-trial multivariate.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/ZXGTRKTZ/Mumford et al_2014_The impact of study design on pattern estimation for single-trial multivariate.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/IXZRT8C4/S105381191400768X.html:text/html}
}

@article{hotelling_generalization_1931,
  title = {The {{Generalization}} of {{Student}}'s {{Ratio}}},
  volume = {2},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732979},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  timestamp = {2016-04-10T07:33:59Z},
  number = {3},
  urldate = {2015-06-19},
  journal = {The Annals of Mathematical Statistics},
  author = {Hotelling, Harold},
  month = aug,
  year = {1931},
  pages = {360--378},
  file = {Hotelling_1931_The Generalization of Student's Ratio.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/2HEC4VV3/Hotelling_1931_The Generalization of Student's Ratio.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/MRVQTBMM/1177732979.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/VH7S648S/1177732979.html:text/html}
}

@book{fujikoshi_multivariate_2011,
  title = {Multivariate {{Statistics}}: {{High-Dimensional}} and {{Large-Sample Approximations}}},
  isbn = {978-0-470-53986-6},
  shorttitle = {Multivariate {{Statistics}}},
  abstract = {A comprehensive examination of high-dimensional analysis of multivariate methods and their real-world applications Multivariate Statistics: High-Dimensional and Large-Sample Approximations is the first book of its kind to explore how classical multivariate methods can be revised and used in place of conventional statistical tools. Written by prominent researchers in the field, the book focuses on high-dimensional and large-scale approximations and details the many basic multivariate methods used to achieve high levels of accuracy. The authors begin with a fundamental presentation of the basic tools and exact distributional results of multivariate statistics, and, in addition, the derivations of most distributional results are provided. Statistical methods for high-dimensional data, such as curve data, spectra, images, and DNA microarrays, are discussed. Bootstrap approximations from a methodological point of view, theoretical accuracies in MANOVA tests, and model selection criteria are also presented. Subsequent chapters feature additional topical coverage including:  High-dimensional approximations of various statistics High-dimensional statistical methods Approximations with computable error bound Selection of variables based on model selection approach Statistics with error bounds and their appearance in discriminant analysis, growth curve models, generalized linear models, profile analysis, and multiple comparison  Each chapter provides real-world applications and thorough analyses of the real data. In addition, approximation formulas found throughout the book are a useful tool for both practical and theoretical statisticians, and basic results on exact distributions in multivariate analysis are included in a comprehensive, yet accessible, format. Multivariate Statistics is an excellent book for courses on probability theory in statistics at the graduate level. It is also an essential reference for both practical and theoretical statisticians who are interested in multivariate analysis and who would benefit from learning the applications of analytical probabilistic methods in statistics.},
  language = {en},
  timestamp = {2016-05-01T07:19:13Z},
  publisher = {{John Wiley \& Sons}},
  author = {Fujikoshi, Yasunori and Ulyanov, Vladimir V. and Shimizu, Ryoichi},
  month = aug,
  year = {2011},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{pernet_human_2015,
  title = {The human voice areas: {{Spatial}} organization and inter-individual variability in temporal and extra-temporal cortices},
  volume = {119},
  issn = {1053-8119},
  shorttitle = {The human voice areas},
  doi = {10.1016/j.neuroimage.2015.06.050},
  abstract = {fMRI studies increasingly examine functions and properties of non-primary areas of human auditory cortex. However there is currently no standardized localization procedure to reliably identify specific areas across individuals such as the standard `localizers' available in the visual domain. Here we present an fMRI `voice localizer' scan allowing rapid and reliable localization of the voice-sensitive `temporal voice areas' (TVA) of human auditory cortex. We describe results obtained using this standardized localizer scan in a large cohort of normal adult subjects. Most participants (94\%) showed bilateral patches of significantly greater response to vocal than non-vocal sounds along the superior temporal sulcus/gyrus (STS/STG). Individual activation patterns, although reproducible, showed high inter-individual variability in precise anatomical location. Cluster analysis of individual peaks from the large cohort highlighted three bilateral clusters of voice-sensitivity, or ``voice patches'' along posterior (TVAp), mid (TVAm) and anterior (TVAa) STS/STG, respectively. A series of extra-temporal areas including bilateral inferior prefrontal cortex and amygdalae showed small, but reliable voice-sensitivity as part of a large-scale cerebral voice network. Stimuli for the voice localizer scan and probabilistic maps in MNI space are available for download.},
  timestamp = {2016-05-03T07:43:38Z},
  urldate = {2015-08-13},
  journal = {NeuroImage},
  author = {Pernet, Cyril R. and McAleer, Phil and Latinus, Marianne and Gorgolewski, Krzysztof J. and Charest, Ian and Bestelmeyer, Patricia E. G. and Watson, Rebecca H. and Fleming, David and Crabbe, Frances and Valdes-Sosa, Mitchell and Belin, Pascal},
  month = oct,
  year = {2015},
  keywords = {Amygdala,Auditory Cortex,Functional Magnetic Resonance Imaging,Inferior prefrontal cortex,Superior temporal gyrus,Superior temporal sulcus,Voice},
  pages = {164--174},
  file = {Pernet et al_2015_The human voice areas.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/K5P9IBIC/Pernet et al_2015_The human voice areas.pdf:application/pdf;Pernet et al_2015_The human voice areas.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TMDRCDKR/Pernet et al_2015_The human voice areas.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/CPWNAN2U/S1053811915005558.html:text/html;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/DUHB2G28/S1053811915005558.html:text/html}
}

@book{breiman_classification_1984,
  title = {Classification and {{Regression Trees}}},
  isbn = {978-0-412-04841-8},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
  language = {en},
  timestamp = {2016-05-17T05:01:34Z},
  publisher = {{Taylor \& Francis}},
  author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J. and Olshen, R. A.},
  month = jan,
  year = {1984},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / General,Psychology / Research \& Methodology,Psychology / Research \& Methodology}
}


@book{hastie_elements_2003,
	title={The elements of statistical learning},
	author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	volume={1},
	year={2001},
	publisher={Springer series in statistics New York}
}

@article{ojala_permutation_2010,
  title = {Permutation {{Tests}} for {{Studying Classifier Performance}}},
  volume = {11},
  issn = {ISSN 1533-7928},
  timestamp = {2016-07-04T17:58:55Z},
  number = {Jun},
  urldate = {2016-07-04},
  journal = {Journal of Machine Learning Research},
  author = {Ojala, Markus and Garriga, Gemma C.},
  year = {2010},
  pages = {1833--1863},
  file = {Ojala_Garriga_2010_Permutation Tests for Studying Classifier Performance.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/3JAFGW2X/Ojala_Garriga_2010_Permutation Tests for Studying Classifier Performance.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/Z5T8DEE6/ojala10a.html:text/html}
}

@unpublished{varoquaux_assessing_2016,
  title = {Assessing and tuning brain decoders: cross-validation, caveats, and guidelines},
  shorttitle = {Assessing and tuning brain decoders},
  abstract = {Decoding, ie prediction from brain images or signals, calls for empirical evaluation of its predictive power. Such evaluation is achieved via cross-validation, a method also used to tune decoders' hyper-parameters. This paper is a review on cross-validation procedures for decoding in neuroimaging. It includes a didactic overview of the relevant theoretical considerations. Practical aspects are highlighted with an extensive empirical study of the common decoders in within-and across-subject predictions, on multiple datasets \textendash{}anatomical and functional MRI and MEG\textendash{} and simulations. Theory and experiments outline that the popular " leave-one-out " strategy leads to unstable and biased estimates, and a repeated random splits method should be preferred. Experiments outline the large error bars of cross-validation in neuroimaging settings: typical confidence intervals of 10\%. Nested cross-validation can tune decoders' parameters while avoiding circularity bias. However we find that it can be more favorable to use sane defaults, in particular for non-sparse decoders.},
  timestamp = {2016-07-04T21:08:54Z},
  urldate = {2016-07-04},
  author = {Varoquaux, Ga{\"e}l and Raamana, Pradeep Reddy and Engemann, Denis and Hoyos-Idrobo, Andr{\'e}s and Schwartz, Yannick and Thirion, Bertrand},
  month = jun,
  year = {2016},
  note = {working paper or preprint},
  keywords = {Bagging,cross-validation,Decoding,fMRI,Model selection,MVPA,sparse},
  file = {Varoquaux et al_2016_Assessing and tuning brain decoders.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/GM2G3W2W/Varoquaux et al_2016_Assessing and tuning brain decoders.pdf:application/pdf;HAL Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/XSPCKFJR/hal-01332785.html:text/html}
}

@inproceedings{golland_permutation_2003,
  title = {Permutation tests for classification: towards statistical significance in image-based studies},
  volume = {3},
  shorttitle = {Permutation tests for classification},
  timestamp = {2016-07-05T10:33:44Z},
  urldate = {2016-07-05},
  booktitle = {{{IPMI}}},
  publisher = {{Springer}},
  author = {Golland, Polina and Fischl, Bruce},
  year = {2003},
  pages = {330--341},
  file = {Golland_Fischl_2003_Permutation tests for classification.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/KIFCRANG/Golland_Fischl_2003_Permutation tests for classification.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/5PZMV3JR/b11820.html:text/html}
}

@article{radmacher_paradigm_2002,
  title = {A {{Paradigm}} for {{Class Prediction Using Gene Expression Profiles}}},
  volume = {9},
  issn = {1066-5277},
  doi = {10.1089/106652702760138592},
  abstract = {We propose a general framework for prediction of predefined tumor classes using gene expression profiles from microarray experiments. The framework consists of 1) evaluating the appropriateness of class          prediction for the given data set, 2) selecting the prediction method, 3) performing cross-validated class prediction, and 4) assessing the significance of prediction results by permutation testing. We          describe an application of the prediction paradigm to gene expression profiles from human breast cancers, with specimens classified as positive or negative for BRCA1 mutations and also for BRCA2          mutations. In both cases, the accuracy of class prediction was statistically significant when compared to the accuracy of prediction expected by chance. The framework proposed here for the application of          class prediction is designed to reduce the occurrence of spurious findings, a legitimate concern for high-dimensional microarray data. The prediction paradigm will serve as a good framework for comparing          different prediction methods and may accelerate the development of molecular classifiers that are clinically useful.},
  timestamp = {2016-07-21T08:48:15Z},
  number = {3},
  urldate = {2016-07-21},
  journal = {Journal of Computational Biology},
  author = {Radmacher, Michael D. and McShane, Lisa M. and Simon, Richard},
  month = jun,
  year = {2002},
  pages = {505--511},
  file = {Radmacher et al_2002_A Paradigm for Class Prediction Using Gene Expression Profiles.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/RE86PPTC/Radmacher et al_2002_A Paradigm for Class Prediction Using Gene Expression Profiles.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/MP5I69DB/106652702760138592.html:text/html}
}

@article{nadeau_inference_????,
  title = {Inference for the {{Generalization Error}}},
  volume = {52},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1024068626366},
  abstract = {In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.},
  language = {en},
  timestamp = {2016-07-21T08:56:33Z},
  number = {3},
  urldate = {2016-07-21},
  journal = {Machine Learning},
  author = {Nadeau, Claude and Bengio, Yoshua},
  pages = {239--281},
  file = {Nadeau_Bengio_Inference for the Generalization Error.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TDV3832D/Nadeau_Bengio_Inference for the Generalization Error.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/7SKKZM3H/A1024068626366.html:text/html}
}

@article{jiang_calculating_2008,
  title = {Calculating confidence intervals for prediction error in microarray classification using resampling},
  volume = {7},
  timestamp = {2016-07-21T09:02:59Z},
  number = {1},
  urldate = {2016-07-21},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  author = {Jiang, Wenyu and Varma, Sudhir and Simon, Richard},
  year = {2008},
  file = {Jiang et al_2008_Calculating confidence intervals for prediction error in microarray.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/XSSU5F5W/Jiang et al_2008_Calculating confidence intervals for prediction error in microarray.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/4H86NDA9/sagmb.2008.7.1.1322.html:text/html}
}

@article{gilron_quantifying_2016,
  title = {Quantifying spatial pattern similarity in multivariate analysis using functional anisotropy},
  abstract = {Multivoxel pattern analysis (MVPA) has gained enormous popularity in the neuroimaging community over the past few years. At the group level, most MVPA studies adopt an "information based" approach in which the sign of the effect of individual subjects is discarded and a non-directional summary statistic is carried over to the second level. This is in contrast to a directional "activation based" approach which is typical in univariate group level analysis, in which both signal magnitude and sign are taken into account. The transition from examining effects in one voxel at a time vs. several voxels (univariate vs. multivariate) has thus tacitly entailed a transition from directional to non-directional signal definition at the group level. While a directional MVPA approach implies that individuals share multivariate spatial patterns of activity, in a non-directional approach each individual may have a distinct spatial pattern of activity. Here we show using an experimental dataset that indeed directional and non-directional MVPA approaches uncover distinct brain regions with some overlap. Moreover, we developed a descriptive measure to quantify the degree to which subjects share spatial patterns of activity. Our approach is based on adapting the Fractional Anisotropy (FA) measure, originally developed for examining diffusion MRI signals, in a novel way to quantify the degree to which subjects share a spatial pattern of activity. We term this measure "Functional Anisotropy" (FuA). Applying FuA to an auditory task, we found higher values in primary auditory regions compared to secondary and control regions. This highlights the potential of the FuA measure in second-level MVPA analysis to detect differences in the consistency of spatial patterns across subjects and their relationship to functional domains in the brain.},
  timestamp = {2016-07-26T04:06:39Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.03482},
  primaryClass = {q-bio},
  urldate = {2016-07-26},
  journal = {arXiv:1605.03482 [q-bio]},
  author = {Gilron, Roee and Rosenblatt, Jonathan and Koyejo, Oluwasanmi and Poldrack, Russell A. and Mukamel, Roy},
  month = may,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods},
  file = {Gilron et al_2016_Quantifying spatial pattern similarity in multivariate analysis using.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/3AE7RZEB/Gilron et al_2016_Quantifying spatial pattern similarity in multivariate analysis using.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/F5VJ3TWX/1605.html:text/html}
}

@article{hemerik_exact_2014,
  title = {Exact testing with random permutations},
  abstract = {The way in which random permutations have been used in various permutation-based methods leads to anti-conservativeness, especially in multiple testing contexts. Problems arise in particular for Westfall and Young's maxT method, a more recent method by Meinshausen and a global test that we introduce. We illustrate this using simulations. We solve the problem of anti-conservativeness by proving that an exact test is obtained, if the identity map is added to the randomly drawn permutations.},
  timestamp = {2016-08-02T19:07:59Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.7565},
  primaryClass = {math, stat},
  urldate = {2016-07-26},
  journal = {arXiv:1411.7565 [math, stat]},
  author = {Hemerik, Jesse and Goeman, Jelle},
  month = nov,
  year = {2014},
  keywords = {62G09,Mathematics - Statistics Theory},
  file = {Hemerik_Goeman_2014_Exact testing with random permutations.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/6NB6FSI9/Hemerik_Goeman_2014_Exact testing with random permutations.pdf:application/pdf;Hemerik_Goeman_2014_Exact testing with random permutations.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/DK287DKI/Hemerik_Goeman_2014_Exact testing with random permutations.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/494EESV4/1411.html:text/html;arXiv.org Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/GKQ23FZS/1411.html:text/html}
}

@article{pereira_machine_2009,
  series = {Mathematics in Brain Imaging},
  title = {Machine learning classifiers and {{fMRI}}: {{A}} tutorial overview},
  volume = {45},
  issn = {1053-8119},
  shorttitle = {Machine learning classifiers and {{fMRI}}},
  doi = {10.1016/j.neuroimage.2008.11.007},
  abstract = {Interpreting brain image experiments requires analysis of complex, multivariate data. In recent years, one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli, mental states, behaviours and other variables of interest from fMRI data and thereby show the data contain information about them. In this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results, illustrating each point from a case study. Furthermore, we show how, in addition to answering the question of `is there information about a variable of interest' (pattern discrimination), classifiers can be used to tackle other classes of question, namely `where is the information' (pattern localization) and `how is that information encoded' (pattern characterization).},
  timestamp = {2016-07-28T05:45:34Z},
  number = {1, Supplement 1},
  urldate = {2016-07-28},
  journal = {NeuroImage},
  author = {Pereira, Francisco and Mitchell, Tom and Botvinick, Matthew},
  month = mar,
  year = {2009},
  pages = {S199--S209},
  file = {Pereira et al_2009_Machine learning classifiers and fMRI.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/BNQ4Z2SM/Pereira et al_2009_Machine learning classifiers and fMRI.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SIQXCRCC/S1053811908012263.html:text/html}
}

@article{simon_pitfalls_2003,
  title = {Pitfalls in the {{Use}} of {{DNA Microarray Data}} for {{Diagnostic}} and {{Prognostic Classification}}},
  volume = {95},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/95.1.14},
  language = {en},
  timestamp = {2016-08-19T13:50:55Z},
  number = {1},
  urldate = {2016-07-28},
  journal = {Journal of the National Cancer Institute},
  author = {Simon, Richard and Radmacher, Michael D. and Dobbin, Kevin and McShane, Lisa M.},
  month = jan,
  year = {2003},
  pages = {14--18},
  file = {Simon et al_2003_Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/NKJU5FII/Simon et al_2003_Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic.pdf:application/pdf;Simon et al_2003_Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/S2N3HXB2/Simon et al_2003_Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TPKZHDS4/14.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/UA3MHPBV/14.html:text/html},
  pmid = {12509396}
}

@article{fu_estimating_2005,
  title = {Estimating misclassification error with small samples via bootstrap cross-validation},
  volume = {21},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/bti294},
  abstract = {Motivation: Estimation of misclassification error has received increasing attention in clinical diagnosis and bioinformatics studies, especially in small sample studies with microarray data. Current error estimation methods are not satisfactory because they either have large variability (such as leave-one-out cross-validation) or large bias (such as resubstitution and leave-one-out bootstrap). While small sample size remains one of the key features of costly clinical investigations or of microarray studies that have limited resources in funding, time and tissue materials, accurate and easy-to-implement error estimation methods for small samples are desirable and will be beneficial.
Results: A bootstrap cross-validation method is studied. It achieves accurate error estimation through a simple procedure with bootstrap resampling and only costs computer CPU time. Simulation studies and applications to microarray data demonstrate that it performs consistently better than its competitors. This method possesses several attractive properties: (1) it is implemented through a simple procedure; (2) it performs well for small samples with sample size, as small as 16; (3) it is not restricted to any particular classification rules and thus applies to many parametric or non-parametric methods.
Contact: wfu@stat.tamu.edu},
  language = {en},
  timestamp = {2016-07-28T10:16:50Z},
  number = {9},
  urldate = {2016-07-28},
  journal = {Bioinformatics},
  author = {Fu, Wenjiang J. and Carroll, Raymond J. and Wang, Suojin},
  month = jan,
  year = {2005},
  pages = {1979--1986},
  file = {Fu et al_2005_Estimating misclassification error with small samples via bootstrap.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/32HCSCVH/Fu et al_2005_Estimating misclassification error with small samples via bootstrap.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/AUPFQ73E/1979.html:text/html},
  pmid = {15691862}
}

@article{bzdok_neuroimaging_2016,
  title = {Neuroimaging {{Research}}: {{From Null-Hypothesis Falsification}} to {{Out}}-of-sample {{Generalization}}},
  shorttitle = {Neuroimaging {{Research}}},
  abstract = {Brain imaging technology has boosted the quantification of neurobiological phenomena underlying human mental operations and their disturbances. Since its inception, drawing inference on neurophysiological effects hinged on classical statistical methods, especially, the general linear model. The tens of thousands variables per brain scan were routinely tackled by independent statistical tests on each voxel. This circumvented the curse of dimensionality in exchange for neurobiologically imperfect observation units, a challenging multiple comparisons problem, and limited scaling to currently growing data repositories. Yet, the always-bigger information granularity of neuroimaging data repositories has lunched a rapidly increasing adoption of statistical learning algorithms. These scale naturally to high-dimensional data, extract models from data rather than prespecifying them, and are empirically evaluated for extrapolation to unseen data. The present paper portrays commonalities and differences between long-standing classical inference and upcoming generalization inference relevant for conducting neuroimaging research.},
  timestamp = {2016-07-31T15:42:35Z},
  urldate = {2016-07-31},
  journal = {Educational and Psychological Measurement},
  author = {Bzdok, Danilo and Varoquaux, Ga{\"e}l and Thirion, Bertrand},
  month = aug,
  year = {2016},
  file = {Bzdok et al_2016_Neuroimaging Research.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SVUEEE65/Bzdok et al_2016_Neuroimaging Research.pdf:application/pdf;HAL Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SCITXXGU/hal-01338313.html:text/html}
}

@article{wu_genome-wide_2009,
  title = {Genome-wide association analysis by lasso penalized logistic regression},
  volume = {25},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btp041},
  abstract = {Motivation: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.
Method: The present article evaluates the performance of lasso penalized logistic regression in case\textendash{}control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.
Results: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.
Availability: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.
Contact: klange@ucla.edu
Supplementary information: Supplementary data are available at Bioinformatics online.},
  language = {en},
  timestamp = {2016-08-02T19:07:32Z},
  number = {6},
  urldate = {2016-07-31},
  journal = {Bioinformatics},
  author = {Wu, Tong Tong and Chen, Yi Fang and Hastie, Trevor and Sobel, Eric and Lange, Kenneth},
  month = mar,
  year = {2009},
  pages = {714--721},
  file = {Wu et al_2009_Genome-wide association analysis by lasso penalized logistic regression.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/2MG95MJR/Wu et al_2009_Genome-wide association analysis by lasso penalized logistic regression.pdf:application/pdf;Wu et al_2009_Genome-wide association analysis by lasso penalized logistic regression.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/Q3ZNM95I/Wu et al_2009_Genome-wide association analysis by lasso penalized logistic regression.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/J3FEGJ9X/714.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/UAKQECU3/714.html:text/html},
  pmid = {19176549}
}

@article{wager_fmri-based_2013,
  title = {An {{fMRI-Based Neurologic Signature}} of {{Physical Pain}}},
  volume = {368},
  issn = {0028-4793},
  doi = {10.1056/NEJMoa1204471},
  abstract = {The experience of pain is poorly understood. The authors describe a neurologic signature that discriminates between the sensations of painful heat and nonpainful heat, is specific to physical pain, and is responsive to the analgesic agent remifentanil.},
  timestamp = {2016-07-31T16:47:07Z},
  number = {15},
  urldate = {2016-07-31},
  journal = {New England Journal of Medicine},
  author = {Wager, Tor D. and Atlas, Lauren Y. and Lindquist, Martin A. and Roy, Mathieu and Woo, Choong-Wan and Kross, Ethan},
  month = apr,
  year = {2013},
  pages = {1388--1397},
  file = {Wager et al_2013_An fMRI-Based Neurologic Signature of Physical Pain.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/B4KJHPBJ/Wager et al_2013_An fMRI-Based Neurologic Signature of Physical Pain.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/8UDMPBKV/NEJMoa1204471.html:text/html},
  pmid = {23574118}
}

@article{gabrieli_prediction_2015,
  title = {Prediction as a {{Humanitarian}} and {{Pragmatic Contribution}} from {{Human Cognitive Neuroscience}}},
  volume = {85},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2014.10.047},
  abstract = {Neuroimaging has greatly enhanced the cognitive neuroscience understanding of the human brain and its variation across individuals (neurodiversity) in both health and disease. Such progress has not yet, however, propelled changes in educational or medical practices that improve people's lives. We review neuroimaging findings in which initial brain measures (neuromarkers) are correlated with or predict future education, learning, and performance in children and adults; criminality; health-related behaviors; and responses to pharmacological or behavioral treatments. Neuromarkers often provide better predictions (neuroprognosis), alone or in combination with other measures, than traditional behavioral measures. With further advances in study designs and analyses, neuromarkers may offer opportunities to personalize educational and clinical practices that lead to better outcomes for people.},
  language = {English},
  timestamp = {2016-07-31T16:47:10Z},
  number = {1},
  urldate = {2016-07-31},
  journal = {Neuron},
  author = {Gabrieli, John D. E. and Ghosh, Satrajit S. and Whitfield-Gabrieli, Susan},
  month = jan,
  year = {2015},
  pages = {11--26},
  file = {Gabrieli et al_2015_Prediction as a Humanitarian and Pragmatic Contribution from Human Cognitive.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/2DQVWC6B/Gabrieli et al_2015_Prediction as a Humanitarian and Pragmatic Contribution from Human Cognitive.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/BVW7PT6C/S0896-6273(14)00967-2.html:text/html},
  pmid = {25569345, 25569345}
}

@article{olivetti_statistical_2014,
  title = {Statistical independence for the evaluation of classifier-based diagnosis},
  volume = {2},
  issn = {2198-4018, 2198-4026},
  doi = {10.1007/s40708-014-0007-6},
  abstract = {Machine learning techniques are increasingly adopted in computer-aided diagnosis. Evaluation methods for classification results that are based on the study of one or more metrics can be unable to distinguish between cases in which the classifier is discriminating the classes from cases in which it is not. In the binary setting, such circumstances can be encountered when data are unbalanced with respect to the diagnostic groups. Having more healthy controls than pathological subjects, datasets meant for diagnosis frequently show a certain degree of unbalancedness. In this work, we propose to recast the evaluation of classification results as a test of statistical independence between the predicted and the actual diagnostic groups. We address the problem within the Bayesian hypothesis testing framework. Different from the standard metrics, the proposed method is able to handle unbalanced data and takes into account the size of the available data. We show experimental evidence of the efficacy of the approach both on simulated data and on real data about the diagnosis of the Attention Deficit Hyperactivity Disorder (ADHD).},
  language = {en},
  timestamp = {2016-07-31T18:57:51Z},
  number = {1},
  urldate = {2016-07-31},
  journal = {Brain Informatics},
  author = {Olivetti, Emanuele and Greiner, Susanne and Avesani, Paolo},
  month = dec,
  year = {2014},
  pages = {13--19},
  file = {Olivetti et al_2014_Statistical independence for the evaluation of classifier-based diagnosis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/WKF38NPF/Olivetti et al_2014_Statistical independence for the evaluation of classifier-based diagnosis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/79SXWP64/s40708-014-0007-6.html:text/html}
}

@incollection{vayatis_auc_2009,
  title = {{{AUC}} optimization and the two-sample problem},
  timestamp = {2016-07-31T19:08:49Z},
  urldate = {2016-07-31},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  publisher = {{Curran Associates, Inc.}},
  author = {Vayatis, Nicolas and Depecker, Marine and Cl{\'e}men{\c c}con, St{\'e}phan J.},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  year = {2009},
  pages = {360--368},
  file = {Vayatis et al_2009_AUC optimization and the two-sample problem.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/72UPSU78/Vayatis et al_2009_AUC optimization and the two-sample problem.pdf:application/pdf;NIPS Snapshort:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/FS3AZ8MR/3838-auc-optimization-and-the-two-sample-problem.html:text/html}
}

@article{casella_assessing_2009,
  title = {Assessing {{Robustness}} of {{Intrinsic Tests}} of {{Independence}} in {{Two-Way Contingency Tables}}},
  volume = {104},
  issn = {0162-1459},
  doi = {10.1198/jasa.2009.tm08106},
  timestamp = {2016-07-31T19:11:46Z},
  number = {487},
  urldate = {2016-07-31},
  journal = {Journal of the American Statistical Association},
  author = {Casella, George and Moreno, El{\'\i}as},
  month = sep,
  year = {2009},
  pages = {1261--1271},
  file = {Casella_Moreno_2009_Assessing Robustness of Intrinsic Tests of Independence in Two-Way Contingency.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/B4AFMUVH/Casella_Moreno_2009_Assessing Robustness of Intrinsic Tests of Independence in Two-Way Contingency.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/6UJABK53/jasa.2009.html:text/html}
}

@article{scott_neyman-pearson_2005,
  title = {A {{Neyman-Pearson}} approach to statistical learning},
  volume = {51},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.856955},
  abstract = {The Neyman-Pearson (NP) approach to hypothesis testing is useful in situations where different types of error have different consequences or a priori probabilities are unknown. For any \ensuremath{\alpha{}>}0, the NP lemma specifies the most powerful test of size \ensuremath{\alpha}, but assumes the distributions for each hypothesis are known or (in some cases) the likelihood ratio is monotonic in an unknown parameter. This paper investigates an extension of NP theory to situations in which one has no knowledge of the underlying distributions except for a collection of independent and identically distributed (i.i.d.) training examples from each hypothesis. Building on a "fundamental lemma" of Cannon et al., we demonstrate that several concepts from statistical learning theory have counterparts in the NP context. Specifically, we consider constrained versions of empirical risk minimization (NP-ERM) and structural risk minimization (NP-SRM), and prove performance guarantees for both. General conditions are given under which NP-SRM leads to strong universal consistency. We also apply NP-SRM to (dyadic) decision trees to derive rates of convergence. Finally, we present explicit algorithms to implement NP-SRM for histograms and dyadic decision trees.},
  timestamp = {2016-08-06T06:04:02Z},
  number = {11},
  journal = {IEEE Transactions on Information Theory},
  author = {Scott, C. and Nowak, R.},
  month = nov,
  year = {2005},
  keywords = {a priori probability,Buildings,Condition monitoring,Convergence,convergence of numerical methods,convergence rate,decision trees,dyadic decision tree,empirical risk minimization,ERM,error analysis,explicit algorithm,Filtering,generalization error bound,Generalization error bounds,histogram,Histograms,Hypothesis testing,i.i.d.,independent-identically distributed training,learning (artificial intelligence),minimisation,monotonic likelihood ratio,Neyman-Pearson approach,Neyman–Pearson (NP) classification,NP theory,probability,risk management,signal classification,SRM,statistical learning,statistical learning theory,structural risk minimization,Testing},
  pages = {3806--3819},
  file = {Scott_Nowak_2005_A Neyman-Pearson approach to statistical learning.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TNIVPEG7/Scott_Nowak_2005_A Neyman-Pearson approach to statistical learning.pdf:application/pdf;IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/V783WGI9/abs_all.html:text/html}
}

@article{heller_consistent_2013,
  title = {A consistent multivariate test of association based on ranks of distances},
  volume = {100},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/ass070},
  abstract = {We consider the problem of detecting associations between random vectors of any dimension. Few tests of independence exist that are consistent against all dependent alternatives. We propose a powerful test that is applicable in all dimensions and consistent against all alternatives. The test has a simple form, is easy to implement, and has good power.},
  language = {en},
  timestamp = {2016-08-09T09:26:43Z},
  number = {2},
  urldate = {2016-08-03},
  journal = {Biometrika},
  author = {Heller, Ruth and Heller, Yair and Gorfine, Malka},
  month = jan,
  year = {2013},
  keywords = {High-dimensional response,Independence test,Multivariate data,Random vector},
  pages = {503--510},
  file = {Heller et al_2012_A consistent multivariate test of association based on ranks of distances.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/3GVUHNSW/Heller et al_2012_A consistent multivariate test of association based on ranks of distances.pdf:application/pdf;Heller et al_2013_A consistent multivariate test of association based on ranks of distances.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/3UVAGT4S/Heller et al_2013_A consistent multivariate test of association based on ranks of distances.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/2SAUKDXJ/biomet.html:text/html;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/G2J8UTTZ/503.html:text/html}
}

@article{mclachlan_bias_1976,
  title = {The bias of the apparent error rate in discriminant analysis},
  volume = {63},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/63.2.239},
  abstract = {SUMMARY The apparent error rate is a commonly used estimator of the actual error rate in discriminant analysis. In this study the asymptotic bias of the apparent error rate is derived in the context of two multivariate normal populations with unknown different means and unknown common covariance matrix. From the derived expansion a correction term ia available for reducing the bias of the apparent error rate from the first to the second order with respect to the reciprocals of the initial sample sizes. Also, some previously unanswered questions on inequalities between the average apparent, the optimal, and the average actual error rates are solved.},
  language = {en},
  timestamp = {2016-08-06T06:42:46Z},
  number = {2},
  urldate = {2016-08-05},
  journal = {Biometrika},
  author = {McLachlan, G. J.},
  month = jan,
  year = {1976},
  keywords = {Bias of apparent error rate,Error rates; optimal; actual and apparent,Inequalities between error rates,Linear discriminant function},
  pages = {239--244},
  file = {McLACHLAN_1976_The bias of the apparent error rate in discriminant analysis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/X9S3IQWJ/McLACHLAN_1976_The bias of the apparent error rate in discriminant analysis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/EC39SN57/239.html:text/html}
}

@article{braga-neto_is_2004,
  title = {Is cross-validation better than resubstitution for ranking genes?},
  volume = {20},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btg399},
  abstract = {Motivation: Ranking gene feature sets is a key issue for both phenotype classification, for instance, tumor classification in a DNA microarray experiment, and prediction in the context of genetic regulatory networks. Two broad methods are available to estimate the error (misclassification rate) of a classifier. Resubstitution fits a single classifier to the data, and applies this classifier in turn to each data observation. Cross-validation (in leave-one-out form) removes each observation in turn, constructs the classifier, and then computes whether this leave-one-out classifier correctly classifies the deleted observation. Resubstitution typically underestimates classifier error, severely so in many cases. Cross-validation has the advantage of producing an effectively unbiased error estimate, but the estimate is highly variable. In many applications it is not the misclassification rate per se that is of interest, but rather the construction of gene sets that have the potential to classify or predict. Hence, one needs to rank feature sets based on their performance.
Results: A model-based approach is used to compare the ranking performances of resubstitution and cross-validation for classification based on real-valued feature sets and for prediction in the context of probabilistic Boolean networks (PBNs). For classification, a Gaussian model is considered, along with classification via linear discriminant analysis and the 3-nearest-neighbor classification rule. Prediction is examined in the steady-distribution of a PBN. Three metrics are proposed to compare feature-set ranking based on error estimation with ranking based on the true error, which is known owing to the model-based approach. In all cases, resubstitution is competitive with cross-validation relative to ranking accuracy. This is in addition to the enormous savings in computation time afforded by resubstitution.},
  language = {en},
  timestamp = {2016-08-05T06:36:42Z},
  number = {2},
  urldate = {2016-08-05},
  journal = {Bioinformatics},
  author = {Braga-Neto, Ulisses and Hashimoto, Ronaldo and Dougherty, Edward R. and Nguyen, Danh V. and Carroll, Raymond J.},
  month = jan,
  year = {2004},
  pages = {253--258},
  file = {Braga-Neto et al_2004_Is cross-validation better than resubstitution for ranking genes.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/V3JSA76W/Braga-Neto et al_2004_Is cross-validation better than resubstitution for ranking genes.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/55E75EJ6/253.html:text/html},
  pmid = {14734317}
}

@article{blanchard_semi-supervised_2010,
  title = {Semi-{{Supervised Novelty Detection}}},
  volume = {11},
  issn = {ISSN 1533-7928},
  timestamp = {2016-08-06T05:47:34Z},
  number = {Nov},
  urldate = {2016-08-06},
  journal = {Journal of Machine Learning Research},
  author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
  year = {2010},
  pages = {2973--3009},
  file = {Blanchard et al_2010_Semi-Supervised Novelty Detection.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/VHWFXXZX/Blanchard et al_2010_Semi-Supervised Novelty Detection.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/3C47FU5K/blanchard10a.html:text/html}
}

@inproceedings{davenport_controlling_2006,
  title = {Controlling {{False Alarms With Support Vector Machines}}},
  volume = {5},
  doi = {10.1109/ICASSP.2006.1661344},
  abstract = {We study the problem of designing support vector classifiers with respect to a Neyman-Pearson criterion. Specifically, given a user-specified level alpha isin (0,1), how can we ensure a false alarm rate no greater than q while minimizing the miss rate? We examine two approaches, one based on shifting the offset of a conventionally trained SVM and the other based on the introduction of class-specific weights. Our contributions include a novel heuristic for improved error estimation and a strategy for efficiently searching the parameter space of the second method. We also provide a characterization of the feasible parameter set of the 2v-SVM on which the second approach is based. The proposed methods are compared on four benchmark datasets},
  timestamp = {2016-08-06T05:53:50Z},
  booktitle = {2006 {{IEEE International Conference}} on {{Acoustics Speech}} and {{Signal Processing Proceedings}}},
  author = {Davenport, M. A. and Baraniuk, R. G. and Scott, C. D.},
  month = may,
  year = {2006},
  keywords = {Benign tumors,Cancer,Constraint theory,Costs,error analysis,error estimation,false alarms,Neoplasms,Neyman-Pearson criterion,pattern classification,Statistics,Support vector machine classification,support vector machines,Training data},
  pages = {V--V},
  file = {Davenport et al_2006_Controlling False Alarms With Support Vector Machines.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/CD3MIFKQ/Davenport et al_2006_Controlling False Alarms With Support Vector Machines.pdf:application/pdf;IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/P5XHAZXJ/abs_all.html:text/html}
}

@inproceedings{scott_novelty_2009,
  title = {Novelty detection: {{Unlabeled}} data definitely help.},
  shorttitle = {Novelty detection},
  timestamp = {2016-08-06T05:59:00Z},
  urldate = {2016-08-06},
  booktitle = {{{AISTATS}}},
  author = {Scott, Clayton and Blanchard, Gilles},
  year = {2009},
  pages = {464--471},
  file = {Scott_Blanchard_2009_Novelty detection.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SXE6WR5Q/Scott_Blanchard_2009_Novelty detection.pdf:application/pdf}
}

@article{scott_performance_2007,
  title = {Performance {{Measures}} for {{Neyman}} \#x2013;{{Pearson Classification}}},
  volume = {53},
  issn = {0018-9448},
  doi = {10.1109/TIT.2007.901152},
  abstract = {In the Neyman-Pearson (NP) classification paradigm, the goal is to learn a classifier from labeled training data such that the probability of a false negative is minimized while the probability of a false positive is below a user-specified level alpha isin (0,1). This work addresses the question of how to evaluate and compare classifiers in the NP setting. Simply reporting false positives and false negatives leaves some ambiguity about which classifier is best. Unlike conventional classification, however, there is no natural performance measure for NP classification. We cannot reject classifiers whose false positive rate exceeds a since, among other reasons, the false positive rate must be estimated from data and hence is not known with certainty. We propose two families of performance measures for evaluating and comparing classifiers and suggest one criterion in particular for practical use. We then present general learning rules that satisfy performance guarantees with respect to these criteria. As in conventional classification, the notion of uniform convergence plays a central role, and leads to finite sample bounds, oracle inequalities, consistency, and rates of convergence. The proposed performance measures are also applicable to the problem of anomaly prediction.},
  timestamp = {2016-08-06T06:01:45Z},
  number = {8},
  journal = {IEEE Transactions on Information Theory},
  author = {Scott, C.},
  month = aug,
  year = {2007},
  keywords = {anomaly prediction,computational complexity,Convergence,Costs,Diseases,false negative probability,false positive probability,frequency,Intrusion detection,Neyman-Pearson classification,Neyman–Pearson (NP) classification,NP classification paradigm,Particle measurements,pattern classification,performance measures,probability,Statistical analysis,statistical learning,statistical learning theory,Testing,Training data},
  pages = {2852--2863},
  file = {Scott_2007_Performance Measures for Neyman #x2013\;Pearson Classification.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/AWD7V9BK/Scott_2007_Performance Measures for Neyman #x2013\;Pearson Classification.pdf:application/pdf;IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/WRXQ7J63/abs_all.html:text/html}
}

@incollection{golland_permutation_2005,
  series = {Lecture Notes in Computer Science},
  title = {Permutation {{Tests}} for {{Classification}}},
  copyright = {\textcopyright{}2005 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-26556-6 978-3-540-31892-7},
  abstract = {We describe a permutation procedure used extensively in classification problems in computational biology and medical imaging. We empirically study the procedure on simulated data and real examples from neuroimaging studies and DNA microarray analysis. A theoretical analysis is also suggested to assess the asymptotic behavior of the test. An interesting observation is that concentration of the permutation procedure is controlled by a Rademacher average which also controls the concentration of empirical errors to expected errors.},
  language = {en},
  timestamp = {2016-08-06T07:40:43Z},
  number = {3559},
  urldate = {2016-08-06},
  booktitle = {Learning {{Theory}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Golland, Polina and Liang, Feng and Mukherjee, Sayan and Panchenko, Dmitry},
  editor = {Auer, Peter and Meir, Ron},
  month = jun,
  year = {2005},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  pages = {501--515},
  file = {Golland et al_2005_Permutation Tests for Classification.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/DEC9CKG7/Golland et al_2005_Permutation Tests for Classification.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/PUJT9AG2/10.html:text/html},
  doi = {10.1007/11503415_34}
}

@misc{harrell_cross_validated,
  title = {classification - {{How}} to assess statistical significance of the accuracy of a classifier? - {{Cross Validated}}},
  shorttitle = {classification - {{How}} to assess statistical significance of the accuracy of a classifier?},
  timestamp = {2016-08-06T07:27:18Z},
  urldate = {2016-08-06},
  howpublished = {\url{http://stats.stackexchange.com/questions/17408/how-to-assess-statistical-significance-of-the-accuracy-of-a-classifier}},
  file = {Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/IQ3WEKQK/how-to-assess-statistical-significance-of-the-accuracy-of-a-classifier.html:text/html}
}

@misc{_significance_????,
  title = {Significance testing of cross-validated classification accuracy: shuffling vs. binomial test - {{Cross Validated}}},
  shorttitle = {Significance testing of cross-validated classification accuracy},
  timestamp = {2016-08-06T07:18:33Z},
  urldate = {2016-08-06},
  howpublished = {\url{http://stats.stackexchange.com/questions/88809/significance-testing-of-cross-validated-classification-accuracy-shuffling-vs-b}},
  file = {Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/J4SDCS7U/significance-testing-of-cross-validated-classification-accuracy-shuffling-vs-b.html:text/html}
}

@inproceedings{slonim_class_2000,
  address = {New York, NY, USA},
  series = {RECOMB '00},
  title = {Class {{Prediction}} and {{Discovery Using Gene Expression Data}}},
  isbn = {978-1-58113-186-4},
  doi = {10.1145/332306.332564},
  abstract = {Classification of patient samples is a crucial aspect of cancer diagnosis and treatment. We present a method for classifying samples by computational analysis of gene expression data. We consider the classification problem in two parts: class discovery and class prediction. Class discovery refers to the process of dividing samples into reproducible classes that have similar behavior or properties, while class prediction places new samples into already known classes. We describe a method for performing class prediction and illustrate its strength by correctly classifying bone marrow and blood samples from acute leukemia patients. We also describe how to use our predictor to validate newly discovered classes, and we demonstrate how this technique could have discovered the key distinctions among leukemias if they were not already known. This proof-of-concept experiment paves the way for a wealth of future work on the molecular classification and understanding of disease.},
  timestamp = {2016-08-06T07:38:39Z},
  urldate = {2016-08-06},
  booktitle = {Proceedings of the {{Fourth Annual International Conference}} on {{Computational Molecular Biology}}},
  publisher = {{ACM}},
  author = {Slonim, Donna K. and Tamayo, Pablo and Mesirov, Jill P. and Golub, Todd R. and Lander, Eric S.},
  year = {2000},
  pages = {263--272},
  file = {Slonim et al_2000_Class Prediction and Discovery Using Gene Expression Data.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/BP69R72G/Slonim et al_2000_Class Prediction and Discovery Using Gene Expression Data.pdf:application/pdf}
}

@article{mukherjee_estimating_2003,
  title = {Estimating dataset size requirements for classifying {{DNA}} microarray data},
  volume = {10},
  issn = {1066-5277},
  doi = {10.1089/106652703321825928},
  abstract = {A statistical methodology for estimating dataset size requirements for classifying microarray data using learning curves is introduced. The goal is to use existing classification results to estimate dataset size requirements for future classification experiments and to evaluate the gain in accuracy and significance of classifiers built with additional data. The method is based on fitting inverse power-law models to construct empirical learning curves. It also includes a permutation test procedure to assess the statistical significance of classification performance for a given dataset size. This procedure is applied to several molecular classification problems representing a broad spectrum of levels of complexity.},
  language = {eng},
  timestamp = {2016-08-06T07:39:06Z},
  number = {2},
  journal = {Journal of Computational Biology: A Journal of Computational Molecular Cell Biology},
  author = {Mukherjee, Sayan and Tamayo, Pablo and Rogers, Simon and Rifkin, Ryan and Engle, Anna and Campbell, Colin and Golub, Todd R. and Mesirov, Jill P.},
  year = {2003},
  keywords = {Algorithms,Computational Biology,Computer Simulation,Gene Expression Profiling,Humans,Models; Molecular,Neoplasms,Oligonucleotide Array Sequence Analysis},
  pages = {119--142},
  file = {Mukherjee et al_2003_Estimating dataset size requirements for classifying DNA microarray data.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/XD77K4K5/Mukherjee et al_2003_Estimating dataset size requirements for classifying DNA microarray data.pdf:application/pdf},
  pmid = {12804087}
}

@article{juan_prediction_2004,
  title = {Prediction of tumor outcome based on gene expression data},
  volume = {9},
  issn = {1007-1202, 1993-4998},
  doi = {10.1007/BF02830598},
  abstract = {Gene expression microarray data can be used to classify tumor types. We proposed a new procedure to classify human tumor samples based on microarray gene expressions by using a hybrid supervised learning method called MOEA+WV (Multi-Objective Evolutionary Algorithm+Weighted Voting). MOEA is used to search for a relatively few subsets of informative genes from the high-dimensional gene space, and WV is used as a classification tool. This new method has been applied to predicate the subtypes of lymphoma and outcomes of medulloblastoma. The results are relatively accurate and meaningful compared to those from other methods.},
  language = {en},
  timestamp = {2016-08-06T07:39:29Z},
  number = {2},
  urldate = {2016-08-06},
  journal = {Wuhan University Journal of Natural Sciences},
  author = {Juan, Liu and Iba, Hitoshi},
  month = mar,
  year = {2004},
  keywords = {Bioinformatics,Computer Science; general,Life Sciences; general,Mathematics; general,MOEA,Pareto optimization,Physics; general,Q 786,TP 181,tumor classification},
  pages = {177--182},
  file = {Juan_Iba_2004_Prediction of tumor outcome based on gene expression data.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/VI2D6ETR/Juan_Iba_2004_Prediction of tumor outcome based on gene expression data.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/KNGZDD4K/BF02830598.html:text/html}
}

@article{golub_molecular_1999,
  title = {Molecular {{Classification}} of {{Cancer}}: {{Class Discovery}} and {{Class Prediction}} by {{Gene Expression Monitoring}}},
  volume = {286},
  issn = {0036-8075, 1095-9203},
  shorttitle = {Molecular {{Classification}} of {{Cancer}}},
  doi = {10.1126/science.286.5439.531},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  language = {en},
  timestamp = {2016-08-06T07:40:04Z},
  number = {5439},
  urldate = {2016-08-06},
  journal = {Science},
  author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
  month = oct,
  year = {1999},
  pages = {531--537},
  file = {Golub et al_1999_Molecular Classification of Cancer.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SGGJM52S/Golub et al_1999_Molecular Classification of Cancer.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/EWUEE8IZ/531.html:text/html},
  pmid = {10521349}
}

@article{efron_estimating_1983,
  title = {Estimating the {{Error Rate}} of a {{Prediction Rule}}: {{Improvement}} on {{Cross-Validation}}},
  volume = {78},
  issn = {0162-1459},
  shorttitle = {Estimating the {{Error Rate}} of a {{Prediction Rule}}},
  doi = {10.1080/01621459.1983.10477973},
  abstract = {We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation provides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related closely to the bootstrap estimate of the error rate. This article has two purposes: to understand better the theoretical basis of the prediction problem, and to investigate some related estimators, which seem to offer considerably improved estimation in small samples.},
  timestamp = {2016-08-10T04:39:52Z},
  number = {382},
  urldate = {2016-08-10},
  journal = {Journal of the American Statistical Association},
  author = {Efron, Bradley},
  month = jun,
  year = {1983},
  pages = {316--331},
  file = {Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TZFB3NCW/01621459.1983.html:text/html}
}

@article{hsing_relation_????,
  title = {Relation {{Between Permutation-Test P Values}} and {{Classifier Error Estimates}}},
  volume = {52},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1023985022691},
  abstract = {Gene-expression-based classifiers suffer from the small number of microarrays usually available for classifier design. Hence, one is confronted with the dual problem of designing a classifier and estimating its error with only a small sample. Permutation testing has been recommended to assess the dependency of a designed classifier on the specific data set. This involves randomly permuting the labels of the data points, estimating the error of the designed classifiers for each permutation, and then finding the p value of the error for the actual labeling relative to the population of errors for the random labelings. This paper addresses the issue of whether or not this p value is informative. It provides both analytic and simulation results to show that the permutation p value is, up to very small deviation, a function of the error estimate. Moreover, even though the p value is a monotonically increasing function of the error estimate, in the range of the error where the majority of the p values lie, the function is very slowly increasing, so that inversion is problematic. Hence, the conclusion is that the p value is less informative than the error estimate. This result demonstrates that random labeling does not provide any further insight into the accuracy of the classifier or the precision of the error estimate. We have no knowledge beyond the error estimate itself and the various distribution-free, classifier-specific bounds developed for this estimate.},
  language = {en},
  timestamp = {2016-08-10T06:23:29Z},
  number = {1-2},
  urldate = {2016-08-10},
  journal = {Machine Learning},
  author = {Hsing, Tailen and Attoor, Sanju and Dougherty, Edward},
  pages = {11--30},
  file = {Hsing et al_Relation Between Permutation-Test P Values and Classifier Error Estimates.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/NKNDPGEF/Hsing et al_Relation Between Permutation-Test P Values and Classifier Error Estimates.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/QRWTUHZF/A1023985022691.html:text/html}
}

@article{fay_using_2007,
  title = {On {{Using Truncated Sequential Probability Ratio Test Boundaries}} for {{Monte Carlo Implementation}} of {{Hypothesis Tests}}},
  volume = {16},
  issn = {1061-8600},
  doi = {10.1198/106186007X257025},
  abstract = {When designing programs or software for the implementation of Monte Carlo (MC) hypothesis tests, we can save computation time by using sequential stopping boundaries. Such boundaries imply stopping resampling after relatively few replications if the early replications indicate a very large or a very small p value. We study a truncated sequential probability ratio test (SPRT) boundary and provide a tractable algorithm to implement it. We review two properties desired of any MC p value, the validity of the p value and a small resampling risk, where resampling risk is the probability that the accept/reject decision will be different than the decision from complete enumeration. We show how the algorithm can be used to calculate a valid p value and confidence intervals for any truncated SPRT boundary. We show that a class of SPRT boundaries is minimax with respect to resampling risk and recommend a truncated version of boundaries in that class by comparing their resampling risk (RR) to the RR of fixed boundaries with the same maximum resample size. We study the lack of validity of some simple estimators of p values and offer a new, simple valid p value for the recommended truncated SPRT boundary. We explore the use of these methods in a practical example and provide the MChtest R package to perform the methods.},
  timestamp = {2016-08-10T06:31:08Z},
  number = {4},
  urldate = {2016-08-10},
  journal = {Journal of Computational and Graphical Statistics},
  author = {Fay, Michael P and Kim, Hyune-Ju and Hachey, Mark},
  month = dec,
  year = {2007},
  pages = {946--967},
  file = {Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/CZB7RFFK/106186007X257025.html:text/html}
}

@article{ernst_permutation_2004,
  title = {Permutation {{Methods}}: {{A Basis}} for {{Exact Inference}}},
  volume = {19},
  issn = {0883-4237, 2168-8745},
  shorttitle = {Permutation {{Methods}}},
  doi = {10.1214/088342304000000396},
  abstract = {The use of permutation methods for exact inference dates back to Fisher in 1935. Since then, the practicality of such methods has increased steadily with computing power. They can now easily be employed in many situations without concern for computing difficulties. We discuss the reasoning behind these methods and describe situations when they are exact and distribution-free. We illustrate their use in several examples.},
  timestamp = {2016-08-10T11:01:06Z},
  number = {4},
  urldate = {2016-08-10},
  journal = {Statistical Science},
  author = {Ernst, Michael D.},
  month = nov,
  year = {2004},
  keywords = {Distribution-free,Monte Carlo,nonparametric,Permutation tests,Randomization tests},
  pages = {676--685},
  file = {Ernst_2004_Permutation Methods.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/3ZSJFBBP/Ernst_2004_Permutation Methods.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/9IP4MM9Q/1113832732.html:text/html},
  mrnumber = {MR2185589},
  zmnumber = {1100.62563}
}

@article{pang_shrinkage-based_2009,
  title = {Shrinkage-based {{Diagonal Discriminant Analysis}} and {{Its Applications}} in {{High-Dimensional Data}}},
  volume = {65},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2009.01200.x},
  abstract = {Summary High-dimensional data such as microarrays have brought us new statistical challenges. For example, using a large number of genes to classify samples based on a small number of microarrays remains a difficult problem. Diagonal discriminant analysis, support vector machines, and\hspace{0.6em}k-nearest neighbor have been suggested as among the best methods for small sample size situations, but none was found to be superior to others. In this article, we propose an improved diagonal discriminant approach through shrinkage and regularization of the variances. The performance of our new approach along with the existing methods is studied through simulations and applications to real data. These studies show that the proposed shrinkage-based and regularization diagonal discriminant methods have lower misclassification rates than existing methods in many cases.},
  language = {en},
  timestamp = {2016-08-11T13:56:08Z},
  number = {4},
  urldate = {2016-08-11},
  journal = {Biometrics},
  author = {Pang, Herbert and Tong, Tiejun and Zhao, Hongyu},
  month = dec,
  year = {2009},
  keywords = {discriminant analysis,Large p small n,Microarray,regularization,Shrinkage,tumor classification},
  pages = {1021--1029},
  file = {Pang et al_2009_Shrinkage-based Diagonal Discriminant Analysis and Its Applications in.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/GZMAEAK8/Pang et al_2009_Shrinkage-based Diagonal Discriminant Analysis and Its Applications in.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/UV9GNXG4/abstract.html:text/html}
}

@article{tong_improved_2012,
  title = {Improved mean estimation and its application to diagonal discriminant analysis},
  volume = {28},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btr690},
  abstract = {Motivation: High-dimensional data such as microarrays have created new challenges to traditional statistical methods. One such example is on class prediction with high-dimension, low-sample size data. Due to the small sample size, the sample mean estimates are usually unreliable. As a consequence, the performance of the class prediction methods using the sample mean may also be unsatisfactory. To obtain more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, are often desired.
Results: In this article, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. The optimal shrinkage parameter is proposed under the scenario when the sample size is fixed and the dimension is large. We then construct a shrinkage-based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean. Finally, we demonstrate via simulation studies and real data analysis that the proposed shrinkage-based rule outperforms its original competitor in a wide range of settings.
Contact: tongt@hkbu.edu.hk},
  language = {en},
  timestamp = {2016-08-11T13:56:14Z},
  number = {4},
  urldate = {2016-08-11},
  journal = {Bioinformatics},
  author = {Tong, Tiejun and Chen, Liang and Zhao, Hongyu},
  month = feb,
  year = {2012},
  pages = {531--537},
  file = {Tong et al_2012_Improved mean estimation and its application to diagonal discriminant analysis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/7KAVXN3A/Tong et al_2012_Improved mean estimation and its application to diagonal discriminant analysis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SCC5VK57/531.html:text/html},
  pmid = {22171335}
}

@article{srivastava_comparison_2007,
  title = {Comparison of {{Discrimination Methods}} for {{High Dimensional Data}}},
  volume = {37},
  doi = {10.14490/jjss.37.123},
  timestamp = {2016-08-11T13:56:43Z},
  number = {1},
  journal = {Journal of the Japan Statistical Society},
  author = {Srivastava, Muni S. and Kubokawa, Tatsuya},
  year = {2007},
  keywords = {classification,discrimination analysis,minimum distance,Moore-Penrose inverse},
  pages = {123--134},
  file = {Srivastava_Kubokawa_2007_Comparison of Discrimination Methods for High Dimensional Data.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TFXE8IQS/Srivastava_Kubokawa_2007_Comparison of Discrimination Methods for High Dimensional Data.pdf:application/pdf}
}

@article{dudoit_comparison_2002,
  title = {Comparison of {{Discrimination Methods}} for the {{Classification}} of {{Tumors Using Gene Expression Data}}},
  volume = {97},
  issn = {0162-1459},
  doi = {10.1198/016214502753479248},
  abstract = {A reliable and precise classification of tumors is essential for successful diagnosis and treatment of cancer. cDNA microarrays and high-density oligonucleotide chips are novel biotechnologies increasingly used in cancer research. By allowing the monitoring of expression levels in cells for thousands of genes simultaneously, microarray experiments may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. The ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. This article compares the performance of different discrimination methods for the classification of tumors based on gene expression data. The methods include nearest-neighbor classifiers, linear discriminant analysis, and classification trees. Recent machine learning approaches, such as bagging and boosting, are also considered. The discrimination methods are applied to datasets from three recently published cancer gene expression studies.},
  timestamp = {2016-08-11T13:56:58Z},
  number = {457},
  urldate = {2016-08-11},
  journal = {Journal of the American Statistical Association},
  author = {Dudoit, Sandrine and Fridlyand, Jane and Speed, Terence P.},
  month = mar,
  year = {2002},
  pages = {77--87},
  file = {Dudoit et al_2002_Comparison of Discrimination Methods for the Classification of Tumors Using.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/7E4I6QI6/Dudoit et al_2002_Comparison of Discrimination Methods for the Classification of Tumors Using.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/CUDUDMF8/016214502753479248.html:text/html}
}

@article{ramey_comparison_2013,
  title = {A comparison of regularization methods applied to the linear discriminant function with high-dimensional microarray data},
  volume = {83},
  issn = {0094-9655},
  doi = {10.1080/00949655.2011.625946},
  abstract = {Classification of gene expression microarray data is important in the diagnosis of diseases such as cancer, but often the analysis of microarray data presents difficult challenges because the gene expression dimension is typically much larger than the sample size. Consequently, classification methods for microarray data often rely on regularization techniques to stabilize the classifier for improved classification performance. In particular, numerous regularization techniques, such as covariance-matrix regularization, are available, which, in practice, lead to a difficult choice of regularization methods. In this paper, we compare the classification performance of five covariance-matrix regularization methods applied to the linear discriminant function using two simulated high-dimensional data sets and five well-known, high-dimensional microarray data sets. In our simulation study, we found the minimum distance empirical Bayes method reported in Srivastava and Kubokawa [Comparison of discrimination methods for high dimensional data, J. Japan Statist. Soc. 37(1) (2007), pp. 123\textendash{}134], and the new linear discriminant analysis reported in Thomaz, Kitani, and Gillies [A Maximum Uncertainty LDA-based approach for Limited Sample Size problems \textendash{} with application to Face Recognition, J. Braz. Comput. Soc. 12(1) (2006), pp. 1\textendash{}12], to perform consistently well and often outperform three other prominent regularization methods. Finally, we conclude with some recommendations for practitioners.},
  timestamp = {2016-08-11T13:59:03Z},
  number = {3},
  urldate = {2016-08-11},
  journal = {Journal of Statistical Computation and Simulation},
  author = {Ramey, John A. and Young, Phil D.},
  month = mar,
  year = {2013},
  pages = {581--596},
  file = {Ramey_Young_2013_A comparison of regularization methods applied to the linear discriminant.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/AFPUWMFX/Ramey_Young_2013_A comparison of regularization methods applied to the linear discriminant.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/9Q66C8BX/00949655.2011.html:text/html}
}

@article{ramey_high-dimensional_2016,
  title = {High-{{Dimensional Regularized Discriminant Analysis}}},
  timestamp = {2016-08-11T14:04:27Z},
  urldate = {2016-08-11},
  journal = {arXiv preprint arXiv:1602.01182},
  author = {Ramey, John A. and Stein, Caleb K. and Young, Phil D. and Young, Dean M.},
  year = {2016},
  file = {Ramey et al_2016_High-Dimensional Regularized Discriminant Analysis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SU2GI5X7/Ramey et al_2016_High-Dimensional Regularized Discriminant Analysis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/IQZ62PZC/1602.html:text/html}
}

@article{friedman_regularization_2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  volume = {33},
  timestamp = {2016-08-12T05:42:16Z},
  number = {1},
  journal = {Journal of Statistical Software},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2010},
  pages = {1--22}
}

@book{ramey_sparsediscrim:_2016,
  title = {sparsediscrim: {{Sparse}} and {{Regularized Discriminant Analysis}}},
  timestamp = {2016-08-12T05:43:07Z},
  author = {Ramey, John A.},
  year = {2016},
  note = {R package version 0.2.3}
}

@misc{_ramhiser/sparsediscrim_????,
  title = {ramhiser/sparsediscrim},
  abstract = {sparsediscrim - Sparse and Regularized Discriminant Analysis in R},
  timestamp = {2016-08-13T14:02:35Z},
  urldate = {2016-08-13},
  howpublished = {\url{https://github.com/ramhiser/sparsediscrim}},
  journal = {GitHub},
  file = {Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/U3F4XJ38/sparsediscrim.html:text/html}
}

@book{meyer_e1071:_2015,
  title = {e1071: {{Misc Functions}} of the {{Department}} of {{Statistics}}, {{Probability Theory Group}} ({{Formerly}}: {{E1071}}), {{TU Wien}}},
  timestamp = {2016-08-14T20:29:54Z},
  author = {Meyer, David and Dimitriadou, Evgenia and Hornik, Kurt and Weingessel, Andreas and Leisch, Friedrich},
  year = {2015},
  note = {R package version 1.6-7}
}

@article{breiman_using_1998,
  title = {Using convex pseudo-data to increase prediction accuracy},
  volume = {699},
  timestamp = {2016-08-16T04:37:16Z},
  number = {9},
  urldate = {2016-08-16},
  journal = {breast (Wis)},
  author = {Breiman, Leo},
  year = {1998},
  pages = {2},
  file = {Breiman_1998_Using convex pseudo-data to increase prediction accuracy.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/BBZ3QJDR/Breiman_1998_Using convex pseudo-data to increase prediction accuracy.pdf:application/pdf}
}

@article{szekely_brownian_2009,
  title = {Brownian distance covariance},
  volume = {3},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/09-AOAS312},
  abstract = {Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.},
  language = {EN},
  timestamp = {2016-08-19T13:50:21Z},
  number = {4},
  urldate = {2016-08-19},
  journal = {The Annals of Applied Statistics},
  author = {Sz{\'e}kely, G{\'a}bor J. and Rizzo, Maria L.},
  month = dec,
  year = {2009},
  keywords = {Brownian covariance,dcor,Distance correlation,independence,Multivariate},
  pages = {1236--1265},
  file = {Székely_Rizzo_2009_Brownian distance covariance.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/ERR2RH2U/Székely_Rizzo_2009_Brownian distance covariance.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/PA6EQT2U/1267453933.html:text/html},
  mrnumber = {MR2752127}
}

@article{gretton_discussion_2009,
  title = {Discussion of: {{Brownian}} distance covariance},
  volume = {3},
  issn = {1932-6157, 1941-7330},
  shorttitle = {Discussion of},
  doi = {10.1214/09-AOAS312E},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  timestamp = {2016-08-19T13:50:39Z},
  number = {4},
  urldate = {2016-08-19},
  journal = {The Annals of Applied Statistics},
  author = {Gretton, Arthur and Fukumizu, Kenji and Sriperumbudur, Bharath K.},
  month = dec,
  year = {2009},
  keywords = {Brownian distance covariance,covariance operator,Independence testing,kernel methods,reproducing kernel Hilbert space},
  pages = {1285--1294},
  file = {Gretton et al_2009_Discussion of.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/XICFVH8J/Gretton et al_2009_Discussion of.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/5Z4E2JAM/1267453938.html:text/html},
  mrnumber = {MR2752132},
  zmnumber = {05696876}
}

@inproceedings{smola_hilbert_2007-1,
  title = {A {{Hilbert}} space embedding for distributions},
  timestamp = {2016-08-19T13:51:02Z},
  urldate = {2016-08-19},
  booktitle = {International {{Conference}} on {{Algorithmic Learning Theory}}},
  publisher = {{Springer}},
  author = {Smola, Alex and Gretton, Arthur and Song, Le and Sch{\"o}lkopf, Bernhard},
  year = {2007},
  pages = {13--31},
  file = {Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/JE62NE4D/978-3-540-75225-7_5.html:text/html}
}

@article{dauxois_nonlinear_1998,
  title = {Nonlinear canonical analysis and independence tests},
  volume = {26},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1024691242},
  abstract = {.Measures of association between two random variables (r.v.) that are symmetric nondecreasing functions of the canonical coefficients provided by the nonlinear canonical analysis of the r.v.'s are studied. These measures can be used to characterize independence. Their estimators are obtained by estimating a suitable approximation to nonlinear canonical analysis. When some conditions are satisfied, asymptotic distributions of the estimators, both under the independence hypothesis and under dependence, are given. A class of tests of independence with asymptotic level of significance can be investigated.},
  timestamp = {2016-08-19T13:48:07Z},
  number = {4},
  urldate = {2016-08-19},
  journal = {The Annals of Statistics},
  author = {Dauxois, Jacques and Nkiet, Guy Martial},
  month = aug,
  year = {1998},
  keywords = {approximating and estimating nonlinear canonical
			 analysis,independence tests,Measure of association,nonlinear canonical analysis,symmetric nondecreasing functions},
  pages = {1254--1278},
  file = {Dauxois_Nkiet_1998_Nonlinear canonical analysis and independence tests.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/42GK3GTG/Dauxois_Nkiet_1998_Nonlinear canonical analysis and independence tests.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/SGXBMZNN/1024691242.html:text/html},
  mrnumber = {MR1647653},
  zmnumber = {0934.62061}
}

@article{bach_kernel_2002,
  title = {Kernel {{Independent Component Analysis}}},
  volume = {3},
  issn = {ISSN 1533-7928},
  timestamp = {2016-08-19T13:48:53Z},
  number = {Jul},
  urldate = {2016-08-19},
  journal = {Journal of Machine Learning Research},
  author = {Bach, Francis R. and Jordan, Michael I.},
  year = {2002},
  pages = {1--48},
  file = {Bach_Jordan_2002_Kernel Independent Component Analysis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/F2N6XPBJ/Bach_Jordan_2002_Kernel Independent Component Analysis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/56MTGJNS/bach02a.html:text/html}
}

@article{fukumizu_statistical_2007,
  title = {Statistical {{Consistency}} of {{Kernel Canonical Correlation Analysis}}},
  volume = {8},
  issn = {ISSN 1533-7928},
  timestamp = {2016-08-19T13:49:31Z},
  number = {Feb},
  urldate = {2016-08-19},
  journal = {Journal of Machine Learning Research},
  author = {Fukumizu, Kenji and Bach, Francis R. and Gretton, Arthur},
  year = {2007},
  pages = {361--383},
  file = {Fukumizu et al_2007_Statistical Consistency of Kernel Canonical Correlation Analysis.pdf:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/TZER5RX9/Fukumizu et al_2007_Statistical Consistency of Kernel Canonical Correlation Analysis.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/vs27un40.default/zotero/storage/NVE7XNFC/fukumizu07a.html:text/html}
}


@article{goeman2006testing,
	title={Testing against a high dimensional alternative},
	author={Goeman, Jelle J and Van De Geer, Sara A and Van Houwelingen, Hans C},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={68},
	number={3},
	pages={477--493},
	year={2006},
	publisher={Wiley Online Library}
}

@article{ley2015high,
	title={High-dimensional tests for spherical location and spiked covariance},
	author={Ley, Christophe and Paindaveine, Davy and Verdebout, Thomas},
	journal={Journal of Multivariate Analysis},
	volume={139},
	pages={79--91},
	year={2015},
	publisher={Elsevier}
}

@article{gilron2017s,
	title={What's in a pattern? Examining the type of signal multivariate analysis uncovers at the group level},
	author={Gilron, Roee and Rosenblatt, Jonathan and Koyejo, Oluwasanmi and Poldrack, Russell A and Mukamel, Roy},
	journal={NeuroImage},
	volume={146},
	pages={113--120},
	year={2017},
	publisher={Elsevier}
}

@article{timmer1995generating,
	title={On generating power law noise.},
	author={Timmer, J and K{\"o}nig, M},
	journal={Astronomy and Astrophysics},
	volume={300},
	pages={707},
	year={1995}
}


@article{thieler2016robper,
	title={RobPer: An R Package to Calculate Periodograms for Light Curves Based on Robust Regression},
	author={Thieler, Anita M and Fried, Roland and Rathjens, Jonathan and others},
	journal={Journal of Statistical Software},
	volume={69},
	number={9},
	pages={1--36},
	year={2016}
}


@article{kasdin1995discrete,
	title={Discrete simulation of colored noise and stochastic processes and 1/f/sup/spl alpha//power law noise generation},
	author={Kasdin, N Jeremy},
	journal={Proceedings of the IEEE},
	volume={83},
	number={5},
	pages={802--827},
	year={1995},
	publisher={IEEE}
}

@article{bai1996effect,
	title={Effect of high dimension: by an example of a two sample problem},
	author={Bai, Zhidong and Saranadasa, Hewa},
	journal={Statistica Sinica},
	pages={311--329},
	year={1996},
	publisher={JSTOR}
}


@article{rosenblatt2017mixture,
	title={On Mixture Alternatives and Wilcoxon's Signed-Rank Test},
	author={Rosenblatt, Jonathan D and Benjamini, Yoav},
	journal={The American Statistician},
	number={just-accepted},
	year={2017},
	publisher={Taylor \& Francis}
}
