\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{natbib}
\usepackage{url}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lineno}
\usepackage{tcolorbox}
\usepackage{caption}
\usepackage{hyperref}
\AtBeginDocument{\let\textlabel\label}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black}


\captionsetup[figure]{labelfont=it,textfont={it},textfont=footnotesize}
\captionsetup[subfigure]{width=0.8\hsize,labelfont=bf,textfont=footnotesize,singlelinecheck=off,justification=raggedright,format=hang}

\theoremstyle{definition}
\newtheorem{definition}{Definition}



\author{Jonathan Rosenblatt \and Roee Gilron \and Roy Mukamel}


%% OPTIONAL MACRO DEFINITIONS
%\def\s{\sigma}
\newcommand{\set}[1]{\{ #1 \}} % A set
\newcommand{\indicator}[1]{I_{\set{#1}}} % The indicator function.
\newcommand{\reals}{\mathbb{R}} % the set of real numbers
\newcommand{\features}{x} % The feature space
\newcommand{\outcomes}{y} % The feature space
\newcommand{\featureS}{\mathcal{X}} % The feature space
\newcommand{\outcomeS}{\mathcal{Y}} % The feature space
\newcommand{\expect}[1]{\mathbf{E}\left[ #1 \right]} % The expectation operator
\newcommand{\acc}{\mathcal{E}} 
\newcommand{\accEstim}{\hat{\mathcal{E}}} 
\newcommand{\accZ}{\hat{\mathcal{Z}}} 
\newcommand{\hyp}{f} % A hypothesis
\newcommand{\hypEstim}{\algo(\data)} %{\hat{\hyp}} % A hypothesis
\newcommand{\hypclass}{\mathcal{F}}
\newcommand{\majority}{\hat{\pi}}
\newcommand{\minority}{\hat{p}_{min}}
\newcommand{\prob}[1]{Prob( #1 )} % the probability of an event
\newcommand{\rv}[1]{\mathbf{#1}} % A random variable
\newcommand{\x}{\rv x} % The random variable x 
\newcommand{\y}{\rv y} % The random variable x 
\newcommand{\X}{\rv X} % The random variable x 
\newcommand{\Y}{\rv Y} % The random variable y
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)} % The Gaussian distribution
\newcommand{\gaussp}[2]{\mathcal{N}_{#1}\left(#2\right)} % The Gaussian distribution
\newcommand{\mycaption}{Simulation details in Appendix~\ref{apx:simulation_details} except the changes in the sub-captions.}
\newcommand{\argmin}[2]{\mathop{argmin} _{#1}\set{#2}} % The argmin operator
\newcommand{\argmax}[2]{\mathop{argmax} _{#1}\set{#2}} % The argmin operator
\newcommand{\R}{\textsf{R }}
\newcommand{\algo}{\mathcal{A}}
\newcommand{\data}{\mathcal{S}}
\newcommand{\measure}{P}
\newcommand{\measuren}{P^n}


\title{Better-Than-Chance Classification for Signal Detection}


\begin{document}
%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?


\maketitle
\linenumbers

\begin{abstract}
[TODO]
\end{abstract}


%%%% Introduction %%%
\section{Introduction}
\label{sec:introduction}

A common workflow in neuroimaging consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Examples in the neuroscientific literature include \citet{golland_permutation_2003,pereira_machine_2009,varoquaux_assessing_2016}, and especially the recently popularized \emph{multivariate pattern analysis} (MVPA) framework of \citet{kriegeskorte_information-based_2006}.
This practice is also observed in very high profile publications in the genetics literature: 
\citet{golub_molecular_1999,slonim_class_2000,radmacher_paradigm_2002,mukherjee_estimating_2003,juan_prediction_2004,jiang_calculating_2008}.


To fix ideas, we will adhere to a concrete example.
In \cite{gilron_quantifying_2016}, the authors seek to detect brain regions which encode differences between vocal and non-vocal stimuli. 
Following the MVPA workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the spatial activation pattern significantly better than chance, then a region is declared to encode vocal/non-vocal information. 
We call this an \emph{accuracy test}, a.k.a.\ \emph{class prediction}, or \emph{pattern discrimination} 

This same signal detection task can be also approached as a two-group multivariate test.
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
As put in \cite{pereira_machine_2009}: 
\begin{quote}
... the problem of deciding whether the classifier learned to discriminate the classes can be subsumed into the more general question as to whether there is evidence that the underlying distributions of each class are equal or not.
\end{quote}
A practitioner may then call upon a two-group population test such as Hotelling's $T^2$ \citep{anderson_introduction_2003}.
Alternatively, if the size of a brain region is large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of Hotelling's test can be called upon, such as in \cite{schafer_shrinkage_2005} or \cite{srivastava_multivariate_2007}.
For brevity, and in contrast to \emph{accuracy tests}, we will call any two-sample multivariate tests simply \emph{population tests}, also termed \emph{class comparisons}. [TODO: rename to parameter test?]


At this point, it becomes unclear which is preferable: a population test or an accuracy test?
The former with a heritage dating back to \cite{hotelling_generalization_1931}, and the latter being extremely popular, as the $959$ citations\footnote{GoogleScholar. Accessed on Aug 4, 2016.} of \cite{kriegeskorte_information-based_2006} suggest. 

The comparison between location and accuracy tests was precisely the goal of \cite{ramdas_classification_2016}, who compared the $T^2$ population test to the accuracy of \emph{Fisher's linear discriminant analysis} classifier (LDA). 
By comparing the rates of convergence of the powers to $1$, \cite{ramdas_classification_2016} concluded that accuracy and population tests are rate equivalent. 

Asymptotic relative efficiency measures (ARE) are typically used by statisticians to compare between rate-equivalent test statistics \citep{vaart_asymptotic_1998}.
\cite{ramdas_classification_2016} derive the asymptotic power functions of the two test statistics, which allows to compute the ARE between Hotelling's $T^2$ (location) test and Fisher's LDA (accuracy) test.
Theorem~14.7 of \cite{vaart_asymptotic_1998} relates asymptotic power functions to ARE.
Using the results of \cite{ramdas_classification_2016} we deduce that the ARE is lower bounded by $2 \pi \approx 6.3$. 
This means that Fisher's LDA requires at least $6.3$ more samples to achieve the same (asymptotic) power than the $T^2$ test. 
In this light, the accuracy test is remarkably inefficient compared to the population test.  
For comparison, the t-test is only $1.04$ more (asymptotically) efficient than Wilcoxon's rank-sum test \citep{lehmann_parametric_2009}, so that an ARE of $6.3$ is strong evidence in favor of the population test. 

Before discarding accuracy tests as inefficient, we recall that \cite{ramdas_classification_2016} analyzed a \emph{half-sample} holdout. 
The authors conjectured that a leave-one-out approach, which makes more efficient use of the data, may have better performance. 
Also, the analysis in \cite{ramdas_classification_2016} is asymptotic. 
This eschews the discrete nature of the accuracy statistic, which will be shown to have  crucial impact. 
Since typical sample sizes in neuroscience are not large, we seek to study which test is to be preferred in finite samples? 
Our conclusion will be quite simple: {\em population tests almost always have more power than accuracy tests}.

Our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Permutation testing with discrete test statistics are known to be conservative \citep{hemerik_exact_2014}, since they are insensitive to mild perturbations of the data, and they cannot exhaust the permissible false positive rate. 
The degree of discretization is governed by the number of samples. 
In our neuroscience example from \citet{gilron_quantifying_2016}, the classification is performed based on $40$ trials, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of trial-repeats, or the number of subjects in an neuroimaging study. 

The discretization effect is aggravated if the test statistic is highly concentrated. 
For an intuition consider the usage of a the \emph{resubstitution accuracy} as a test statistic. 
This statistic simply means that the accuracy is not cross validated. 
If the data is high dimensional, the resubstitution accuracy will be very high due to over fitting. 
In a very high dimensional model, the resubstitution accuracy will be $1$ for the observed data \cite[Theorem 1]{mclachlan_bias_1976}, but also for any permutation.
The concentration of resubstitution accuracy near $1$, and its discreteness, render this test completely useless, with a power tending to $0$ for any (fixed) effect size, as the dimension of the model grows. 


To compare the power of accuracy tests and population tests in finite samples, we perform a simulation study of a battery of test statistics. 
We start with formalizing the problem in Section~\ref{sec:problem_setup}.
The main findings are reported in Sections~\ref{sec:power} and \ref{sec:example}. 
A discussion follows in Section~\ref{sec:discussion}. 



%%%% Section %%%%
\section{Problem setup}
\label{sec:problem_setup}

Let $\outcomes \in \outcomeS$ be a class encoding. 
Let $\features \in \featureS$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example we have $\outcomeS=\set{-1,1}$ and $p$, the number of voxels in a brain region so that $\featureS=\reals^{27}$. 

Given $n$ pairs of $(\features_i,\outcomes_i)$, typically assumed i.i.d., a population test amounts to testing whether $\features|\outcomes=1$ has the the same distribution as $\features|\outcomes=-1$. 
I.e., we test if the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 

An accuracy test amounts to learning a predictive model and testing if its predictions are better than chance. 
Denoting a dataset by $\data:=(\features_i,\outcomes_i)_{i=1}^n$, and a predictor by $\hyp$, then a learning algorithm $\algo$ is a mapping $\algo:\data \to \hyp$. 
The accuracy of predictor $\hyp=\algo_{\data}$ is defined as the probability of $\hyp$ making a correct prediction.
Denoting by $\measure$ the probability measure of $(\x,\y)$, and by $\measuren$ the same for the i.i.d, sample $\data$
\begin{align}
	\acc_{\algo_\data}:=\measure(\algo_\data(x)=y).
\end{align}
The accuracy of an algorithm $\algo$ is defined as the average accuracy, over all possible data sets
\begin{align}
	\acc_{\algo}:=\int_\data \acc_{\data} \; d\measuren(\data)
\end{align}


The accuracy of an algorithm, $\acc_\algo$ is defined as the a

Denoting an estimate of $\acc_{\hyp}$ by $\accEstim_{\hyp}$, a statistically significant ``better than chance'' estimate of $\accEstim_{\hyp}$ is evidence that the classes are distinct. 


\subsection{Candidate Tests}
\label{sec:considerations}

The design of a permutation test using $\accEstim_{\hyp}$, requires the following design choices: 
\begin{enumerate}
\item Is $\accEstim_{\hyp}$ cross validated or not?
\item For a V-fold cross validated test statistic:
\begin{enumerate}
\item Should the data be refolded in each permutation? 
\item Should the data folding be balanced (a.k.a.\ stratified)?
\item How many folds? 
\end{enumerate}
\item How to estimate $\accEstim_{\hyp}$?
\end{enumerate}

We will now address these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of $\accEstim_{\hyp}$, but rather in the mere detection of a difference between two classes. 

\paragraph{Cross validate or not?}
Given our goal, a biased error estimate is not a problem provided that bias is consistent over all permutations. 
The underlying intuition is that if the exact same computation is performed over all permutations, then a permutation test will be ``fair'', i.e., will not inflate the false positive rate. 
We will thus be considering both cross validated accuracies, and \emph{resubstitution accuracies}, where the accuracy is evaluated on the training set and not on a holdout.


\paragraph{Balanced folding?}
The standard practice when cross validating is to constrain the data folds to be balanced, i.e. stratified \citep[e.g.][]{ojala_permutation_2010}.
This means that each fold has the same number of examples from each class. 
We will report results with both balanced and unbalanced data foldings, only to discover, it does not really matter. 


\paragraph{Refolding?}
The standard practice in neuroimaging is to permute labels and refold the data after each permutation, so that the balance of the classes in each fold is preserved.
We will adhere to this practice due to its popularity, even though it can be simplified by permuting features instead of labels, as done by \citet{golland_permutation_2005}.


\paragraph{How many folds?}
Different authors suggest different rules for the number of folds. 
We will be varying the number of folds, and ultimately discover that the power \emph{decreases with the number of folds}. 

\paragraph{How to estimate accuracy?}
\label{sec:estimate_accuracy}
Given a predictor for the $i$'th data example\footnote{We add the unorthodox observation index $i$ to $\hyp_i$ to accommodate for cross validation methods, where each prediction is made with a different classifier, fit to a slightly different dataset.}, $\hyp_i$, a natural test statistic its empirical accuracy 
\begin{align}
\label{eq:accuracy_estim}
	\accEstim_{\hyp}:= \sum_i I(\hyp_i(x_i)=y_i).
\end{align}
Since low accuracies, even $0$, are evidence that the classes are separated, we can consider the departure from chance level $|\accEstim_{\hyp}-0.5|$, as another candidate test statistic.
For unbalanced classes, chance level is not $0.5$, but rather the the probability of the majority class, we denote by $\majority$.
This suggests the following test statistic $|\accEstim_{\hyp}-\majority|$.
Since we will be aggregating these statistics over random data sets where $\majority$ may vary, it seems appropriate to standardize the scale. 
We thus propose the z-scored accuracy statistic, $\accZ_\hyp$:
\begin{align}
\label{eq:z_scored_accuracy}
	\accZ_{\hyp} := |\accEstim_{\hyp}-\majority|/\sqrt{\majority(1-\majority)}.
\end{align} 

Table~\ref{tab:collected} collects an initial battery of tests we will be comparing. 
\begin{tcolorbox}
\centering
\begin{tabular}{l|c|c|c|c}
Name & Basis & CV & Accuracy & Parameters\\ 
\hline
\hline
Hotelling & Hotelling & -- & -- & -- \\ 
Hotelling.shrink & Hotelling & -- & -- & -- \\ 
lda.CV.1 & LDA & V-fold & $\accEstim$ &  -- \\ 
lda.CV.2 & LDA & V-fold & $\accZ$ & -- \\ 
lda.noCV.1 & LDA & -- & accuracy &  --\\ 
lda.noCV.2 & LDA & -- & z-accuracy &  --\\ 
sd & SD & -- & -- & -- \\ 
svm.CV.1 & SVM & V-fold & accuracy & cost=1e1 \\ 
svm.CV.2 & SVM & V-fold & accuracy & cost=1e-1 \\ 
svm.CV.3 & SVM & V-fold & z-accuracy & cost=1e1 \\ 
svm.CV.4 & SVM & V-fold & z-accuracy & cost=1e-1 \\ 
svm.noCV.1 & SVM & -- & accuracy & cost=1e1 \\ 
svm.noCV.2 & SVM & -- & accuracy & cost=1e-1 \\ 
svm.noCV.3 & SVM & -- & z-accuracy & cost=1e1 \\ 
svm.noCV.4 & SVM & -- & z-accuracy & cost=1e-1 \\
\end{tabular} 
\captionsetup{type=table}
\caption{\footnotesize
This table collects the various test statistics we will be studying. 
Three are population tests: Hotelling, Hotelling.shrink, and sd.
\textit{Hotelling} is the classical two-group $T^2$ statistic. 
\textit{Hotelling.shrink} is a high dimensional version with the regularized covariance in \cite{schafer_shrinkage_2005}. 
\textit{sd} is another high dimensional version of the $T^2$, from \cite{srivastava_two_2013}. 
The rest of the tests are variations of the linear SVM, and Fisher's LDA, with varying accuracy measures, cross validated or not, and varying tuning parameters. 
For example, \textit{svm.CV.4} is a linear SVM implemented with the \emph{svm} \R function,
the cost parameter set at $0.1$, and using the cross validated z-scored accuracy in Eq.~\ref{eq:z_scored_accuracy}.
Another example is \textit{lda.noCV.1}, which is Fisher's LDA, returning the resubstitution accuracy.}
\label{tab:collected}
\end{tcolorbox}







%%%% Section %%%%
\section{Controlling the False Positive Rate}
\label{sec:type_i}

Figure~\ref{fig:simulation_1} demonstrates that all of the tests considered conserve the desired $0.05$ false positive rate, up to varying levels of conservatism.
This can be seen from the fact that the probability of rejection is no larger than $0.05$ in the absence of any effect, encoded by a red circle. 
This is true, in particular if:
(a) the folds are balanced or not,
(b) the tuning parameters of some test statistic are varied,
(d) the number of folds is varied.
We also observe that the most conservative tests are the resubstitution accuracy statistics. 
We return to this matter in the Discussion.




\begin{figure}[h]
\centering
\caption{
The power of a permutation test with various test statistics. 
The power on the $x$ axis. 
Effect are color and shape coded. 
The various statistics on the $y$ axis. 
Their details are given in Table~\ref{tab:collected}. 
Effects vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
Simulation details in Appendix~\ref{apx:simulation_details}.
Cross-validation was performed with balanced and unbalanced data folding. See sub-captions.}	
\label{fig:simulation_1}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=1\linewidth]{"art/2016-07-26 20:55:48"}
\caption{\textbf{Unbalanced.}} %TODO: caption  
\label{fig:simulation_11}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
\caption{\textbf{Balanced.}} %TODO: caption 
\label{fig:simulation_12}
\end{subfigure}
\end{figure}




%%%% Section %%%%
\section{Power}
\label{sec:power}

Having established that all of the tests in our battery control the false positive rate, it remains to be seen if they have similar power-- 
especially when comparing population tests to accuracy tests. 
From the simulation results reported in Appendix~\ref{apx:simulations} we collect the following insights:
\begin{enumerate}
\item population tests have more power than accuracy tests in all our configurations.

\item The conservativeness decays as the sample grows (Figures~\ref{fig:large_sample_1}, \ref{fig:large_sample_2} and \ref{fig:large_sample_3})

\item For heavy tailed distributions (Figure~\ref{fig:t_null}), the extra power of the location test vanishes. 

\item The presence of correlations between coordinates reduces the signal to noise ratio (SNR), thus reduces power. 
More importantly, in the presence of correlations the effect of regularization is amplified, increasing the power difference between regularized and non-regularized test statistics.
Put differently- in low SNR regimes, regularization proves crucial (Figure~\ref{fig:ar_1}).

\item The z-scoring of the accuracies was introduced to deal with unbalanced foldings. 
If the z-scoring has any effect at all, it merely kills power.

\item Both accuracy and population tests are inappropriate for scale alternatives (Figure~\ref{fig:scale_change}). 
This was to be expected and is reported mostly as a sanity check.

\item Balanced folding only affects the z-scored accuracy, in the opposite direction than we anticipated. 

\item Increasing the SVM's cost parameter, which reduces the number of support vectors entering the classifier, reduces power. 

\end{enumerate}


The major insight from simulations is that the use of accuracy tests for signal detection is underpowered compared to population tests. 
We now verify this finding on a neuroimaging dataset.



\section{Neuroimaging Example}
\label{sec:example}
% Power of SVM versus SD.
% Difficulties of implementation.

Figure~\ref{fig:read_data} is an application of both a location and an accuracy test to the data of \cite{pernet_human_2015}. 
The authors of \cite{pernet_human_2015} collected fMRI data while subjects were exposed to the sounds of human speech (vocal), and other non-vocal sounds. 
Each subject was exposed to $20$ sounds of each type, totaling in $n=40$ trials in each scan.
The study was rather large and consisted of about $200$ subjects.
The data was kindly made available by the authors at the OpenfMRI website\footnote{\url{https://openfmri.org/}}.

We perform group inference using within-subject permutations along the analysis pipeline of \cite{stelzer_statistical_2013}, which was also reported in \cite{gilron_quantifying_2016}. 
For completeness, the pipeline is described in Appendix~\ref{apx:analysis}. 
To demonstrate our point, we compare the \emph{sd} population test with the \emph{svm.cv.1} accuracy test. 

In agreement with our simulation results, the population test (\emph{sd}) discovers more brain regions of interest when compared to an accuracy test (\emph{svm.cv.1}).
The former discovers $1,232$ regions, while the latter only $441$, as depicted in Figure~\ref{fig:read_data}.
We emphasize that both test statistics were compared with the same permutation scheme, and the same error controls, so that any difference in detections is due to their different power.

Having established that accuracy tests are typically underpowered for signal detection compared to population tests, we wish to identify the conditions under which this will occur, and discuss practical implications. 


\begin{figure}[th]
\centering
\includegraphics[width=0.7\linewidth]{"art/svm_vs_SD"}
\caption{\footnotesize
Brain regions encoding information discriminating between vocal and non-vocal stimuli.
Map reports the centers of $27$-voxel sized spherical regions, as discovered by an accuracy test (\emph{svm.cv.1}), and a population test (\emph{sd}). 
\emph{svm.cv.1} was computed using $5$-fold cross validation, and a cost parameter of $1$. 
Region-wise significance was determined using the permutation scheme of \cite{stelzer_statistical_2013}, followed by region-wise $FDR \leq 0.05$ control using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}.
Number of permutations equals $400$.
The population test detect $1,232$ regions, and the accuracy test $441$, $399$ of which are common to both.
For the details of the analysis see Appendix~\ref{apx:analysis} and \cite{gilron_quantifying_2016}.  
  }
\label{fig:read_data}
\end{figure}








%%%% Section %%%%
\section{Discussion}
\label{sec:discussion}

We have set out to understand which of the tests is more powerful: the accuracy test or the population test. 
No amount of simulations can replace the insight provided by a good closed-form analytic result. 
The finite sample power of permutation tests is a formidable mathematical problem, so we currently content ourselves with simulations 
We have concluded that the population tests are typically preferable. 
Their high dimensional versions, such as \cite{srivastava_multivariate_2007} and \cite{schafer_shrinkage_2005},  are particularly well suited for neuroimaging problems such as MVPA.
We attribute this to several phenomena: 
(a) Discretization introduced in finite samples by the accuracy test statistic. 
(b) Inefficient use of the data for the validation holdout set. 
(c) Regularization crucial in high dimensional problems.

The presence of heavy tails shrinks the power advantage of the population tests over accuracy tests. 
Our empirical example suggests that even if the population test does not necessarily dominate the accuracy test in power, empirically, it does have an advantage. 


The degree of discretization is governed by the sample size. 
For this reason, an asymptotic analysis such as \cite{ramdas_classification_2016} may uncover the holdout inefficiency, but will not uncover the discretization effect. 

The practical advice for the practitioner, is that for the purpose of signal detection, there is typically a population test that is more powerful than an accuracy test. 
There is also a good chance that it would be easier to implement, and faster to run, since no cross validation will be involved. 



\subsection{Ease of implementation}
A very important consideration is the ease of implementation. 
The need for cross validation of the accuracy test greatly increases its computational complexity. 
Moreover, anyone who has actually implemented tests with discrete statistics, will attest they are more prone to programming errors. 
This is because their unforgiveness to the type of inequalities used. 
Indeed, mistakenly replacing a weak inequality with a strong inequality in one's program may considerably change the results. 
This is not the case for continuous test statistics. 




\subsection{Reservations}
\label{sec:reservations}

Some reservations to the generality of our findings are in order. 
Firstly, not all accuracy tests are concerned with signal detection.
Consider brain decoding for machine interfaces, or clinical diagnosis, where the presence of a medical condition is predicted from imaging data \citep[e.g.][]{olivetti_induction_2012,wager_fmri-based_2013}. 
In those examples, the purpose of the test is not to detect a difference between classes, but to actually test the performance of a particular classifier.  

Secondly, it may be argued that accuracy tests permits the separation between classes in high dimensions, such as in \emph{reproducing kernel Hilbert spaces} (RKHS) by using non-linear predictors. 
This is a false argument-- accuracy test do not have any more flexibility that population tests. 
Indeed, it is possible to test for location in the same dimension the classifier is learned. 
\citet{gretton_kernel_2012-1} is an example where the test for location is performed in the RKHS of the data.
It is also possible to test for the equality of two multivariate distributions [TODO: cite  vogelstein].
On the other hand, based on our reported neuroimaging example, and others, we find that a population test in the original feature space is indeed a simple and powerful approach to signal detection.







\subsection{A good accuracy test}
For the cases a population test cannot replace an accuracy test, we collect some conclusions and best practices from our simulations.
We give particular emphasis in this section to V-fold cross validation due to it popularity, but note that sampling the test set with replacement is actually preferable, as we discuss in Section~\ref{sec:bootstrap}.

\paragraph{Sample size.} The conservativeness of accuracy tests decrease with sample size. 

\paragraph{Permute features.} Permuting features is easier than permuting labels. 
It allows to preserve balanced folds after a permutation without refolding.
Although we not we did not find a power difference between balanced and unbalanced foldings. 

\paragraph{Use less folds.} For V-fold CV, power decreases as the number of folds increases. 
This is quite interesting since two phenomena compete as the number of folds increase:
(a) the train set is larger so that better accuracies are achievable. 
(b) The test set is smaller so that the accuracy estimate is more variable. 
The decrease in power with increase fold number suggests that the latter dominates the former. 
Put differently: it is easier to detect a small stable departure from chance level, than a large but unstable one.


\paragraph{Resubstitution accuracy in low dimension.} Resubstitution accuracy useful in low dimension.
In high dimension, the power loss is considerable compared to a cross validated approach. 
We attribute this to the compounding of discretization and concentration effects: the difference between the sampling distribution of the resubstitution accuracy is simply indistinguishable under the null and under the alternative. 
In low dimensional problems, the discretization is less impactful, and the computational burden of cross validation can be avoided by using the resubstitution accuracy. 
There is a fundamental difference between V-folding and resubstitution. The latter should not be thought of as the limit of the former. 

\paragraph{Regularize}
Regularizing the accuracy test proves very useful in high dimensional problems. 
Put differently: reducing variance by adding some bias is very useful to detect better-than-chance classification. 

\paragraph{Don't z-score.} There is no gain in z-scoring the accuracy scores. Our motivating rational was clearly flawed. [TODO: why?]






\subsection{Smoothing accuracy estimates}
\label{sec:bootstrap}
It may be possible to alleviate the effect of discretization by appropriate cross-validation. 
The discreteness of the accuracy statistic is governed by the number of examples in the union (over all validation iterations) of test sets.
For V-fold CV, for instance, this number is simply the sample size. 
This suggests that the accuracy can be ``smoothed'' by allowing the test sample to be drawn with replacement. 
The \emph{bootstrap} may seem like a good candidate approach since it samples examples with replacement. 
It does so, however, for the train set, and not the test set. 
An algorithm that samples test sets with replacement is the \emph{leave-one-out bootstrap estimator} (bLOO) and its derivation-- the \emph{0.632 bootstrap estimator} (b$0.632$) \citep[Sec 7.11]{hastie_elements_2003}.
\begin{definition}[bLOO]
\label{def:bloo}
Denoting by $C^{(i)}$ the index set of bootstrap samples, $b$, where observation $i$ is not in the train set,
and by $\hypEstim^b$ the classifier fitted to the $b$'th bootstrap training sample, 
then the \emph{leave-one-out bootstrap} estimate is defined as:
$$
	\acc_{bLOO}:= \frac 1n \sum_{i=1}^{n} \frac{1}{|C^{(i)}|} \sum_{b \in C^{(i)}} I(\hypEstim^b(x_i)=y_i).
$$ 
Equivalently, denoting by $S^{(b)}$ the indexes of observations, $i$, that are not in the bootstrap train sample $b$,
$$
	\acc_{bLOO}= \frac 1B \sum_{b=1}^{B} \frac{1}{|S^{(b)}|} \sum_{i \in S^{(b)}} I(\hypEstim^b(x_i)=y_i).
$$
\end{definition}

\begin{definition}[b$0.632$]
\label{def:b0632}
Denoting by $\acc_{resub}$ the resubstitution accuracy estimate, 
the b$0.632$ accuracy estimator, $\acc_{0.632}$, is defined as 
$$
	\acc_{0.632} := 0.368 \; \acc_{resub}  + 0.632 \; \acc_{bLOO}.
$$
\end{definition}

Simulation results reported in Figure~\ref{fig:bootstrap} with naming conventions in Table~\ref{tab:collected_2}.
It can be seen that selecting test sets with replacement does increase the power, when compared to V-fold cross validation, but still falls short from the power of population tests. 
It can also be seen that power increases with the number of bootstrap replications, itself reducing the level of discretization. 
The type of bootstrap, bLOO versus b$0.632$, does not change the power. 

\bigskip

\begin{tcolorbox}
\centering
\begin{tabular}{l|c|c|c|c|c}
Name & Basis & Type & B & Accuracy & Parameters\\ 
\hline
\hline
lda.Boot.1 & LDA & b$0.632$ 	& 10 & accuracy &  -- \\ 
lda.Boot.2 & LDA & bLOO 	& 10 & accuracy &  -- \\ 
svm.Boot.1 & SVM & b$0.632$ 	& 10 & accuracy & cost=1e1 \\ 
svm.Boot.2 & SVM & bLOO 	& 10 & accuracy & cost=1e1 \\ 
svm.Boot.3 & SVM & b$0.632$ 	& 50 & accuracy & cost=1e1 \\ 
svm.Boot.4 & SVM & bLOO 	& 50 & accuracy & cost=1e1 \\ 
\end{tabular} 
\captionsetup{type=table}
\caption{
The same as Table~\ref{tab:collected} for bootstraped accuracy estimates. 
bLOO and b$0.632$ are defined in definitions~\ref{def:bloo} and \ref{def:b0632} respectively.
$B$ denotes the number of Bootstrap samples. } 
\label{tab:collected_2}
\end{tcolorbox}


\begin{figure}[ht]
\centering
	  \includegraphics[width=0.7\linewidth]{"art/2016-08-09 20:09:20"}
	  \caption{
		  \textbf{Bootstrap--}
		  The power of a permutation test with various test statistics. 
		  The power on the $x$ axis. 
		  Effect are color and shape coded. 
		  The various statistics on the $y$ axis. 
		  Their details are given in tables~\ref{tab:collected} and \ref{tab:collected_2}. 
		  Effects vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
		  Simulation details in Appendix~\ref{apx:simulation_details}.
		  } 
	\label{fig:bootstrap}
\end{figure}


\subsection{High dimensional classifiers}
Inspecting Figure~\ref{fig:simulation_11} (for instance), it can be seen that Hotelling's $T^2$ test has similar power to accuracy tests. 
It should thus be argued that the real advantage of the population tests is due to their adaptation to high dimension by regularization (\emph{sd} and \emph{Hotelling.shrink}), and not only to discretization.
To study this, we call upon several regularized classifiers, designed for high dimensional problems. 
In the spirit of the regularized covariance of \emph{Hotelling.shrink}, we try an $l_2$ regularized svm \cite{friedman_regularization_2010}, and shrinkage based LDA \citep{pang_shrinkage-based_2009,ramey_high-dimensional_2016}. %TODO: verify references
In the spirit of the diagonalized covariance of \emph{sd}, we try a diagonalized LDA \citep{dudoit_comparison_2002}, which can be thought of a method intersecting Fisher's LDA and Naive Bayes. 


Simulation results reported in Figure~\ref{fig:highdim} with naming conventions in Table~\ref{tab:collected_3}.
It can be seen that regularizing a classifier in high dimension, just like a parameter test, improves power. 
It can also be seen that (regularized) parameter tests are still more powerful than (regularized) accuracy tests. 
This was to be expected, since we already saw in (e.g. Figure~\ref{fig:simulation_11}) that the unregularized parameter test, \emph{Hotelling}, is slightly more powerful than the regularized accuracy test, \emph{svm.CV.1} for instance.

We can compound regularization in this section with the bootstrapping from Section~\ref{sec:bootstrap}, to improve finite sample power of the accuracy tests. 
This is done in the \emph{svm.highdim.2} test, which still falls short from the power of the location tests, but is a much more powerful accuracy test than the original non-regularized, V-fold validated, version of \emph{svm.CV.1}.

\bigskip

\begin{tcolorbox}
\centering
\begin{tabular}{l|c|c|c|c}
Name & Basis & CV & Accuracy & Parameters\\ 
\hline
\hline
svm.highdim.1 & SVM & V-fold & accuracy & cost=1e-1 \\ 
svm.highdim.2 & SVM & B=50 & accuracy & cost=1e-1 \\ 
lda.highdim.1 & LDA & V-fold & accuracy & -- \\ 
lda.highdim.2 & LDA & V-fold & accuracy & -- \\ 
lda.highdim.3 & LDA & V-fold & accuracy & -- \\ 
\end{tabular} 
\captionsetup{type=table}
\caption{
The same as Table~\ref{tab:collected} for regularized (high dimensional) predictors. 
\emph{svm.highdim.1} is an $l2$ regularized SVM \cite{friedman_regularization_2010}. 
\emph{svm.highdim.2} is the same with b$0.632$ instead of V-fold cross validation. 
\emph{lda.highdim.1} is the Diagonal Linear Discriminant Analysis of \cite{dudoit_comparison_2002}.
\emph{lda.highdim.2} is the High-Dimensional Regularized Discriminant Analysis of \cite{ramey_high-dimensional_2016}.
\emph{lda.highdim.3} is the Shrinkage-based Diagonal Linear Discriminant Analysis of \cite{pang_shrinkage-based_2009}.
} 
\label{tab:collected_3}
\end{tcolorbox}


\begin{figure}[ht]
\centering
	  \includegraphics[width=0.7\linewidth]{"art/2016-08-13 22:55:43"}
	  \caption{
\textbf{HighDim Classifier--} 
		The power of a permutation test with various test statistics. 
		The power on the $x$ axis. 
		Effect are color and shape coded. 
		The various statistics on the $y$ axis. 
		Their details are given in tables~\ref{tab:collected} and \ref{tab:collected_3}. 
		Effects vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
		Simulation details in Appendix~\ref{apx:simulation_details}.
} 
	\label{fig:highdim}
\end{figure}






\subsection{Related Literature}
\cite{ojala_permutation_2010} study the power of two accuracy tests: one testing the ``no signal'' null hypothesis, and the other testing the ``independent features'' null hypothesis. 
They perform an asymptotic analysis, and a simulation study. 
They also apply various classifiers to various data sets. 
Their emphasis is the effect of the underlying classifier on the power, and the potential of the ``independent features'' test for feature selection.
This is a very different emphasis from our own.


\cite{olivetti_induction_2012} and \cite{olivetti_statistical_2014} looked into the problem of choosing a good accuracy test. 
They propose a new test they call an \emph{independence test}, and demonstrate by simulation that it has more power than other accuracy tests, and can deal with non-balanced data sets. 
We did not include this test in the battery we compared, but we note the following: 
(a) The independence test of \cite{olivetti_induction_2012} relies on a discrete test statistic. It may thus be improved with the methods discussed in this section, before the application of \cite{olivetti_induction_2012}'s independence test. 
(b) In contrast with the underlying motivation of \cite{olivetti_induction_2012}'s independence test, we did not find that balancing the data folds is crucial for an accuracy test. 


\cite{golland_permutation_2005} study accuracy tests using simulation, neuroimaging data, genetic data, and analytically.
Their analytic results formalize our intuition from Section~\ref{sec:introduction} on the effect of concentration of the accuracy statistic:
The finite Vapnikâ€“Chervonenkis (VC) dimension requirement \citep[Sec 4.3]{golland_permutation_2003} prevents the permutation p-value from (asymptotically) concentrating near $1$. 
Like ourselves, they also find that the power increases with the size of the test set (Figure~4, middle). 
This is seen in their Figure~4, where the size of the test-set, $K$, governs the discretization. 
Since they permute features, not labels, then all their permutation samples are balanced, and there is no issue of refolding. 

\cite{golland_permutation_2005} simulate the power of accuracy tests by sampling from a Gaussian mixture family of models, and not from a location family as our own simulations. 
Under their model 
$(x_i|y_i=1) \sim p \gauss{\mu_1,I}+ (1-p) \gauss{\mu_2,I}$ 
and 
$(x_i|y_i=-1) \sim (1-p) \gauss{\mu_1,I}+ p \gauss{\mu_2,I}$.
Varying $p$ interpolates between the null distribution $(p=0.5)$ and a location shift model $(p=0)$. 
We now perform the same simulation as \cite{golland_permutation_2005}, after parameterizing $p$ so that $p=0$ corresponds to the null model, and in the same dimensionality as our previous simulations
We find that also in this mixture class of models a population test has more power than an accuracy test (Figure~\ref{fig:golland}).



\begin{figure}[ht]
\centering
	  \includegraphics[width=0.7\linewidth]{"art/2016-08-08 07:33:05"}
	  \caption{\textbf{Mixture--} $\x_i = \chi_i \mu + \eta_i; \chi_i = \set{-1,1}$ and $\prob{\chi_i=1}=(1/2-p)^{\y^*_i}  (1/2+p)^{1-\y^*_i}$. $\mu$ is a $p$-vector with $3/\sqrt{p}$ in all coordinates.
	  The effect, $p$, is color and shape coded and varies over $0$ (red circle), $1/4$ (green triangle) and $1/2$ (blue square). }
	\label{fig:golland}
\end{figure}










\subsection{Epilogue}
Given all the above, we find the popularity of accuracy tests quite puzzling. 
We believe this is due to a reversal of the inference cascade. 
Researchers first fit a classifier, and then ask if the classes are any different.
Were they to start by asking if classes are any different, and only then try to classify, then population tests would naturally arise as the preferred method. 
As put by \cite{ramdas_classification_2016}:
\begin{quote}
The recent popularity of machine learning has resulted in the extensive teaching and use
of prediction in theoretical and applied communities and the relative lack of awareness or
popularity of the topic of Neyman-Pearson style hypothesis testing in the computer science
and related ``data science'' communities.
\end{quote}
And more simply by Frank Harrell in the \textsf{CrossValidated} Q\&A site\footnote{\url{http://stats.stackexchange.com/questions/17408/how-to-assess-statistical-significance-of-the-accuracy-of-a-classifier}.}:
\begin{quote}
 ... your use of proportion classified correctly as your accuracy score. This is a discontinuous improper scoring rule that can be easily manipulated because it is arbitrary and insensitive.
\end{quote}






\section{Acknowledgments}
% isf 900/60, Jelle, Jesse B.A. Hemerik, Yakir Brechenko, Omer Shamir, Joshua Vogelstein, Gilles Blanchard, Jason 




\newpage
\bibliographystyle{abbrvnat}
\bibliography{Permuting_Accuracy.bib}

\appendix


\newpage

\section{Analysis pipeline}
\label{apx:analysis}

Here is the analysis pipeline of \cite{stelzer_statistical_2013} we for the auditory data in \cite{gilron_quantifying_2016}.
Denoting by 
$i=1,\dots,I$ the subject index, 
$v=1,\dots,V$ the voxel index, and 
$s = 1,\dots,S$ the permutation index. 
Since regions\footnote{\emph{searchlight} or \emph{sphere} in the MVPA parlance} are centered around a unique voxel, the voxel index $v$ also serves as a unique region index.
Algorithm~\ref{algo:statistic} computes a region-wise test statistic, which is compared to its permutation null distribution computed by Algorithm~\ref{algo:permutation}.


\begin{algorithm}[H]
\caption{Compute a group parametric map.}
\label{algo:statistic}

 \KwData{fMRI scans, and experimental design.}
 \KwResult{Brain map of group statistics: $\{\bar{T}_v\}_{v=1}^V$}
	 \For{$v \in 1,\dots,V$}{
		 \For{$i \in 1,\dots,I$}{
			 $T_{i,v} \leftarrow$ test statistic for subject $i$ in a region centered at $v$.
			 } 	  
	  	 $\bar{T}_{v} \leftarrow \frac{1}{I}\sum_{i=1}^I T_{i,v}$. 
 	 }
\end{algorithm}


\begin{algorithm}[H]
\caption{Compute a permutation p-value map.} 
\label{algo:permutation}

 \KwData{fMRI scans of $20$ subjects, experimental design.}
 \KwResult{Brain map of permutation p-values: $\{p_v\}_{v=1}^V$}
  \For{$s \in 1,\dots\,S$}{
    	    permute labels\;
    	    $\bar{T}_{v}^s \leftarrow$ parametric map 
  
  	}
\end{algorithm}

\newpage


\section{Simulation Details}
\label{apx:simulation_details}

The following details are common to all the reported simulations, unless stated otherwise in a figure's caption. 
The \R code for the simulations can be found in [TODO].

Each simulation is based on $4,000$ replications. 
In each replication, we generate $n$ i.i.d. samples from a shift model $\x_i = \mu \y^*_i + \eta_i$.
Where $y^*_i=\set{0,1}$ is the class of subject $i$ in dummy coding. 
Recalling that $y_i=\set{-1,1}$ is the class in effect coding, then clearly $y_i=2 y^*_i-1$.
The noise is distributed as $\eta_i \sim \gaussp{p}{0,\Sigma}$. 
The sample size $n=40$. 
The dimension of the data is $p=23$. 
The covariance $\Sigma=I$. 
Effects, i.e. shifts $\mu$, are equal coordinate $p$-vectors with coordinates that vary over $\mu \in \set{0,1/4,1/2}$.

Having generated the data, we compute each of the test statistics in Table~\ref{tab:collected}.
For test statistics that require data folding, we used $8$ folds. 
We then compute a permutation p-value by permuting the class labels, and recomputing each test statistic. 
We perform $400$ such permutations. 
We then reject the $\mu_i=0$ null hypothesis if the permutation p-value is smaller than $0.05$.
The reported power is the proportion of replication where the permutation p-value falls below $0.05$.




\newpage

\section{Simulation Results}
\label{apx:simulations}



\begin{figure}[h]
\centering
\caption{\mycaption}	
\label{fig:n_folds}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{\textbf{2-fold} cross validation. Balanced folding.}  
	\label{fig:n_folds_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-29 07:18:24"}
	  \caption{\textbf{20-fold} cross validation. Balanced folding} 
	\label{fig:n_folds_2}
	\end{subfigure}
\end{figure}



\begin{figure}[h]
\centering
\caption{\mycaption}	
\label{fig:n_folds_unbalanced}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-05 09:37:35"}
	  \caption{\textbf{2-fold} cross validation. Unbalanced folding.} 
	\label{fig:n_folds_unbalanced_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-06 07:57:22"}
	  \caption{\textbf{20-fold} cross validation. Unbalanced folding.} 
	\label{fig:n_folds_unbalanced_2}
	\end{subfigure}
\end{figure}



\begin{figure}[h]
\centering
\caption{\mycaption}	
%\label{fig:simulation_1}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-30 10:33:05"}
	  \caption{\textbf{Scale Change--} $\x_i =  \eta_i * \mu^ {\y^*_i}$ so that the effect are a scale change.}  
	\label{fig:scale_change}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-04 19:32:17"}
	  \caption{\textbf{Heavytailed--} $\eta_i$ is not $p$-variate Gaussian, but rather $p$-variate t, with $df=3$ .  } 
	\label{fig:t_null}
	\end{subfigure}
\end{figure}




\begin{figure}[h]
\centering
\caption{\mycaption}	
\label{fig:large_sample}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05zoom"}
	  \caption{\textbf{Low-Dimension--} False positive rates for $n=40$.} 
	\label{fig:large_sample_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-04 13:59:33zoom"}
	  \caption{\textbf{High-Dimension--} False positive rates for $n=400$.} 
	\label{fig:large_sample_2}
	\end{subfigure}
\end{figure}




\begin{figure}[h]
\centering
\caption{\mycaption}	
	\begin{subfigure}{.4\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-11 08:32:39"}
	  \caption{\textbf{High-Dimension, local alternative--} $n=400$, $\mu \in \frac{1}{\sqrt{10}} \times \set{0,1/4,1/2}.$} 
	\label{fig:large_sample_3}
	\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{"art/2016-08-07 20:11:46"}
		  \caption{\textbf{AR(1) dependence--} $\Sigma_{k,l}=\rho^{|k-l|}; \rho=0.8$. } 
		\label{fig:ar_1}
	\end{subfigure}
\end{figure}




\end{document}
