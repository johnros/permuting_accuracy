\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{natbib}
\usepackage{url}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lineno}


\author{Jonathan Rosenblatt \and Roee Gilron \and Roy Mukamel}


%% OPTIONAL MACRO DEFINITIONS
%\def\s{\sigma}
\newcommand{\set}[1]{\{ #1 \}} % A set
\newcommand{\indicator}[1]{I_{\set{#1}}} % The indicator function.
\newcommand{\reals}{\mathbb{R}} % the set of real numbers
\newcommand{\features}{x} % The feature space
\newcommand{\outcomes}{y} % The feature space
\newcommand{\featureS}{\mathcal{X}} % The feature space
\newcommand{\outcomeS}{\mathcal{Y}} % The feature space
\newcommand{\hyp}{f} % A hypothesis
\newcommand{\hypEstim}{\hat{\hyp}} % A hypothesis
\newcommand{\hypclass}{\mathcal{F}}
\newcommand{\expect}[1]{\mathbf{E}\left[ #1 \right]} % The expectation operator
\newcommand{\acc}{T^{acc}} 
\newcommand{\dominant}{\hat{p}_{max}}
\newcommand{\prob}[1]{Prob( #1 )} % the probability of an event






\title{Better-than-chance classification for signal detection}


\begin{document}
%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?


\maketitle
\linenumbers

\begin{abstract}
[TODO]
\end{abstract}


%%%% Introduction %%%
\section{Introduction}
\label{sec:introduction}






A common workflow in genetics or neuroimaging consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Examples in the genetics literature include \cite{jiang_calculating_2008,radmacher_paradigm_2002} [TODO: elaborate].
Example sin the neuroscientific literature include \citep{golland_permutation_2003,kriegeskorte_information-based_2006,pereira_machine_2009,varoquaux_assessing_2016}.
The number of citations\footnote{Based on GoogleScholar. Accesses on 26.7.2016.} of these papers attest to the popularity of the above workflow: $956$ for \cite{kriegeskorte_information-based_2006}, and $274$ for \cite{radmacher_paradigm_2002}, as examples. 


To fix ideas, we will adhere to a neuroscientific example: 
In \cite{gilron_quantifying_2016}, the authors seek to detect brain regions which encode differences between vocal and non-vocal stimuli. 
According to the MVPA analysis workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the spatial activation pattern, significantly better than chance, then a region is declared to encode vocal/non-vocal information. 
We call this an \emph{accuracy test}, a.k.a.\ \emph{class prediction} in \cite{simon_pitfalls_2003}.

This same signal detection task can be also approached as a two-group multivariate test:
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
A practitioner may then call upon a two-group location test such as Hotelling's $T^2$ \citep{fujikoshi_multivariate_2011}.
Alternatively, if the size of the brain region is too large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of Hotelling's test can be called upon, such as in \cite{srivastava_testing_2013} or \cite{schafer_shrinkage_2005}.
In contrast to \emph{accuracy tests}, we call these \emph{location tests}, a.k.a. \emph{class comparison} in \cite{simon_pitfalls_2003}.

At this point, it becomes unclear which is the preferred test. 
The comparison between location and accuracy tests was precisely the topic of \cite{ramdas_classification_2016}, who compared the Hotelling location test to the accuracy of \emph{Fisher's linear discriminant analysis} classifier (LDA) \citep{hastie_elements_2003-1}. 
Using an asymptotic analysis, \cite{ramdas_classification_2016} concluded that accuracy and location tests are equivalent with respect to their order of convergence to a consistent test, while they may differ in constants. 
Put differently, the (asymptotic) relative efficiency of the tests is not trivially $0$ nor $\infty$.

The relative efficiency, governing the power of the tests, may prove crucial when dealing with the finite sample sizes in neuroscience and genetics, and thus the focus of this study. 
We thus seek to study which test is to be preferred in finite samples? 
Our conclusion will be quite simple: {\em location tests almost always have more power than accuracy tests}.

The main argument for our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Discrete test statistics are known to be conservative \citep{hemerik_exact_2014-1}, since they cannot exhaust the permissible false positive rate. 
For accuracy tests, the degree of discretization is governed by the number of samples. 
In our running neuroscience example \citep{gilron_quantifying_2016}, the classification is performed based on $40$ trials, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of subject in a genetic study, or the number of trial-repeats in an fMRI brain scan. 

The discretization effect is aggravated if the test statistic is highly concentrated. 
For an intuition consider the usage of the \emph{train} accuracy test statistic (i.e., not cross validated).
In Section~\ref{sec:power} we then address our main question- which test has more power? 
Based on the finding that the location test is typically more powerful, we try to offer an intuition for this phenomenon in the Discussion section. 






%%%% Section %%%%
\section{Problem setup}
\label{sec:problem_setup}

Adhering to our neuroscientific example, we now formalize terminology and notation. 
Let $\outcomes \in \outcomeS$ be a class encoding. In our vocal/non-vocal example we have $\outcomeS=\set{-1,1}$.
Let $\features \in \featureS$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example $p$ is the number of voxels in a brain region. We thus have $\featureS=\reals^{27}$. 

Given $n$ pairs of $(\features_i,\outcomes_i)$, typically assumed i.i.d., a location test amounts to testing whether $\features|\outcomes=1$ has the the same distribution as $\features|\outcomes=-1$ (or at least the same location). 
I.e., the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 
An accuracy test amounts to learning a predictive model $\hypEstim(\features)$ from some assumed model class $\hypEstim \in \hypclass$. 
The prediction accuracy, denoted $\acc_{\hypEstim}$, is defined as the probability of a given classifier $\hypEstim$ of making a correct prediction $\acc_{\hypEstim}:=\prob{\hypEstim(x)=y}$ when given a new, randomly drawn data point, ($\features,\outcomes$).
A statistically significant ``better than chance'' estimate of $\acc_{\hypEstim}$ is evidence that the classes are distinct. 


\subsection{Candidate Tests}
\label{sec:considerations}

The design of a permutation test using the prediction accuracy, requires the following design choices: 
\begin{enumerate}
\item How to estimate accuracy?
\item Is the statistic cross validated or not?
\item For a K-fold cross validated test statistic: should the data be refolded in each permutation? 
\item Permute labels of features?
\item For a K-fold cross validated test statistic: should the data folding balanced? (a.k.a. stratified).
\item How many folds? 
\end{enumerate}

We will now address these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of the prediction error, but rather in the mere detection of a difference between two groups, leading to a better-than-chance accuracy. 

\paragraph{How to estimate accuracy?}
Given a predictor $\hypEstim$, a natural test statistic is some estimate of its accuracy $\acc_{\hypEstim}$.
Complicating matters: very low accuracies, even $0$, is evidence that the classes are separated, and we only need to invert the predictions. 
We can thus consider $|\acc_{\hypEstim}-0.5|$ as the test statistic.
This, however, implies that if the classes are identical, random guessing has a $0.5$ accuracy. This is not true if the classes are not balanced. 
The chance level in which case is the prevalence of the dominant class, we denote by $\dominant$.
This suggests the following test statistic $|\acc_{\hypEstim}-\dominant|$.
Since we will be aggregating these statistic over random data sets where the dominant class may have varying frequencies, it seems appropriate to standardize the scale of this statistic. 
We thus also consider the z-scored accuracy: $|\acc_{\hypEstim}-\dominant|/\sqrt{\dominant(1-\dominant)}$.


\paragraph{Cross validate or not?}
Were we interested in an unbiased estimator of the prediction error, there is no question that some independent validation is in order. 
Since we are merely interested in detecting a difference between classes, a biased error estimate is not an issue provided that bias is consistent over all permutations. 
The underlying intuition is that if the exact same computation is performed over all permutations, then a permutation test will be ``fair'', i.e., will not inflate the false positive rate. 
We will thus be considering both cross validated accuracies, and \emph{train} accuracies as our test statistics. 


\paragraph{Refolding?}
The standard practice in neuroimaging is to refold the data after each permutation.
This is imperative if permuting labels while aiming at balanced data folds. 
This is not, however, imperative in general. 
For simplicity, we will adhere to the standard practice of refolding the data within each permutation.


\paragraph{Permute labels of features?}
While seemingly identical, the compounding of permutations with data foldings renders these two approaches distinct. 
As an example, consider balanced (stratified) K-fold cross validation where the initial data folding is balanced. 
After a label permutation, the original folds will probably not be balanced. 
If the \emph{features} are permuted, then the labels conserve their original fold assignments, and the original folds are balanced after each permutation. 
Since we only report results while refolding the data in each permutation, then the only difference between permuting labels and permuting features seems to be a computational one. 
We thus adhere to the more common, albeit less efficient practice, of permuting labels. 


\paragraph{Balanced folding?}
As already implied, a standard practice when cross validating is to constrain the data folds to be balanced (i.e. stratified).
This is well justified when aiming at unbiased accuracy estimation. 
This also simplifies matter when aiming at signal detection, as can be seen from the above discussion of the appropriate test statistic. 
On the other hand, it may complicate matters, as can be seen from the above discussion on label versus feature permutation. 
We will report results with both balanced and unbalanced data foldings, only to discover, it does not really matter. 


\paragraph{How many folds?}
Different authors suggest different rules for the number of folds. 
We will be varying the number of folds.
This will affect the concentration of permutation distribution of the estimated accuracy, which will have a crucial effect on the conservativeness of the accuracy test. 
Our intuition suggests that since more folds imply a less concentrated estimate, then leave-one-out should be the less conservative, and 2-fold should be the most conservative. 


There are indeed many design choices when performing a permutation test using a cross validated statistic. 
The subset of tests we will be comparing is collected for convenience in Table~\ref{tab:collected}.


\begin{table}[th]
\centering
\begin{tabular}{l|c|c|c|c}
Name & Basis & CV & Accuracy & Parameters\\ 
\hline
\hline
Hotelling & Hotelling & -- & -- & shrink=FALSE\\ 
Hotelling.shrink & Hotelling & -- & -- & shrink=TRUE \\ 
lda.CV.1 & LDA & TRUE & accuracy &  -- \\ 
lda.CV.2 & LDA & TRUE & z-accuracy & -- \\ 
lda.noCV.1 & LDA & FALSE & accuracy &  --\\ 
lda.noCV.2 & LDA & FALSE & z-accuracy &  --\\ 
sd & SD & -- & -- & -- \\ 
svm.CV.1 & SVM & TRUE & accuracy & cost=1e1 \\ 
svm.CV.2 & SVM & TRUE & accuracy & cost=1e-1 \\ 
svm.CV.3 & SVM & TRUE & z-accuracy & cost=1e1 \\ 
svm.CV.4 & SVM & TRUE & z-accuracy & cost=1e-1 \\ 
svm.noCV.1 & SVM & FALSE & accuracy & cost=1e1 \\ 
svm.noCV.2 & SVM & FALSE & accuracy & cost=1e-1 \\ 
svm.noCV.3 & SVM & FALSE & z-accuracy & cost=1e1 \\ 
svm.noCV.4 & SVM & FALSE & z-accuracy & cost=1e-1 \\
\end{tabular} 
\caption{\footnotesize
This table enumerates the various test statistics we will be studying. 
Three are location tests: Hotelling, Hotelling.shrink, and sd.
\textit{Hotelling} is the classical two-group $T^2$ statistic. 
\textit{Hotelling.shrink} is a high dimensional version with the regularized covariance in \cite{schafer_shrinkage_2005}. 
\textit{sd} is another high dimensional version of the $T^2$, from \cite{srivastava_two_2013}. 
The rest of the tests are variations of the linear SVM, and Fisher's LDA, with varying accuracy measures, cross validated or not, and varying tuning parameters. 
For example, \textit{svm.CV.4} is a linear SVM, with \textit{libsvm}'s cost parameter set at $0.1$, using the cross validated z-scored accuracy ($|\acc_{\hypEstim}-\dominant|/\sqrt{\dominant(1-\dominant)}$, see Section~\ref{sec:considerations}).
Another example is \textit{lda.noCV.1}, which is Fisher's LDA, returning the train accuracy, without cross validation, and without z-scoring. 
}
\label{tab:collected}
\end{table}







%%%% Section %%%%
\section{Controlling the False Positive Rate}
\label{sec:type_i}

We start by verifying that the battery of tests in Table~\ref{tab:collected} control the false positive rate at the desired $0.05$ level, with varying conservativeness levels. 
Figure~\ref{fig:simulation_1} demonstrates that this is indeed the case. 
All our candidate tests control the type I error, with varying degrees of conservativeness. 
In particular:
(a) if the folds are balanced or not,
(b) if the tuning parameters of some test statistic are varied,
(d) if the number of folds is varied.




\begin{figure}[h]
\centering
\caption{\footnotesize
The power of a permutation test with various test statistics. 
The power on the $x$ axis. 
Effect are color and shape coded. They are assumed to be equal in all the $23$ dimensions, and vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
The various statistics on the $y$ axis. Their details are given in Table~\ref{tab:collected}. 
Simulation code available at [TODO].}	
\label{fig:simulation_1}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=1\linewidth]{"art/2016-07-26 20:55:48"}
\caption{Unbalanced.}  %TODO
\label{fig:simulation_11}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
\caption{Balanced.} %TODO
\label{fig:simulation_12}
\end{subfigure}
\end{figure}




%%%% Section %%%%
\section{Power}
\label{sec:power}

Having established that all of the tests in our battery control the false positive rate, it remains to be seen if they have similar power; 
Especially,when comparing the power of the location tests to the accuracy tests. 
The theoretical results of \cite{ramdas_classification_2016} suggest that power should be of the same order. 
On the other hand, the results of our previous sections suggest that the conservativeness of some of the considered tests can be considerable, rendering them underpowered. 

[TODO: discuss power of various tests after finishing simulations]

We see by now that the use of accuracy tests for signal detection is underpowered compared to location tests. 
Simulations alone cannot, however, support such a universal statement. 
We will thus verify on a neuroimaging dataset, and discuss the causes for this phenomenon with implications on the scope of our statement.



\section{Neuroimaging Example}
\label{sec:example}
% Power of SVM versus SD.
% Difficulties of implementation.

Figure~\ref{fig:read_data} is an application of both a location and an accuracy test to the data of \cite{pernet_human_2015}. 
The authors of \cite{pernet_human_2015} collected fMRI data while subjects were exposed to the sounds of human speech (vocal), and other non-vocal sounds. 
Each subject was exposed to $20$ sounds of each type, totalling in $n=40$ trials in each scan.
The study was rather large and consisted of about $200$ subjects.
The data was kindly made available by the authors at the OpenfMRI website\footnote{\url{https://openfmri.org/}}.

We perform permutation inference using the pipeline of \cite{stelzer_statistical_2013}, which was also used in \cite{gilron_quantifying_2016}. 
For completeness, the pipeline is described in Appendix~\ref{apx:analysis}. 
To demonstrate our point, we compare the \emph{sd} location test with the \emph{svm.cv.1} accuracy test (see Table~\ref{tab:collected} for the definition of these statistics). 

In agreement with our simulation results, the location test (\emph{sd}) discovers more brain regions when compared to an accuracy test (\emph{svm.cv.1}).
The former discovers $1,232$ regions, while the latter only $441$, as reported in Figure~\ref{fig:read_data}.
We emphasize that both test statistics were compared with the same permutation scheme, and the same error controls, so that any difference in detections is due to their different power.

Having established that accuracy tests are underpowered both in simulation and in application, we wish to identify the conditions under which this will occur, and discuss implications on the practice of accuracy tests. 


\begin{figure}[th]
\centering
\includegraphics[width=0.7\linewidth]{"art/svm_vs_SD"}
\caption{\footnotesize
Brain regions encoding information discriminating between vocal and non-vocal stimuli.
Map reports the centres of $27$-voxel sized spherical regions, as discovered by an accuracy test (\emph{svm.cv.1}), and a location test (\emph{sd}). 
\emph{svm.cv.1} was computed using $5$-fold cross validation, and a cost parameter of $1$. 
Region-wise significance was determined using the permutation scheme of \cite{stelzer_statistical_2013}, followed by region-wise $FDR \leq 0.05$ control using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}.
Number of permutations equals $400$.
The location test detect $1,232$ regions, and the accuracy test $441$, $399$ of which are common to both.
For the details of the analysis see Appendix~\ref{apx:analysis} and \cite{gilron_quantifying_2016}.  
  }
\label{fig:read_data}
\end{figure}








%%%% Section %%%%
\section{Discussion}
\label{sec:discussion}
% Not all problems are signal detection.
% Implementation difficulty with discrete test statistics.
% Signal not in location.
% Heavy tails.


We have set out to understand which of the tests is more powerful: the accuracy test or the location test. 
Using simulations, we have concluded that the location tests are preferable. 
We attribute this to the discretization introduced in finite samples by the accuracy test statistic. 
This also explains why an asymptotic analysis, such as \cite{ramdas_classification_2016}, did not find a qualitative difference. [TODO: relate to large sample simulation].

At this point some reservations to the generality of our findings are in order. 
Firstly, not all accuracy tests are concerned with signal detection.
Indeed, it is possible that the purpose of the test is not to detect a difference between classes, but to actually test is a particular classifier is better than chance. 
This would be the case, for instance, with brain-machine interfaces, where the detection of a signal is not enough \citep{olivetti_induction_2012}. 
Another example is the search for an objective measure of pain \citep{wager_fmri-based_2013}, in which case the accuracy test was called upon not to localize pain regions, but rather to select those where pain can actually be decoded with a given classifier. 
In such cases, the performance of a particular classifier is the object of study, rendering the accuracy test the appropriate choice. 

Secondly, there may be cases where the accuracy test does have more power then the location test. 
Our simulations were unable to point out such a scenario, but the fact that in our neuroimaging example (Section~\ref{sec:example}) some brain regions were detected with the accuracy test, and not the location test, suggest that the accuracy test does have more power for particular types of signal. 
[TODO: signal in scale? heavy tails?]

A very important point is the ease of implementation. The need for cross validation of the accuracy test greatly increases its computational complexity. 
Moreover, anyone who has actually implemented tests with discrete statistics, will attest they are considerably harder to implement. This is because their unforgiveness to the type of inequality. 
Indeed, mistakenly replacing a weak inequality with a strong inequality in one's program may considerably change the results. 
This is not the case for continuous test statistics. 


Given all the above, we find the popularity of accuracy tests quite puzzling. 
We believe this is due to a reversal of the inference cascade. 
Researchers first fit a classifier, and then ask if the classes are any different.
Were they to start by asking if classes are any different, and only then try to classify, then location tests would naturally arise as the preferred method. 
As put by \cite{ramdas_classification_2016}:
\begin{quote}
The recent popularity of machine learning has resulted in the extensive teaching and use
of prediction in theoretical and applied communities and the relative lack of awareness or
popularity of the topic of Neyman-Pearson style hypothesis testing in the computer science
and related ``data science'' communities.
\end{quote}






%\begin{acknowledgments}
%TODO: 
% isf 900/60, Jelle, Jesse B.A. Hemerik, Yakir Brechenko, Omer Shamir
%\end{acknowledgments}












\bibliographystyle{abbrvnat}
\bibliography{Permuting_Accuracy.bib}

\appendix


\newpage

\section{Analysis pipeline}
\label{apx:analysis}

Here is the analysis pipeline of \cite{stelzer_statistical_2013} we for the auditory data in \cite{gilron_quantifying_2016}.
Denoting by 
$i=1,\dots,I$ the subject index, 
$v=1,\dots,V$ the voxel index, and 
$s = 1,\dots,S$ the permutation index. 
Since regions\footnote{\emph{searchlight} or \emph{sphere} in the MVPA parlance} are centred around a unique voxel, the voxel index $v$ also serves as a unique region index.
Algorithm~\ref{algo:statistic} computes a region-wise test statistic, which is compared to its permutation null distribution computed by Algorithm~\ref{algo:permutation}.


\begin{algorithm}[H]
\caption{Compute a group parametric map.}
\label{algo:statistic}

 \KwData{fMRI scans, and experimental design.}
 \KwResult{Brain map of group statistics: $\{\bar{T}_v\}_{v=1}^V$}
	 \For{$v \in 1,\dots,V$}{
		 \For{$i \in 1,\dots,I$}{
			 $T_{i,v} \leftarrow$ test statistic for subject $i$ in a region centered at $v$.
			 } 	  
	  	 $\bar{T}_{v} \leftarrow \frac{1}{I}\sum_{i=1}^I T_{i,v}$. 
 	 }
\end{algorithm}


\begin{algorithm}[H]
\caption{Compute a permutation p-value map.} 
\label{algo:permutation}

 \KwData{fMRI scans of $20$ subjects, experimental design.}
 \KwResult{Brain map of permutation p-values: $\{p_v\}_{v=1}^V$}
  \For{$s \in 1,\dots\,S$}{
    	    permute labels\;
    	    $\bar{T}_{v}^s \leftarrow$ parametric map 
  
  	}
\end{algorithm}

\newpage

\section{More Simulations}


\begin{figure}[h]
\centering
%\label{fig:simulation_1}
\caption{\footnotesize [TODO].}	
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{2 Folds.}  %TODO
	\label{fig:2016-07-2721:21:12}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-29 07:18:24"}
	  \caption{20 Folds.} %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}
\end{figure}


\begin{figure}[h]
\centering
%\label{fig:simulation_1}
\caption{\footnotesize [TODO].}	
	\begin{subfigure}{.5\textwidth}
	  \centering
  	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{Scale Change.}  %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
	  \caption{t Null} %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}
\end{figure}



\begin{figure}[h]
\centering
%\label{fig:simulation_1}
\caption{\footnotesize [TODO].}	
	\begin{subfigure}{.5\textwidth}
	  \centering
  	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{Compound symmetry}  
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
	  \caption{AR(1)} %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}
\end{figure}



\begin{figure}[h]
\centering
%\label{fig:simulation_1}
\caption{\footnotesize [TODO].}	
	\begin{subfigure}{.5\textwidth}
	  \centering
  	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{n=400}  %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
	  \caption{?} %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}
\end{figure}





\end{document}