\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{natbib}
\usepackage{url}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}


\author{Jonathan Rosenblatt \and Roee Gilron \and Roy Mukamel}


%% OPTIONAL MACRO DEFINITIONS
%\def\s{\sigma}
\newcommand{\set}[1]{\{ #1 \}} % A set
\newcommand{\indicator}[1]{I_{\set{#1}}} % The indicator function.
\newcommand{\reals}{\mathbb{R}} % the set of real numbers
\newcommand{\features}{x} % The feature space
\newcommand{\outcomes}{y} % The feature space
\newcommand{\featureS}{\mathcal{X}} % The feature space
\newcommand{\outcomeS}{\mathcal{Y}} % The feature space
\newcommand{\hyp}{f} % A hypothesis
\newcommand{\hypEstim}{\hat{\hyp}} % A hypothesis
\newcommand{\hypclass}{\mathcal{F}}
\newcommand{\expect}[1]{\mathbf{E}\left[ #1 \right]} % The expectation operator
\newcommand{\acc}{T^{acc}} 
\newcommand{\dominant}{\hat{p}_{max}}
\newcommand{\prob}[1]{P\left( #1 \right)} % the probability of an event






\title{Better than chance classification for signal detection}


\begin{document}
%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?


\maketitle

\begin{abstract}
[TODO]
\end{abstract}


%%%% Introduction %%%
\section{Introduction}
\label{sec:introduction}






A common workflow in genetics or neuroimaging consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Genetic examples include \cite{jiang_calculating_2008,radmacher_paradigm_2002-1} [TODO: elaborate].
Neuroscience examples include \emph{multivariate pattern analysis} (MVPA) \cite{kriegeskorte_information-based_2006,varoquaux_assessing_2016,golland_permutation_2003}.
In both fields this method has become quite popular. 
\cite{kriegeskorte_information-based_2006} has already gained $956$ citations\footnote{Based on GoogleScholar. Accesses on 26.7.2016.}, and \cite{radmacher_paradigm_2002-1} has been cited $274$ times.  


To fix ideas, we will adhere to a Neuroscientific example: 
In \cite{gilron_quantifying_2016}, the authors seek to detect auditory brain regions which distinguish between vocal and non-vocal stimuli. 
According to the MVPA analysis workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the spatial activation pattern, significantly better than chance, then a region is declared to encode vocal/non-vocal information. 
We call this an \emph{accuracy test}.

This same signal detection task can be also approached as a two-group multivariate test:
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
A practitioner may then call upon a two-group location test such as Hotelling's $T^2$ \cite{fujikoshi_multivariate_2011}.
Alternatively, if the size of the brain region is too large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of Hotelling's test can be called upon, such as in \cite{srivastava_testing_2013} or \cite{schafer_shrinkage_2005}.
In contrast to \emph{accuracy tests}, we call these \emph{location tests}.

At this point, it becomes unclear which is the preferred test. 
This was precisely the topic of \cite{ramdas_classification_2016}, who compared the Hotelling location test to the accuracy of \emph{Fisher's linear discriminant analysis} classifier (LDA) \citep{hastie_elements_2003-1}. 
Using an asymptotic analysis, \cite{ramdas_classification_2016} concluded that accuracy and location tests are equivalent with respect to their order of convergence to a consistent test, while they may differ in constants. 

Those constants, governing the power of the tests, are crucial when dealing with typical sample sizes in neuroscience and genetics, and thus the focus of this study. 
In particular, which test is to be preferred in finite samples? 
Our conclusion is quite simple: {\em location tests almost always have more power than accuracy tests}.

The main argument for our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Discrete test statistics are known to be conservative \citep{hemerik_exact_2014-1}, since they cannot exhaust the permissible false positive rate. 
In accuracy tests, the degree of discretization of the accuracy statistic is governed by the number of samples. 
In our running neuroscience example \citep{gilron_quantifying_2016}, the classification is performed based on $40$ trials, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of subject in a genetic study, or the number of trial repeats in an fMRI brain scan. 

The discretization effect is aggravated if the test statistic is highly concentrated. 
For an intuition consider the usage of the train-accuracy test statistic (i.e., not cross validated).
Because the testing problem is high dimensional, the observed train accuracy will be close to $1$. 
The same will occur in every permutation, for the same reason. 
The permutation p-value will thus be $1$ for almost all data sets, the null will never be rejected, and the test will have no power. 

Given these considerations, it is quite surprising that signal detection using accuracy tests is so popular in neuroscience and genetics. 
In the following, we quantify the power loss to be expected in typical studies, and identify the problems' characteristics that govern its severity. 
We start by establishing a best practice for permutation testing using the accuracy test statistic, 
We also discuss the problem characteristics that govern the magnitude of the conservativeness, and try to offer an intuition to the scope of the observation that a multivariate test should always be preferred over a classification approach. 






%%%% Section %%%%
\section{Problem setup}
\label{sec:problem_setup}

Adhering to our neuroscientific example, we now formalize terminology and notation. 
Let $\outcomes \in \outcomeS$ be a class encoding. In our vocal/non-vocal example, using effect coding, we have $\outcomeS=\set{-1,1}$.
Let $\features \in \featureS$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example $p$ is governed by the number of voxels in a regions, which is the number of voxels in each brain region tested. We thus have $\featureS=\reals^{27}$. 

Given $n$ pairs of $(\features_i,\outcomes_i)$, typically assumed i.i.d., the \emph{testing} approach to localization amounts to testing whether $\features|\outcomes=1$ has the the same distribution as $\features|\outcomes=-1$.
I.e., the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 
The \emph{classification} approach to the localization problem amounts to learning a predictive model $\hypEstim(\features)$ from some assumed model class $\hypEstim \in \hypclass$. 
The prediction accuracy, denoted $\acc_{\hypEstim}$, is defined as the probability of a given classifier $\hypEstim$ of making a correct prediction $\acc_{\hypEstim}:=\prob{\hypEstim(x)=y}$ when given a new, randomly drawn data point, ($\features,\outcomes$).



\subsection{Candidate Tests}
\label{sec:considerations}

The design of a permutation test using the prediction accuracy, requires the following design choices: 
\begin{description}
\item [What test statistic?] 
\item [Cross validated or not?] Is the statistic cross validated or not?
\item [Refolding?] For a K-fold cross validated test statistic: is the data refolded in each permutation? 
\item [Permute labels of features?] Should the $\outcomes$ be permuted or should the $\features$?
\item [Balanced folding?] For a K-fold cross validated test statistic: is the data folding balanced? (a.k.a. stratified).
\item [How many folds?] 

We will now address these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of the prediction error, but rather in the mere detection of a difference between two groups, leading to a better-than-chance accuracy. 


\paragraph{What test statistic?}
Given a predictor $\hypEstim$, a natural test statistic is some estimate of its accuracy $\acc_{\hypEstim}$.
Then again, very low accuracies, even $0$, is evidence that the classes are separated, and we only need to invert the predictions. We can thus consider some estimate of $|\acc_{\hypEstim}-0.5|$ as the test statistic.
This, however, implies that if the classes are identical, random guessing has a $0.5$ accuracy. This is not true if the classes are not balanced. 
The chance level in which case is the prevalence of the dominant class, we denote by $\dominant$.
This suggests the following test statistic $|\acc_{\hypEstim}-\dominant|$.
Since we will latter be aggregating these statistic over random data foldings, where the dominant class may have varying frequencies, it seems appropriate to standardize the scale of this statistic. 
We thus also consider a z-scored accuracy: $|\acc_{\hypEstim}-\dominant|/\sqrt{\dominant(1-\dominant)}$.


\paragraph{Cross validated or not?}
Were we interested in an unbiased estimator of the prediction error, there is no question that some validation is in order. 
Since we are merely interested in detecting a difference between groups, a biased error estimate is not an issue provided that it is consistent over all permutations. 
The underlying intuition is that if the exact same computation is performed over all permutations, then a permutation test will be ``fair'', i.e., will not inflate the false positive rate. 
We will thus be considering both cross validated accuracies, and train accuracies as our test statistics. 


\paragraph{Refolding?}
The standard practice in neuroimaging is to refold the data after each permutation.
This is imperative if permuting labels while aiming at balanced data folds. 
This is not, however, imperative in general. 
In this work, we will adhere to the standard practice of refolding the data within each permutation.


\paragraph{Permute labels of features?}
While seemingly identical, the compounding of permutations with data foldings renders these two approaches distinct. 
As an example, consider balanced (stratified) K-fold cross validation where the initial data folding is balanced. 
After a label permutation, the folds will probably not be balanced, and will thus have to be refolded. 
If the features are permuted, then the labels conserve their original fold assignments, and the data need not be refolded. 
Since we only report results while refolding the data in each permutation, then the only difference between permuting labels and permuting features seems to be a computational one. 
We thus adhere to the more common, albeit less efficient practice, of permuting labels. 


\paragraph{Balanced folding?}
A standard practice when cross validating is to constrain the data folds to be balanced (i.e. stratified).
This is well justified when aiming at unbiased accuracy estimation. 
This also simplifies matter when aiming at signal detection, as can be seen from the above discussion of the appropriate test statistic. 
Then again, it may complicate matters, as can be seen from the above discussion on label versus feature permutation. 
In general , it is not imperative in general, and we will indeed be comparing the effect of balanced foldings versus unbalanced. 
We will thus report results with both balanced and unbalanced data foldings. 


\paragraph{How many folds?}
Different authors suggest different rules for the number of folds. 
We will be varying the number of folds, since it will affect the concentration of the estimated accuracy, which will have a crucial effect on the conservativeness of the permutation test. 
Our intuition suggests that since more folds imply a less concentrated estimate, then leave-one-out should be the less conservative, and 2-fold should be the most conservative. 

\end{description}

By now, the reader will have observed that there are indeed many ways to perform a permutation test using a cross validated statistic. 
The subset of tests we will be comparing is collected for convenience in Table~\ref{tab:collected}.


\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c|c}
Name & Basis & CV & Accuracy & Parameters\\ 
\hline
\hline
Hotelling & Hotelling & -- & -- & shrink=FALSE\\ 
Hotelling.shrink & Hotelling & -- & -- & shrink=TRUE \\ 
lda.CV.1 & LDA & TRUE & accuracy &  -- \\ 
lda.CV.2 & LDA & TRUE & z-accuracy & -- \\ 
lda.noCV.1 & LDA & FALSE & accuracy &  --\\ 
lda.noCV.2 & LDA & FALSE & z-accuracy &  --\\ 
sd & SD & -- & -- & -- \\ 
svm.CV.1 & SVM & TRUE & accuracy & cost=1e1 \\ 
svm.CV.2 & SVM & TRUE & accuracy & cost=1e-1 \\ 
svm.CV.3 & SVM & TRUE & z-accuracy & cost=1e1 \\ 
svm.CV.4 & SVM & TRUE & z-accuracy & cost=1e-1 \\ 
svm.noCV.1 & SVM & FALSE & accuracy & cost=1e1 \\ 
svm.noCV.2 & SVM & FALSE & accuracy & cost=1e-1 \\ 
svm.noCV.3 & SVM & FALSE & z-accuracy & cost=1e1 \\ 
svm.noCV.4 & SVM & FALSE & z-accuracy & cost=1e-1 \\
\end{tabular} 
\caption{\footnotesize
This table enumerates the various test statistics we will be studying. 
Three are location tests: Hotelling, Hotelling.shrink, and sd.
\textit{Hotelling} is the classical two-group $T^2$ statistic. 
\textit{Hotelling.shrink} is a high dimensional version with the regularized covariance in \cite{schafer_shrinkage_2005}. 
\textit{sd} is another high dimensional version of the $T^2$, from \cite{srivastava_two_2013}. 
The rest of the tests are variations of the linear SVM, and Fisher's LDA, with varying accuracy measures, cross validated or not, and varying tuning parameters. 
For example, \textit{svm.CV.4} is a linear SVM, with \textit{libsvm}'s cost parameter set at $0.1$, using the cross validated z-scored accuracy ($|\acc_{\hypEstim}-\dominant|/\sqrt{\dominant(1-\dominant)}$, see Section~\ref{sec:considerations}).
Another example is \textit{lda.noCV.1}, which is Fisher's LDA, returning the train accuracy, without cross validation, and without z-scoring. 
}
\label{tab:collected}
\end{table}







%%%% Section %%%%
\section{Controlling the False Positive Rate}
\label{sec:type_i}

In the first of our battery of simulations we verify that various test statistics and permutation schemes control the type I error. 
Figure~\ref{fig:simulation_1} demonstrates that this is indeed the case. 
All our candidate tests control the type I error, with varying degrees of conservativeness. 
In particular:
(a) if the folds are balanced or not,
(b) if the labels are permuted or the features, 
(c) if the test statistic is varied, 
(d) if the regularization level of the support vector machine classifier (SVM) is varied,
(e) if the number of folds is varied.




\begin{figure}[h]
\centering
\label{fig:simulation_1}
\caption{\footnotesize
The power of a permutation test with various test statistics. 
The power on the $x$ axis. 
Effect are color and shape coded. They are assumed to be equal in all the $23$ dimensions, and vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
The various statistics on the $y$ axis. Their details are given in Table~\ref{tab:collected}. 
Simulation code available at [TODO].}	

	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-26 20:55:48"}
	  \caption{Unbalanced.}  %TODO
	\label{fig:simulation_11}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
	  \caption{Balanced.} %TODO
	\label{fig:simulation_12}
	\end{subfigure}
	
\end{figure}




%%%% Section %%%%
\section{Power}
\label{sec:power}

Having established that all of the tests in our battery control the false positive rate, it remains to be seen if they have similar power-- at least when comparing the power of the various classifiers and multivariate tests. 
The results of \cite{ramdas_classification_2016} suggest that power should be of the same order. 
On the other hand, the results of our previous sections suggest that the conservativeness of some of the considered tests can be considerable, rendering them underpowered. 

[TODO: discuss power of various tests]

We see by now that the use of accuracy tests for signal detection is underpowered compared to location tests. 
The above simulations can hardly support such a universal statement. 
We will thus verify on a neuroimaging dataset, and discuss the causes for this phenomenon thus the scope of the statement.



\section{Neuroimaging Example}
\label{sec:example}
% Power of SVM versus SD.
% Difficulties of implementation.

Figure~\ref{fig:read_data} is an application of our battery of tests to the data of \cite{pernet_human_2015}. 
The authors of \cite{pernet_human_2015} collected fMRI data while subjects were exposed to the sounds of human speech (vocal), and other non-vocal sounds. 
Each subject was exposed to $20$ sounds of each type, totalling in $n=40$ trials in each scan.
The study was rather large and consisted of about $200$ subjects.
The data was kindly made available by the authors at the OpenfMRI website\footnote{\url{https://openfmri.org/}}.

To verify the observation that location tests have more power than accuracy tests, we perform permutation inference using the pipeline of \cite{stelzer_statistical_2013}, which was also used in \cite{gilron_quantifying_2016}. 
For completeness, the pipeline is described in Appendix~\ref{apx:analysis}. 
To demonstrate our point, we compare the \emph{sd} location test with the \emph{svm.cv.1} accuracy test (see Table~\ref{tab:collected} for the definition of these statistics). 

In agreement with our simulation results, the location test (\emph{sd}) discovers more brain regions that encode information discriminating between vocal and non-vocal stimuli when compared to an accuracy test (\emph{svm.cv.1}).
The former discovers $1,232$ regions, while the latter only $441$, as reported in Figure~\ref{fig:read_data}.
We emphasize that both test statistics were compared with the same permutation scheme, and the same error controls, so that any difference in detections is due to their different power.

Having established that accuracy tests are underpowered both in simulation and in application, we wish to identify the conditions under which this will occur, and discuss implications on the practice of accuracy tests. 


\begin{figure}[th]
\centering
\includegraphics[width=0.7\linewidth]{"art/svm_vs_SD"}
\caption{\footnotesize
Brain regions encoding information discriminating between vocal and non-vocal stimuli.
Map reports the centres of $27$-voxel sized spherical regions, as discovered by an accuracy test (\emph{svm.cv.1}), and a location test (\emph{sd}). 
\emph{svm.cv.1} was computed using $5$-fold cross validation, and a cost parameter of $1$. 
Region-wise significance was determined using the permutation scheme of \cite{stelzer_statistical_2013}, followed by region-wise $FDR \leq 0.05$ control using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}.
Number of permutations equals $400$.
The location test detect $1,232$ regions, and the accuracy test $441$. The overlap is such that $90\%$ of the accuracy test regions, are also detected by the location test. 
For the details of the analysis see Appendix~\ref{apx:analysis} and \cite{gilron_quantifying_2016}.  
  }
\label{fig:read_data}
\end{figure}








%%%% Section %%%%
\section{Discussion}
\label{sec:discussion}
% Not all problems are signal detection.
% Implementation difficulty with discrete test statistics.
% Signal not in location.
% Heavy tails.


We have set out to understand which of the tests is more powerful: the accuracy test or the location test. 
Using simulations, we have concluded that the location tests are preferable. 
We attribute this to the discretization introduced in finite samples by the accuracy test statistic. 
This also explains why an asymptotic analysis, such as \cite{ramdas_classification_2016}, did not find a qualitative difference. 

At this point some reservations to the generality of our findings are in order. 
Firstly, not all accuracy tests are concerned with signal detection.
Indeed, it is possible that the purpose of the test is not to detect a difference between classes, but to actually test is a particular classifier is better than chance. 
This would be the case, for instance, with brain-machine interfaces, where the detection of a signal is not enough. In such cases, the performance of a particular classifier is the object of study, rendering the accuracy test the appropriate choice. 

Secondly, there may be cases where the accuracy test does have more power then the location test. 
Our simulations were unable to point out such a scenario, but the fact that in our neuroimaging example (Section~\ref{sec:example}) some brain regions were detected with the accuracy test, and not the location test, suggest that the accuracy test does have more power for particular types of signal. 
[TODO: signal in scale? heavy tails?]

A very important point is the ease of implementation. The need for cross validation of the accuracy test greatly increases its computational complexity. 
Moreover, anyone who has actually implemented tests with discrete statistics, will attest they are considerably harder to implement. This is because their unforgiveness to the type of inequality. Indeed, replacing a weak inequality with a strong inequality may considerably change the results. This is not the case for continuous test statistics. 


Given all the above, we find the popularity of accuracy tests quite puzzling. 
We believe this is due to a reversal of the inference cascade. 
Researchers first fit a classifier, and then ask if the classes are any different.
Were they to start by asking if classes are any different, and only then try to classify, then location tests would naturally arise as the preferred method. 






%\begin{acknowledgments}
%TODO: 
% isf 900/60, Jelle, Jesse B.A. Hemerik, Yakir Brechenko, Omer Shamir
%\end{acknowledgments}












\bibliographystyle{abbrvnat}
\bibliography{Permuting_Accuracy.bib}

\appendix


\newpage

\section{Analysis pipeline}
\label{apx:analysis}

Here is the analysis pipeline of \cite{stelzer_statistical_2013} we for the auditory data in \cite{gilron_quantifying_2016}.
Denoting by 
$i=1,\dots,I$ the subject index, 
$v=1,\dots,V$ the voxel index, and 
$s = 1,\dots,S$ the permutation index. 
Since regions\footnote{\emph{searchlight} or \emph{sphere} in the MVPA parlance} are centred around a unique voxel, the voxel index $v$ also serves as a unique region index.
Algorithm~\ref{algo:statistic} computes a region-wise test statistic, which is compared to its permutation null distribution computed by Algorithm~\ref{algo:permutation}.


\begin{algorithm}[H]
\caption{Compute a group parametric map.}
\label{algo:statistic}

 \KwData{fMRI scans, and experimental design.}
 \KwResult{Brain map of group statistics: $\{\bar{T}_v\}_{v=1}^V$}
	 \For{$v \in 1,\dots,V$}{
		 \For{$i \in 1,\dots,I$}{
			 $T_{i,v} \leftarrow$ test statistic for subject $i$ in a region centered at $v$.
			 } 	  
	  	 $\bar{T}_{v} \leftarrow \frac{1}{I}\sum_{i=1}^I T_{i,v}$. 
 	 }
\end{algorithm}


\begin{algorithm}[H]
\caption{Compute a permutation p-value map.} 
\label{algo:permutation}

 \KwData{fMRI scans of $20$ subjects, experimental design.}
 \KwResult{Brain map of permutation p-values: $\{p_v\}_{v=1}^V$}
  \For{$s \in 1,\dots\,S$}{
    	    permute labels\;
    	    $\bar{T}_{v}^s \leftarrow$ parametric map 
  
  	}
\end{algorithm}

\newpage

\section{More Simulations}


\begin{figure}[h]
\centering
%\missingfigure[figwidth=6cm]{TODO}
\label{fig:simulation_1}
\caption{\footnotesize [TODO].}	

	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{2 Folds.}  %TODO
	\label{fig:2016-07-2721:21:12}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \missingfigure[figwidth=6cm]{TODO}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
	  \caption{20 Folds.} %TODO
%	\label{fig:2016-07-2721:21:12}
	\end{subfigure}
	
\end{figure}







\end{document}