%% PNAStwoS.tex
%% Sample file to use for PNAS articles prepared in LaTeX
%% For two column PNAS articles
%% Version1: Apr 15, 2008
%% Version2: Oct 04, 2013

%% BASIC CLASS FILE
\documentclass{pnastwo}

%% ADDITIONAL OPTIONAL STYLE FILES Font specification

%\usepackage{pnastwoF}




%% OPTIONAL MACRO DEFINITIONS
%\def\s{\sigma}
\newcommand{\set}[1]{\{ #1 \}} % A set
\newcommand{\indicator}[1]{I_{\set{#1}}} % The indicator function.
\newcommand{\reals}{\mathbb{R}} % the set of real numbers
\newcommand{\features}{x} % The feature space
\newcommand{\outcomes}{y} % The feature space
\newcommand{\featureS}{\mathcal{X}} % The feature space
\newcommand{\outcomeS}{\mathcal{Y}} % The feature space



%%%%%%%%%%%%
%% For PNAS Only:
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}
%\setcounter{page}{2687} %Set page number here if desired
%%%%%%%%%%%%

\begin{document}

%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?



\title{On permutation testing of classification accuracy for signal detection}

\author{Jonathan Rosenblatt \affil{1}{Ben-Gurion University of the Negev, Beer-Sheva, Israel}}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

%%%Newly updated.
%%% If significance statement need, then can use the below command otherwise just delete it.
%\significancetext{RJSM and ACAC developed the concept of the study. RJSM conducted the analysis, data interpretation and drafted the manuscript. AGB contributed to the development of the statistical methods, data interpretation and drafting of the manuscript.}



\maketitle

\begin{article}

\begin{abstract}
[TODO]
\end{abstract}



\keywords{XXX | YYY | ZZZ }

%\abbreviations{TODO}

\section{Introduction}

A common workflow in genetics or neuroimaging consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Genetic examples include [TODO: elaborate]\cite{jiang_calculating_2008,radmacher_paradigm_2002-1}.
Neuroscience examples include the recently popularized \emph{multivariate pattern analysis} (MVPA) \cite{kriegeskorte_information-based_2006,varoquaux_assessing_2016,golland_permutation_2003}.

To fix ideas, we will adhere to a Neuroscientific example: 
In \cite{gilron_quantifying_2016}, the authors seek to detect auditory brain regions which distinguish between vocal and non-vocal stimuli. 
According to the MVPA analysis workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the spatial activation pattern, significantly better than chance, then a region is declared to encode vocal/non-vocal information. 

This same signal detection task can be also approached as a two-group multivariate test:
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
A practitioner may then call upon a two-group Hotelling test \cite{fujikoshi_multivariate_2011}. 
If the size of the brain region is too large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of the Hotelling test can be called upon, such as  \cite{srivastava_testing_2013}.

At this point, it becomes unclear which is the preferred avenue. 
This question was indeed the subject of \cite{ramdas_classification_2016}, which compared the Hotelling test to the classification accuracy of \emph{Fisher's linear discriminant analysis} (LDA) \cite{hastie_elements_2003-1}. 
Their asymptotic analysis concluded that methods are equivalent with respect to the order of convergence to a consistent test. The same authors did find that the methods differ in the constants governing the convergence.

Those constants, are the focus of this contribution. In particular, which signal detection method is to be preferred in finite samples? 
Our conclusion is quite simple: signal detection using multivariate tests almost always has more power than using the accuracy test statistic.

The main argument for our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Discrete test statistics are known to be conservative, since they cannot exhaust the permissible type $I$ error rates. 
For the accuracy test statistic, the degree of discretization is governed by the number of samples. 
in our running example \cite{gilron_quantifying_2016}, the classification is performed based on $40$ trials, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of subject in a genetic study, or the number of trial repeats in an fMRI brain scan. 

The discretization effect is further augmented if the test statistic is highly concentrated. 
For some intuition consider the usage of the train-accuracy test statistic (i.e., not cross validated).
Because the testing problem is high dimensional, the observed train accuracy will be high. But the same will occur in every permutation. 
The permutation accuracies will tend to concentrate around $1$ with a granularity depending on the sample size. 
In a very high dimensional setup, the accuracy will be $1$ for the observed data, but also for any permuted data set. 
The permutation p-value will thus be $1$ for almost all samples, and the null will never be rejected. 

In the following, we look into the conservativeness of signal detection using an the classification accuracy. 
We start by establishing a best practice for permutation testing using the accuracy test statistic, 

We discuss the problem characteristics that govern the magnitude of the conservativeness, and try to offer an intuition to the scope of the observation that a multivariate test should always be preferred over a classification approach. 


\section{Problem setup}
Adhering to our neuroscientific example, we now formalize terminology and notation. 
Let $\outcomes \in \outcomeS$ be a class encoding. In our vocal/non-vocal example, using effect coding, we have $\outcomeS=\set{-1,1}$.
Let $\features \in \featureS$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example $p$ is governed by the number of voxels in a regions, which is the number of voxels in each brain region tested. We thus have $\featureS=\reals^{27}$. 

Given $n$ pairs of $(\features_i,\outcomes_i)$, typically assumed i.i.d., the \emph{testing} approach to localization amounts to testing whether $\features|\outcomes=1$ has the the same distribution as $\features|\outcomes=-1$.
I.e., the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 
The \emph{classification} approach to the localization problem 


\section{How to perform the permutation test?}

To discuss the power of a permutation test with the accuracy, we need to establish some best practices for such a test. 
The design of the test consists of the following choices: 
\begin{description}
\item [What test statistic?] 
\item [Cross validated or not?] Is the statistic cross validated or not?
\item [Refolding?] For a K-fold cross validated test statistic: is the data refolded in each permutation? 
\item [Balanced folding?] For a K-fold cross validated test statistic: is the data folding balanced? (a.k.a. stratified).
\item [How many folds?] 

We will now adress these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of the prediction error, but rather in the mere detection of a difference between two groups, leading to a better-than-chance accuracy. 

\paragraph{What test statistic}



\end{description}



\section{Power}
fff





\section{Discussion}
% Not all problems are signal detection.
% 
fff

%\begin{acknowledgments}
%TODO
%\end{acknowledgments}













\bibliography{Permuting_Accuracy.bib}
\bibliographystyle{PNAS.bst}
%\bibliographystyle{abbvrnat}

\end{article}





\end{document}


