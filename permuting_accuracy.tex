\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{natbib}
\usepackage{url}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lineno}
\usepackage{tcolorbox}
\usepackage{caption}
\usepackage{hyperref}
\AtBeginDocument{\let\textlabel\label}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black,
}


\captionsetup[figure]{labelfont=it,textfont={bf,it}}
\captionsetup[subfigure]{width=0.8\hsize,labelfont=bf,textfont=footnotesize,singlelinecheck=off,justification=raggedright,format=hang}


\author{Jonathan Rosenblatt \and Roee Gilron \and Roy Mukamel}


%% OPTIONAL MACRO DEFINITIONS
%\def\s{\sigma}
\newcommand{\set}[1]{\{ #1 \}} % A set
\newcommand{\indicator}[1]{I_{\set{#1}}} % The indicator function.
\newcommand{\reals}{\mathbb{R}} % the set of real numbers
\newcommand{\features}{x} % The feature space
\newcommand{\outcomes}{y} % The feature space
\newcommand{\featureS}{\mathcal{X}} % The feature space
\newcommand{\outcomeS}{\mathcal{Y}} % The feature space
\newcommand{\hyp}{f} % A hypothesis
\newcommand{\hypEstim}{\hat{\hyp}} % A hypothesis
\newcommand{\hypclass}{\mathcal{F}}
\newcommand{\expect}[1]{\mathbf{E}\left[ #1 \right]} % The expectation operator
\newcommand{\acc}{T^{acc}} 
\newcommand{\dominant}{\hat{p}_{max}}
\newcommand{\minority}{\hat{p}_{min}}
\newcommand{\prob}[1]{Prob( #1 )} % the probability of an event
\newcommand{\rv}[1]{\mathbf{#1}} % A random variable
\newcommand{\x}{\rv x} % The random variable x 
\newcommand{\y}{\rv y} % The random variable x 
\newcommand{\X}{\rv X} % The random variable x 
\newcommand{\Y}{\rv Y} % The random variable y
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)} % The gaussian distribution
\newcommand{\gaussp}[2]{\mathcal{N}_{#1}\left(#2\right)} % The gaussian distribution
\newcommand{\mycaption}{Simulation details in Appendix~\ref{apx:simulation_details} except the changes in the sub-captions.}
\newcommand{\argmin}[2]{\mathop{argmin} _{#1}\set{#2}} % The argmin operator
\newcommand{\argmax}[2]{\mathop{argmax} _{#1}\set{#2}} % The argmin operator



\title{Better-Than-Chance Classification for Signal Detection}


\begin{document}
%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?


\maketitle
\linenumbers

\begin{abstract}
[TODO]
\end{abstract}


%%%% Introduction %%%
\section{Introduction}
\label{sec:introduction}

A common workflow in neuroimaging consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Examples in the neuroscientific literature include \citet{golland_permutation_2003,pereira_machine_2009,varoquaux_assessing_2016}, and especially the recently popularized \emph{multivariate pattern analysis} (MVPA) framework of \citet{kriegeskorte_information-based_2006}.
This practice is also observed in very high profile publications in the genetics literature: 
\citet{golub_molecular_1999,slonim_class_2000,radmacher_paradigm_2002,mukherjee_estimating_2003,juan_prediction_2004,jiang_calculating_2008}.


To fix ideas, we will adhere to a concrete example.
In \cite{gilron_quantifying_2016}, the authors seek to detect brain regions which encode differences between vocal and non-vocal stimuli. 
Following the MVPA workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the spatial activation pattern significantly better than chance, then a region is declared to encode vocal/non-vocal information. 
We call this an \emph{accuracy test}, a.k.a.\ \emph{class prediction}, or \emph{pattern discrimination}.

This same signal detection task can be also approached as a two-group multivariate test.
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
As put in \cite{pereira_machine_2009}: 
\begin{quote}
... the problem of deciding whether the classifier learned to discriminate the classes can be subsumed into the more general question as to whether there is evidence that the underlying distributions of each class are equal or not.
\end{quote}
A practitioner may then call upon a two-group location test such as Hotelling's $T^2$ \citep{anderson_introduction_2003}.
Alternatively, if the size of a brain region is too large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of Hotelling's test can be called upon, such as in \cite{schafer_shrinkage_2005} or \cite{srivastava_testing_2013}.
For brevity, and in contrast to \emph{accuracy tests}, we will call any two-sample multivariate tests simply \emph{location tests}, also termed \emph{class comparisons}. 


At this point, it becomes unclear which is preferable: a location test or an accuracy test?
The former with a heritage dating back to \cite{hotelling_generalization_1931}, and the latter being extremely popular, as the $959$ citations\footnote{GoogleScholar. Accessed on Aug 4, 2016.} of \cite{kriegeskorte_information-based_2006} suggest. 

The comparison between location and accuracy tests was precisely the goal of \cite{ramdas_classification_2016}, who compared the $T^2$ location test to the accuracy of \emph{Fisher's linear discriminant analysis} classifier (LDA). 
By comparing the rates of convergence of the powers to $1$, \cite{ramdas_classification_2016} concluded that accuracy and location tests are rate equivalent. 

Asymptotic relative efficiency measures (ARE) are typically used by statisticians to compare between test statistics with similar rates \citep{vaart_asymptotic_1998}.
\cite{ramdas_classification_2016} derive the asymptotic power functions of the two test statistics, which allow to extract the ARE between Hotelling's $T^2$ (location) test and Fisher's LDA (accuracy) test.
Using the Theorem~14.7 in \cite{vaart_asymptotic_1998}, we deduce that the ARE is lower bounded by $2 \pi \approx 6.3$. 
This means that Fisher's LDA requires at least $6.3$ more samples to achieve the same (asymptotic) power than the $T^2$ test. 
In this light, the accuracy test is remarkably inefficient compared to the location test.  
For comparison, the t-test is only $1.04$ more (asymptotically) efficient than Wilcoxon's rank-sum test \citep{lehmann_parametric_2009}, so that an ARE of $2.5$ is strong evidence in favor of the location test. 

Before discarding accuracy tests as innefficient, we recall that \cite{ramdas_classification_2016} analyzed a \emph{half-sample} holdout. 
The authors conjectured that a leave-one-out approach, which makes more efficient use of the data, may have better performance. 
Also, the analysis in \cite{ramdas_classification_2016} is asymptotic. 
This eschews the discrete nature of the accuracy statistic, which will be shown to have  crucial impact. 
Since typical sample sizes in neuroscience are not large, we seek to study which test is to be preferred in finite samples? 
Our conclusion will be quite simple: {\em location tests almost always have more power than accuracy tests}.

The main argument for our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Discrete test statistics are known to be conservative \citep{hemerik_exact_2014}, since they are insensitive to mild perturbations of the data, and they cannot exhaust the permissible false positive rate. 
The degree of discretization is governed by the number of samples. 
In our neuroscience example from \citet{gilron_quantifying_2016}, the classification is performed based on $40$ trials, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of subjects, or the number of trial-repeats in an neuroimaging study. 

The discretization effect is aggravated if the test statistic is highly concentrated. 
For an intuition consider the usage of a the \emph{resubstitution accuracy} as a test statistic. 
This statistic simply means that the accuracy is not cross validated. 
If the data is high dimensional, the resubstitution accuracy will be very high due to over fitting. 
In a very high dimensional model, the resubstitution accuracy will be $1$ for the observed data \cite[Thorem 1]{mclachlan_bias_1976}, but also for any permutation.
The concentration of resubstitution accuracy near $1$, and its discreteness, render this test completely useless, with a power tending to $0$ as the dimension of the model grows. 


To compare the power of accuracy tests and location tests in finite samples, we perform a simulation study of a battery of test statistics. 
The main findings are reported in Sections~\ref{sec:power} and \ref{sec:example}, and the intuition for our findings is provided in Section~\ref{sec:discussion}, but first, the problem's setup. 





%%%% Section %%%%
\section{Problem setup}
\label{sec:problem_setup}

Let $\outcomes \in \outcomeS$ be a class encoding. 
Let $\features \in \featureS$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example we have $\outcomeS=\set{-1,1}$ and $p$, the number of voxels in a brain region so that $\featureS=\reals^{27}$. 

Given $n$ pairs of $(\features_i,\outcomes_i)$, typically assumed i.i.d., a location test amounts to testing whether $\features|\outcomes=1$ has the the same distribution as $\features|\outcomes=-1$. 
I.e., we test if the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 
An accuracy test amounts to learning a predictive model $\hypEstim(\features)$ from some assumed model class $\hypEstim \in \hypclass$. 
The prediction accuracy, denoted $\acc_{\hypEstim}$, is defined as the probability of a given classifier $\hypEstim$ of making a correct prediction $\acc_{\hypEstim}:=\prob{\hypEstim(x)=y}$ when given a randomly drawn data point, ($\features,\outcomes$).
A statistically significant ``better than chance'' estimate of $\acc_{\hypEstim}$ is evidence that the classes are distinct. 


\subsection{Candidate Tests}
\label{sec:considerations}

The design of a permutation test using the prediction accuracy, requires the following design choices: 
\begin{enumerate}
\item How to estimate accuracy?
\item Is the statistic cross validated or not?
\item For a K-fold cross validated test statistic: should the data be refolded in each permutation? 
\item Permute labels of features?
\item For a K-fold cross validated test statistic: should the data folding balanced (a.k.a.\ stratified)?
\item How many folds? 
\end{enumerate}

We will now address these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of the prediction error, but rather in the mere detection of a difference between two groups. 

\paragraph{How to estimate accuracy?}
\label{sec:estimate_accuracy}
Given a predictor $\hypEstim$, a natural test statistic is some estimate of its accuracy $\acc_{\hypEstim}$.
Complicating matters: very low accuracies, even $0$, is evidence that the classes are separated, and we only need to invert the predictions. 
We can thus consider $|\acc_{\hypEstim}-0.5|$ as the test statistic.
This, however, implies that if the classes are identical, random guessing has $0.5$ accuracy. This is not true if the classes are not balanced. 
For unbalanced data the chance level is the probability of the minority class, we denote by $\minority$ \citep[ Sec 4.1]{golland_permutation_2005}.
This suggests the following test statistic $|\acc_{\hypEstim}-\minority|$.
Since we will be aggregating these statistics over random data sets where $\minority$ may vary, it seems appropriate to standardize the scale of this statistic. 
We thus also consider the z-scored accuracy: $|\acc_{\hypEstim}-\minority|/\sqrt{\minority(1-\minority)}$. 



\paragraph{Cross validate or not?}
Were we interested in an unbiased estimator of the prediction error, there is no question that some independent validation is in order. 
Since we are merely interested in detecting a difference between classes, a biased error estimate is not an issue provided that bias is consistent over all permutations. 
The underlying intuition is that if the exact same computation is performed over all permutations, then a permutation test will be ``fair'', i.e., will not inflate the false positive rate. 
We will thus be considering both cross validated accuracies, and resubstitution accuracies as our test statistics, a.k.a.\ \emph{resubstitution classification}. 


\paragraph{Refolding?}
The standard practice in neuroimaging is to refold the data after each permutation \citep{pereira_machine_2009}.
This is imperative if permuting labels while aiming at balanced data folds. 
This is not, however, imperative in general. 
For simplicity, we will adhere to the standard practice of refolding the data within each permutation.


\paragraph{Permute labels of features?}
While seemingly identical, the compounding of permutations with data foldings renders these two approaches distinct. 
As an example, consider balanced (stratified) K-fold cross validation where the initial data folding is balanced. 
After a label permutation, the original folds will probably not be balanced. 
If the \emph{features} are permuted, then the labels conserve their original fold assignments, and the original folds are balanced after each permutation. 
Since we only report results while refolding the data in each permutation, then the only difference between permuting labels and permuting features seems to be a computational one. 
We thus adhere to the more common, albeit computationally less efficient practice of permuting labels. 


\paragraph{Balanced folding?}
As already implied, a standard practice when cross validating is to constrain the data folds to be balanced (i.e. stratified).
This is well justified when aiming at unbiased accuracy estimation. 
This also simplifies matter when aiming at signal detection, as can be seen from the above discussion of the appropriate test statistic. 
On the other hand, it may complicate matters, as can be seen from the above discussion on label versus feature permutation. 
We will report results with both balanced and unbalanced data foldings, only to discover, it does not really matter. 


\paragraph{How many folds?}
Different authors suggest different rules for the number of folds. 
We will be varying the number of folds.
This will affect the concentration of permutation distribution of the estimated accuracy, which will have a crucial effect on the conservativeness of the accuracy test. 
Our intuition suggests that since more folds imply a less concentrated estimate, then leave-one-out should be the less conservative, and 2-fold should be the most conservative. 


The of tests we will be comparing is collected for convenience in Table~\ref{tab:collected}.


\begin{tcolorbox}
\centering
\begin{tabular}{l|c|c|c|c}
Name & Basis & CV & Accuracy & Parameters\\ 
\hline
\hline
Hotelling & Hotelling & -- & -- & shrink=FALSE\\ 
Hotelling.shrink & Hotelling & -- & -- & shrink=TRUE \\ 
lda.CV.1 & LDA & TRUE & accuracy &  -- \\ 
lda.CV.2 & LDA & TRUE & z-accuracy & -- \\ 
lda.noCV.1 & LDA & FALSE & accuracy &  --\\ 
lda.noCV.2 & LDA & FALSE & z-accuracy &  --\\ 
sd & SD & -- & -- & -- \\ 
svm.CV.1 & SVM & TRUE & accuracy & cost=1e1 \\ 
svm.CV.2 & SVM & TRUE & accuracy & cost=1e-1 \\ 
svm.CV.3 & SVM & TRUE & z-accuracy & cost=1e1 \\ 
svm.CV.4 & SVM & TRUE & z-accuracy & cost=1e-1 \\ 
svm.noCV.1 & SVM & FALSE & accuracy & cost=1e1 \\ 
svm.noCV.2 & SVM & FALSE & accuracy & cost=1e-1 \\ 
svm.noCV.3 & SVM & FALSE & z-accuracy & cost=1e1 \\ 
svm.noCV.4 & SVM & FALSE & z-accuracy & cost=1e-1 \\
\end{tabular} 
\captionsetup{type=table}
\caption{\footnotesize
This table enumerates the various test statistics we will be studying. 
Three are location tests: Hotelling, Hotelling.shrink, and sd.
\textit{Hotelling} is the classical two-group $T^2$ statistic. 
\textit{Hotelling.shrink} is a high dimensional version with the regularized covariance in \cite{schafer_shrinkage_2005}. 
\textit{sd} is another high dimensional version of the $T^2$, from \cite{srivastava_two_2013}. 
The rest of the tests are variations of the linear SVM, and Fisher's LDA, with varying accuracy measures, cross validated or not, and varying tuning parameters. 
For example, \textit{svm.CV.4} is a linear SVM, with \textit{libsvm}'s cost parameter set at $0.1$, using the cross validated z-scored accuracy ($|\acc_{\hypEstim}-\dominant|/\sqrt{\dominant(1-\dominant)}$, see Section~\ref{sec:considerations}).
Another example is \textit{lda.noCV.1}, which is Fisher's LDA, returning the resubstitution accuracy, without cross validation, and without z-scoring.}
\label{tab:collected}
\end{tcolorbox}







%%%% Section %%%%
\section{Controlling the False Positive Rate}
\label{sec:type_i}

Figure~\ref{fig:simulation_1} demonstrates that all of the tests considered conserve the desired $0.05$ false positive rate, up to varying levels of conservativism.
This can be seen from the fact that the probability of rejection is no larger than $0.05$ in the abscense of any effect, encoded by a red circle. 
This is true, in particular if:
(a) the folds are balanced or not,
(b) the tuning parameters of some test statistic are varied,
(d) the number of folds is varied.
We also observe that the most conservative tests are the resubstitution accuracy measures. 
We return to this matter in the Discussion.




\begin{figure}[h]
\centering
\caption{\footnotesize
The power of a permutation test with various test statistics. 
The power on the $x$ axis. 
Effect are color and shape coded. 
The various statistics on the $y$ axis. 
Their details are given in Table~\ref{tab:collected}. 
Effects vary over $0$ (red circle), $0.25$ (green triangle), and $0.5$ (blue square). 
Simulation details in Appendix~\ref{apx:simulation_details}.
Cross-validation was performed with balanced (stratified) and unbalanced data folding. See sub-captions.}	
\label{fig:simulation_1}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=1\linewidth]{"art/2016-07-26 20:55:48"}
\caption{\textbf{Unbalanced.}}  
\label{fig:simulation_11}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
\caption{\textbf{Balanced.}} 
\label{fig:simulation_12}
\end{subfigure}
\end{figure}




%%%% Section %%%%
\section{Power}
\label{sec:power}

Having established that all of the tests in our battery control the false positive rate, it remains to be seen if they have similar power-- 
especially when comparing the power of location tests to accuracy tests. 
From the simulation results reported in Appendix~\ref{apx:simulations} we collect the following insights:
\begin{enumerate}
\item Location tests have more power than accuracy tests in all our configurations.
\item The conservativeness decays as the sample grows (Figures~\ref{fig:large_sample_1}, \ref{fig:large_sample_2} and \ref{fig:large_sample_3}), suggesting that either concentration or discretization is responsible for power loss. 
\item The power may increase or decrease with the number of folds (Figure~\ref{fig:n_folds}). %TODO: effect of n.folds.
\item The z-scoring of the accuracies was introduced to deal with unbalanced foldings. 
If the z-scoring has any effect at all, it merely kills power.
There is really no reason to use it. 
\item Both accuracy and location tests are innapropriate for scale alternatives (Figure~\ref{fig:scale_change}). 
This was to be expeted and is reported mostly as a sanity check.
\item The presence of heavy tails (Figure~\ref{fig:t_null}) may reduce power, but does not quantitatively change results. 
\item Balanced folding typically has no effect. 
It increased power only for the z-scored statistics (Figure~\ref{fig:simulation_1}). This is surprising given they were precisely designed to deal with the presence of imbalance. 
\item Varying the accuracy test's tunning parameter, such as the cost (i.e. margins) has no effect on the power of the test. 
\item Correlation between coordinates, mimiquing temporal correlation in fMRI data, has no effect on conclusions, since all test statistics account for this corrlation (Figure~\ref{fig:ar_1}). %TODO: verify
\end{enumerate}


The major insight from simulations is that the use of accuracy tests for signal detection is underpowered compared to location tests. 
We now verify this finding on a neuroimaging dataset.



\section{Neuroimaging Example}
\label{sec:example}
% Power of SVM versus SD.
% Difficulties of implementation.

Figure~\ref{fig:read_data} is an application of both a location and an accuracy test to the data of \cite{pernet_human_2015}. 
The authors of \cite{pernet_human_2015} collected fMRI data while subjects were exposed to the sounds of human speech (vocal), and other non-vocal sounds. 
Each subject was exposed to $20$ sounds of each type, totaling in $n=40$ trials in each scan.
The study was rather large and consisted of about $200$ subjects.
The data was kindly made available by the authors at the OpenfMRI website\footnote{\url{https://openfmri.org/}}.

We perform group inference using within-subject permutations using the pipeline of \cite{stelzer_statistical_2013}, which was also reported in \cite{gilron_quantifying_2016}. 
For completeness, the pipeline is described in Appendix~\ref{apx:analysis}. 
To demonstrate our point, we compare the \emph{sd} location test with the \emph{svm.cv.1} accuracy test (see Table~\ref{tab:collected} for the definition of these statistics). 

In agreement with our simulation results, the location test (\emph{sd}) discovers more brain regions when compared to an accuracy test (\emph{svm.cv.1}).
The former discovers $1,232$ regions, while the latter only $441$, as depicted in Figure~\ref{fig:read_data}.
We emphasize that both test statistics were compared with the same permutation scheme, and the same error controls, so that any difference in detections is due to their different power.

Having established that accuracy tests are underpowered both in simulation and in application, we wish to identify the conditions under which this will occur, and discuss implications on the practice of accuracy tests. 


\begin{figure}[th]
\centering
\includegraphics[width=0.7\linewidth]{"art/svm_vs_SD"}
\caption{\footnotesize
Brain regions encoding information discriminating between vocal and non-vocal stimuli.
Map reports the centers of $27$-voxel sized spherical regions, as discovered by an accuracy test (\emph{svm.cv.1}), and a location test (\emph{sd}). 
\emph{svm.cv.1} was computed using $5$-fold cross validation, and a cost parameter of $1$. 
Region-wise significance was determined using the permutation scheme of \cite{stelzer_statistical_2013}, followed by region-wise $FDR \leq 0.05$ control using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}.
Number of permutations equals $400$.
The location test detect $1,232$ regions, and the accuracy test $441$, $399$ of which are common to both.
For the details of the analysis see Appendix~\ref{apx:analysis} and \cite{gilron_quantifying_2016}.  
  }
\label{fig:read_data}
\end{figure}








%%%% Section %%%%
\section{Discussion}
\label{sec:discussion}

We have set out to understand which of the tests is more powerful: the accuracy test or the location test. 
Using simulations, we have concluded that the location tests are preferable. 
Their high dimensional versions such as \cite{srivastava_testing_2013} and \cite{schafer_shrinkage_2005} are preferable for typical neuroimaging problems such as MVPA.
We attribute this to several phenomena: 
(a) Discretization introduced in finite samples by the accuracy test statistic. 
(b) Inefficient use of the data for the validation holdout set. 
In our high dimensional setup, we also confirmed that high-dimensional versions of the $T^2$ test, such as \cite{srivastava_testing_2013} or \cite{schafer_shrinkage_2005} are preferable over the original $T^2$. 


The sensitivity of the power to the number of folds suggests that most of the power is lost due to the discretization and not to the holdout. 
The degree of discretization is governed by the sample size. 
For this reason, an asymptotic analysis such as \cite{ramdas_classification_2016} may uncover the holdout inefficiency, but will not uncover the discretization effect. 
The practical advice for the practitioner, is that for the purpose of signal detection, there is typically a multivariate test (be it a location test or other), that is more powerful than an accuracy test. 
There is also a good chance that it would be easier to implement, since no validation will be involved. 



\subsection{Ease of implementation}
A very important point is the ease of implementation. 
The need for cross validation of the accuracy test greatly increases its computational complexity. 
Moreover, anyone who has actually implemented tests with discrete statistics, will attest they are considerably harder to implement. This is because their unforgiveness to the type of inequality. 
Indeed, mistakenly replacing a weak inequality with a strong inequality in one's program may considerably change the results. 
This is not the case for continuous test statistics. 



\subsection{A good accuracy test}
In Section~\ref{sec:reservations} we discuss cases where an accuracy test cannot replace a location test.
For such cases we collect some conclusions from our simulations on the best practices for accuracy tests.
\begin{enumerate}
\item The conservativeness due to discretization decreases with sample size. 
\item Cross validating the accuracy statistic increases power in moderate sample sizes. 
The power loss due to the holdout inefficiency is smaller than the power loss due to the concentration of the resubstitution accuracy. 
For large sample sizes, discretization and concentration have weaker effects, and the cross validated accuracy may be replaced with the computationally more efficiency resubstitution accuracy.
\item Permuting features is easier than permuting labels. 
It allows to preserve balanced folds after a permutation without refolding, thus reducing computational complexity.
\item There is no gain in z-scoring the accuracy scores. 
\item Cross validated accuracy with balanced folds has more power than unbalanced folds. We currently have no intuition to offer for this phenomenon. %TODO:why balalance?
\item It is unclear what is the effect of the number of folds. More folds increase power by reducing the number of holdout samples. 
On the other hand, it increases the concentration of the accuracy statistic. Compounded with the discreteness of the accuracy statistic, this decreases power. 
\item The value of the tunning parameters of a classifier have little to no effect. 
\end{enumerate}

\subsection{Smoothing accuracy estimates}
It may be possible to alleviate the effect of discretizatin by appropriate cross-validation. 
The discretness of the accuracy statisitc can be ``smoothed'' by allowing the test sample to be drawn with replacement. 
The \emph{bootstrap} may seem like a candidate approach, but since the original data always serves as a test set, the accuracy can still only assume $1/n$ values. 
This is not the case, however, for the \emph{leave-one-out bootstrap estimator} \cite[Sec 7.11]{hastie_elements_2003}. 
It is a simplified version of the \emph{0.632 bootstrap estimator} \citep{efron_improvements_1997}, and suffices for our purpose since we are not interested in unbiased risk estimation, but merely signal detection.
By the same rational, the degree of conservativism should decrease with the number of bootstrap samples. 
The naming conventions of the bootstrapped estimates are detailed in Table~\ref{tab:collected_2}.
As seen in Figure~\ref{fig:bootstrap}...  %TODO: bootstrap.

\bigskip

\begin{tcolorbox}
\centering
\begin{tabular}{l|c|c|c|c|c}
Name & Basis & Boot Type & B & Accuracy & Parameters\\ 
\hline
\hline
lda.Boot.1 & LDA & $0.632$ & 10 & accuracy &  -- \\ 
lda.Boot.2 & LDA & LOOB & 10 & accuracy &  -- \\ 
svm.Boot.1 & SVM & $0.632$ & 10 & accuracy & cost=1e1 \\ 
svm.Boot.2 & SVM & LOOB & 10 & accuracy & cost=1e-1 \\ 
svm.Boot.3 & SVM & $0.632$ & 10 & accuracy & cost=1e1 \\ 
svm.Boot.4 & SVM & LOOB & 10 & accuracy & cost=1e-1 \\ 
\end{tabular} 
\captionsetup{type=table}
\caption{The same as Table~\ref{tab:collected} for bootstraped accuracy estimates. }
\label{tab:collected_2}
\end{tcolorbox}


\begin{figure}[ht]
\centering
	  \includegraphics[width=0.8\linewidth]{"art/2016-08-09 20:09:20"}
	  \caption{\textbf{Bootstrap:} } %TODO:caption
	\label{fig:bootstrap}
\end{figure}


\subsection{High dimensional classifiers}
It is known that when $p>n$ Hotelling's $T^2$, and Fisher's LDA are not computable. 
In our simlations, in which $p=23$ and $n=40$ is ``almost'' high dimensional, but still allows to compute both tests.
We have simulated two high dimensional versions of Hotelling's $T^2$: \emph{sd} \citep{srivastava_testing_2013} and \emph{Hotelling.shrink} \citep{schafer_shrinkage_2005}. 
The former solves the dimensionalty problem by assuming independence over coordinates, and the latter by Tikhonov regularization of the covariance, a-la ridge regression. 
The corresponding high dimensional accuracy tests would be a \emph{naive Bayes} classfier, and $l_2$ regulirized SVM \citep{ramdas_classification_2016}.
We conjecture that they would not alternour conclusions, since the main force driving the conservatrivism is discretization, which they do not solve. 


\subsection{Related Literature}
\cite{olivetti_induction_2012} and \cite{olivetti_statistical_2014} looked into the problem of choosing a good accuracy test. 
They propose a new test they call an \emph{independence test}, and demonstrate by simulation that it has more power than other accuracy tests, and can deal with non-balanced data sets. 
We did not include this test in the battery we compared, but we note the following: 
(a) The independence test of \cite{olivetti_induction_2012} relies on a discrete test statistic. This means that in the cases that the accuracy test is called upon for discriminating populations, it will probably be underpowered compared to location tests. 
(b) In contrast with the underlying motivation of \cite{olivetti_induction_2012}'s independence test, we did not find that balancing the data folds is crucial for an accuracy test. 

\cite{golland_permutation_2005} study accuracy tests using simulation, neuroimaging data, genetic data, and analytically.
Their anaytic results formalize our intuition from Section~\ref{sec:introduction} on the effect of concentration of the accuracy statistic:
The finite Vapnik–Chervonenkis (VC) dimension requirement \citep[Sec 4.3]{golland_permutation_2003} prevents the permutation p-value from (asymptotically) concentrating. 
They also find that the power decresases with the level of discretization of the statistic. 
This is seen in their Figure~4, where the size of the test-set, $K$, governs the discretization. 
Since they permute features, and not labels, then all their permutation samples are balanced, and there is no issue of refolding. 

\cite{golland_permutation_2005} simulate the power of an accuracy test using a multivariate Gaussian mixture, with a parameter $p$ governing the separation between classes. 
Under their model 
$(x_i|y_i=1) \sim p \gauss{\mu_1,I}+ (1-p) \gauss{\mu_2,I}$ 
and 
$(x_i|y_i=-1) \sim (1-p) \gauss{\mu_1,I}+ p \gauss{\mu_2,I}$.
Varying $p$ interpolates between the null distribution $(p=0.5)$ and a location shift model $(p=0)$. 
We perform the same simulation as \cite{golland_permutation_2005}, after reparametrizing $p$ so that $p=0$ corresponds to the null model, and $p=23$ to be comparable to our other simulations.
We find that in this mixture class of models, like the location class of models, a location test has more power than an accuracy test (Figure~\ref{fig:golland}).
%TODO: why number of folds matters?


\begin{figure}[ht]
\centering
	  \includegraphics[width=0.8\linewidth]{"art/2016-08-08 07:33:05"}
	  \caption{\textbf{Mixture:} $\x_i = \chi_i \mu + \eta_i; \chi_i = \set{-1,1}$ and $\prob{\chi_i=1}=(1/2-p)^{\y^*_i}  (1/2+p)^{1-\y^*_i}$. $\mu$ is a $p$-vector with $3/\sqrt{p}$ in all coordinates.
	  The effect, $p$, is color and shape coded and varies over $0$ (red circle), $1/4$ (green tringle) and $1/2$ (blue square). }
	\label{fig:golland}
\end{figure}






%TODO: discuss \cite{ojala_permutation_2010}




\subsection{Reservations}
\label{sec:reservations}

Some reservations to the generality of our findings are in order. 
Firstly, not all accuracy tests are concerned with signal detection.
Indeed, it is possible that the purpose of the test is not to detect a difference between classes, but to actually test the performance of a particular classifier. 
Put differently- classification is harder than detection, so that we may be able to detect a difference between classes, but not be able to classify examples significantly better than chance. 
Examples of such problems include brain decoding for machine interfaces, and clinical diagnosis, where the presence of a medical condition is predicted from imaging data. \citep[e.g.][]{olivetti_induction_2012,wager_fmri-based_2013}

Secondly, it may be argued that accuracy tests permits the separation between classes in high dimensions, such as in \emph{reproducing kernel Hilbert spaces} (RKHS) by using non-linear predictors. 
This is a false argument-- accuracy test do not have any more flexibility that location tests. 
Indeed, it is possible to test for location in the same dimesion the classifier is learned. 
\citet{gretton_kernel_2012-1} is an example where the test for location is performed in the RKHS of the data.
It is also possible to test for the equality of two multivariate distributions without specifying any a-priori alternative \cite[e.g.][]{heller_consistent_2012}).
On the other hand, based on our reported neuroimaging example, and others, we find that a location test in the original feature space is indeed a simple and powerful approach to signal detection.






\subsection{Epilogue}
Given all the above, we find the popularity of accuracy tests quite puzzling. 
We believe this is due to a reversal of the inference cascade. 
Researchers first fit a classifier, and then ask if the classes are any different.
Were they to start by asking if classes are any different, and only then try to classify, then location tests would naturally arise as the preferred method. 
As put by \cite{ramdas_classification_2016}:
\begin{quote}
The recent popularity of machine learning has resulted in the extensive teaching and use
of prediction in theoretical and applied communities and the relative lack of awareness or
popularity of the topic of Neyman-Pearson style hypothesis testing in the computer science
and related ``data science'' communities.
\end{quote}
And more simply by Frank Harrell in the \textsf{CrossValidated} Q\&A site\footnote{\url{http://stats.stackexchange.com/questions/17408/how-to-assess-statistical-significance-of-the-accuracy-of-a-classifier}.}:
\begin{quote}
 ... your use of proportion classified correctly as your accuracy score. This is a discontinuous improper scoring rule that can be easily manipulated because it is arbitrary and insensitive.
\end{quote}






%\begin{acknowledgments}
%TODO: acknowledgemens
% isf 900/60, Jelle, Jesse B.A. Hemerik, Yakir Brechenko, Omer Shamir, Joshua Vogelstein, Gilles Blanchard, Jason Stein
%\end{acknowledgments}











\newpage
\bibliographystyle{abbrvnat}
\bibliography{Permuting_Accuracy.bib}

\appendix


\newpage

\section{Analysis pipeline}
\label{apx:analysis}

Here is the analysis pipeline of \cite{stelzer_statistical_2013} we for the auditory data in \cite{gilron_quantifying_2016}.
Denoting by 
$i=1,\dots,I$ the subject index, 
$v=1,\dots,V$ the voxel index, and 
$s = 1,\dots,S$ the permutation index. 
Since regions\footnote{\emph{searchlight} or \emph{sphere} in the MVPA parlance} are centered around a unique voxel, the voxel index $v$ also serves as a unique region index.
Algorithm~\ref{algo:statistic} computes a region-wise test statistic, which is compared to its permutation null distribution computed by Algorithm~\ref{algo:permutation}.


\begin{algorithm}[H]
\caption{Compute a group parametric map.}
\label{algo:statistic}

 \KwData{fMRI scans, and experimental design.}
 \KwResult{Brain map of group statistics: $\{\bar{T}_v\}_{v=1}^V$}
	 \For{$v \in 1,\dots,V$}{
		 \For{$i \in 1,\dots,I$}{
			 $T_{i,v} \leftarrow$ test statistic for subject $i$ in a region centered at $v$.
			 } 	  
	  	 $\bar{T}_{v} \leftarrow \frac{1}{I}\sum_{i=1}^I T_{i,v}$. 
 	 }
\end{algorithm}


\begin{algorithm}[H]
\caption{Compute a permutation p-value map.} 
\label{algo:permutation}

 \KwData{fMRI scans of $20$ subjects, experimental design.}
 \KwResult{Brain map of permutation p-values: $\{p_v\}_{v=1}^V$}
  \For{$s \in 1,\dots\,S$}{
    	    permute labels\;
    	    $\bar{T}_{v}^s \leftarrow$ parametric map 
  
  	}
\end{algorithm}

\newpage


\section{Simulation Details}
\label{apx:simulation_details}

The follwing details are common to all the reported simulations, unless stated otherwise in a figure's caption. 
The \textsf{R} code for the simulations can be found in [TODO].

Each simulation is based on $4,000$ replications. 
In each replication, we generate $n$ i.i.d. samples from a shift model $\x_i = \mu \y^*_i + \eta_i$.
Where $y^*_i=\set{0,1}$ is the class of subject $i$ in dummy coding. 
Recalling that $y_i=\set{-1,1}$ is the class in effect coding, then clearly $y_i=2 y^*_i-1$.
The noise is distributed as $\eta_i \sim \gaussp{p}{0,\Sigma}$. 
The sample size $n=40$. 
The dimension of the data is $p=23$. 
The covariance $\Sigma=I$. 
Effects, i.e. shifts $\mu$, are equal coordinate $p$-vectors with coordinates that vary over $\mu \in \set{0,1/4,1/2}$.

Having generated the data, we compute each of the test statistics in Table~\ref{tab:collected}.
For test statistics that require data folding, we used $8$ folds. 
We then compute a permutation p-value by permuting the class labels, and recomputing each test statistic. 
We perform $400$ such permutations. 
We then reject the $\mu_i=0$ null hypothesis if the permutation p-value is smaller than $0.05$.
The reported power is the proportion of replication where the permutation p-value falls below $0.05$.




\newpage

\section{Simulation Results}
\label{apx:simulations}



\begin{figure}[h]
\centering
\caption{\mycaption}	
\label{fig:n_folds}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 21:21:12"}
	  \caption{\textbf{2-fold} cross validation. Balanced folding.}  
	\label{fig:n_folds_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-29 07:18:24"}
	  \caption{\textbf{20-fold} cross validation. Balanced folding} 
	\label{fig:n_folds_2}
	\end{subfigure}
\end{figure}



\begin{figure}[h]
\centering
\caption{\mycaption}	
\label{fig:n_folds_unbalanced}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-05 09:37:35"}
	  \caption{\textbf{2-fold} cross validation. Unbalanced folding.} 
	\label{fig:n_folds_unbalanced_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-06 07:57:22"}
	  \caption{\textbf{20-fold} cross validation. Unbalanced folding.} 
	\label{fig:n_folds_unbalanced_2}
	\end{subfigure}
\end{figure}



\begin{figure}[h]
\centering
\caption{\mycaption}	
%\label{fig:simulation_1}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-30 10:33:05"}
	  \caption{\textbf{Scale Change:} $\x_i =  \eta_i * \mu^ {\y^*_i}$ so that the effect are a scale change.}  
	\label{fig:scale_change}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-04 19:32:17"}
	  \caption{\textbf{Heavytailed:} $\eta_i$ is not $p$-variate Gaussian, but rather $p$-variate t, with $df=3$ .  } 
	\label{fig:t_null}
	\end{subfigure}
\end{figure}




\begin{figure}[h]
\centering
\caption{\mycaption}	
\label{fig:large_sample}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05zoom"}
	  \caption{\textbf{Low-Dimension:} False positive rates for $n=40$.} 
	\label{fig:large_sample_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{"art/2016-08-04 13:59:33zoom"}
	  \caption{\textbf{High-Dimension:} False positive rates for $n=400$.} 
	\label{fig:large_sample_2}
	\end{subfigure}
\end{figure}




\begin{figure}[h]
\centering
\caption{\mycaption}	
	\begin{subfigure}{.4\textwidth}
	  \centering
	  \missingfigure[figwidth=6cm]{TODO:figure}
%	  \includegraphics[width=1\linewidth]{"art/2016-07-27 11:42:05"}
	  \caption{\textbf{High-Dimension, local alternative:} $n=400$, $\mu \in \frac{\sqrt{40}}{\sqrt{400}} \times \set{0,1/4,1/2}.$} 
	\label{fig:large_sample_3}
	\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{"art/2016-08-07 20:11:46"}
		  \caption{\textbf{AR(1) dependence:} $\Sigma_{k,l}=\rho^{|k-l|}; \rho=0.8$. } 
		\label{fig:ar_1}
	\end{subfigure}
\end{figure}




\end{document}