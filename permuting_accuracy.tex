%% PNAStwoS.tex
%% Sample file to use for PNAS articles prepared in LaTeX
%% For two column PNAS articles
%% Version1: Apr 15, 2008
%% Version2: Oct 04, 2013

%% BASIC CLASS FILE
\documentclass{pnastwo}

%% ADDITIONAL OPTIONAL STYLE FILES Font specification

%\usepackage{pnastwoF}
\usepackage{todonotes}



%% OPTIONAL MACRO DEFINITIONS
%\def\s{\sigma}
\newcommand{\set}[1]{\{ #1 \}} % A set
\newcommand{\indicator}[1]{I_{\set{#1}}} % The indicator function.
\newcommand{\reals}{\mathbb{R}} % the set of real numbers
\newcommand{\features}{x} % The feature space
\newcommand{\outcomes}{y} % The feature space
\newcommand{\featureS}{\mathcal{X}} % The feature space
\newcommand{\outcomeS}{\mathcal{Y}} % The feature space
\newcommand{\hyp}{f} % A hypothesis
\newcommand{\hypEstim}{\hat{\hyp}} % A hypothesis
\newcommand{\hypclass}{\mathcal{F}}
\newcommand{\expect}[1]{\mathbf{E}\left[ #1 \right]} % The expectation operator
\newcommand{\acc}{T^{acc}} 
\newcommand{\dominant}{\hat{p}_{max}}


%%%%%%%%%%%%
%% For PNAS Only:
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}
%\setcounter{page}{2687} %Set page number here if desired
%%%%%%%%%%%%

\begin{document}

%Sketch: 
%- The conservativeness of the test. 
%- What does it detect?
%- Is it remedied in large samples?
%- Is conservativeness always there?



\title{On permutation testing of classification accuracy for signal detection}

\author{Jonathan Rosenblatt \affil{1}{Ben-Gurion University of the Negev, Beer-Sheva, Israel}}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

%%%Newly updated.
%%% If significance statement need, then can use the below command otherwise just delete it.
%\significancetext{RJSM and ACAC developed the concept of the study. RJSM conducted the analysis, data interpretation and drafted the manuscript. AGB contributed to the development of the statistical methods, data interpretation and drafting of the manuscript.}



\maketitle

\begin{article}

\begin{abstract}
[TODO]
\end{abstract}



\keywords{XXX | YYY | ZZZ }

%\abbreviations{TODO}

\section{Introduction}

A common workflow in genetics or neuroimaging consists of fitting a classifier, and estimating its predictive accuracy using cross validation. 
Given that the cross validated accuracy is a random quantity, it is then common to test if the cross validated accuracy is significantly better than chance using a permutation test.  
Genetic examples include [TODO: elaborate]\cite{jiang_calculating_2008,radmacher_paradigm_2002-1}.
Neuroscience examples include the recently popularized \emph{multivariate pattern analysis} (MVPA) \cite{kriegeskorte_information-based_2006,varoquaux_assessing_2016,golland_permutation_2003}.

To fix ideas, we will adhere to a Neuroscientific example: 
In \cite{gilron_quantifying_2016}, the authors seek to detect auditory brain regions which distinguish between vocal and non-vocal stimuli. 
According to the MVPA analysis workflow, the localization problem is cast as a supervised learning problem: if the type of the stimulus can be predicted from the spatial activation pattern, significantly better than chance, then a region is declared to encode vocal/non-vocal information. 

This same signal detection task can be also approached as a two-group multivariate test:
Inferring that a region encodes vocal/non-vocal information, is essentially inferring that the spatial distribution of brain activations is different given a vocal/non-vocal stimulus. 
A practitioner may then call upon a two-group Hotelling test \cite{fujikoshi_multivariate_2011}. 
If the size of the brain region is too large compared to the number of observations, so that the spatial covariance cannot be fully estimated, then a high dimensional version of the Hotelling test can be called upon, such as  \cite{srivastava_testing_2013}.

At this point, it becomes unclear which is the preferred avenue. 
This question was indeed the subject of \cite{ramdas_classification_2016}, which compared the Hotelling test to the classification accuracy of \emph{Fisher's linear discriminant analysis} (LDA) \cite{hastie_elements_2003-1}. 
Their asymptotic analysis concluded that methods are equivalent with respect to the order of convergence to a consistent test. The same authors did find that the methods differ in the constants governing the convergence.

Those constants, are the focus of this contribution. In particular, which signal detection method is to be preferred in finite samples? 
Our conclusion is quite simple: signal detection using multivariate tests almost always has more power than using the accuracy test statistic.

The main argument for our statement rests upon the observation that with typical sample sizes, the accuracy test statistic is highly discrete. 
Discrete test statistics are known to be conservative, since they cannot exhaust the permissible type $I$ error rates. 
For the accuracy test statistic, the degree of discretization is governed by the number of samples. 
in our running example \cite{gilron_quantifying_2016}, the classification is performed based on $40$ trials, so that the test statistic may assume only $40$ possible values. 
This number of examples is not unusual if considering this is the number of subject in a genetic study, or the number of trial repeats in an fMRI brain scan. 

The discretization effect is further augmented if the test statistic is highly concentrated. 
For some intuition consider the usage of the train-accuracy test statistic (i.e., not cross validated).
Because the testing problem is high dimensional, the observed train accuracy will be high. But the same will occur in every permutation. 
The permutation accuracies will tend to concentrate around $1$ with a granularity depending on the sample size. 
In a very high dimensional setup, the accuracy will be $1$ for the observed data, but also for any permuted data set. 
The permutation p-value will thus be $1$ for almost all samples, and the null will never be rejected. 

In the following, we look into the conservativeness of signal detection using an the classification accuracy. 
We start by establishing a best practice for permutation testing using the accuracy test statistic, 

We discuss the problem characteristics that govern the magnitude of the conservativeness, and try to offer an intuition to the scope of the observation that a multivariate test should always be preferred over a classification approach. 


\section{Problem setup}
Adhering to our neuroscientific example, we now formalize terminology and notation. 
Let $\outcomes \in \outcomeS$ be a class encoding. In our vocal/non-vocal example, using effect coding, we have $\outcomeS=\set{-1,1}$.
Let $\features \in \featureS$ be a $p$ dimensional feature vector. 
In our vocal/non-vocal example $p$ is governed by the number of voxels in a regions, which is the number of voxels in each brain region tested. We thus have $\featureS=\reals^{27}$. 

Given $n$ pairs of $(\features_i,\outcomes_i)$, typically assumed i.i.d., the \emph{testing} approach to localization amounts to testing whether $\features|\outcomes=1$ has the the same distribution as $\features|\outcomes=-1$.
I.e., the multivariate voxel activation pattern has the same distribution when given a vocal stimulus, as when given a non-vocal stimulus. 
The \emph{classification} approach to the localization problem amounts to learning a predictive model $\hypEstim(\features)$ from some assumed model class $\hypEstim \in \hypclass$. 
The prediction accuracy, denoted $\acc_{\hypEstim}$, is defined as the probability of a given classifier $\hypEstim$ of making a correct prediction $\acc_{\hypEstim}:=\expect{\indicator{\hypEstim(x)=y}}$ when given a new, randomly drawn data point, ($\features,\outcomes$).





\section{How to perform the permutation test?}

To discuss the power of a permutation test using the accuracy test statistic, we need to establish some best practices for such a test. 
The design of the test consists of the following choices: 
\begin{description}
\item [What test statistic?] 
\item [Cross validated or not?] Is the statistic cross validated or not?
\item [Refolding?] For a K-fold cross validated test statistic: is the data refolded in each permutation? 
\item [Permute labels of features?] Should the $\outcomes$ be permuted or should the $\features$?
\item [Balanced folding?] For a K-fold cross validated test statistic: is the data folding balanced? (a.k.a. stratified).
\item [How many folds?] 

We will now address these questions while bearing in mind that unlike the typical supervised learning setup, we are not interested in an unbiased estimate of the prediction error, but rather in the mere detection of a difference between two groups, leading to a better-than-chance accuracy. 


\paragraph{What test statistic?}
Given a predictor $\hypEstim$, a natural test statistic is some estimate of its accuracy $\acc_{\hypEstim}$.
Then again, very low accuracies, even $0$, is evidence that the classes are separated, and we only need to invert the predictions. We can thus consider some estimate of $|\acc_{\hypEstim}-0.5|$ as the test statistic.
This, however, implies that if the classes are identical, random guessing has a $0.5$ accuracy. This is not true if the classes are not balanced. 
The chance level in which case is the prevalence of the dominant class, we denote by $\dominant$.
This suggests the following test statistic $|\acc_{\hypEstim}-\dominant|$.
Since we will latter be aggregating these statistic over random data foldings, where the dominant class may have varying frequencies, it seems appropriate to standardize the scale of this statistic. 
We thus also consider a z-scored accuracy: $|\acc_{\hypEstim}-\dominant|/\sqrt{\dominant(1-\dominant)}$.


\paragraph{Cross validated or not?}
Were we interested in an unbiased estimator of the prediction error, there is no question that some validation is in order. 
Since we are merely interested in detecting a difference between groups, a biased error estimate is not an issue provided that it is consistent over all permutations. 
The underlying intuition is that if the exact same computation is performed over all permutations, then a permutation test will be ``fair'', i.e., will not inflate the false positive rate. 
We will thus be considering both cross validated accuracies, and train accuracies as our test statistics. 


\paragraph{Refolding?}
The standard practice in neuroimaging is to refold the data after each permutation [TODO:ref].
This is imperative if permuting labels while aiming at balanced data folds. 
This is not, however, imperative in general. 
In this work, we will adhere however to the standard practice of refolding the data within each permutation.


\paragraph{Permute labels of features?}
While seemingly identical, the compounding of permutations with data foldings renders these two approaches distinct. 
As an example, consider balanced (stratified) K-fold cross validation where the initial data folding is balanced. 
After a label permutation, the folds will probably not be balanced, and will thus have to be refolded. 
If the features are permuted, then the labels conserve their original fold assignments, and the data need not be refolded. 


\paragraph{Balanced folding?}
The standard practice when cross validating is to constrain the data folds to be balanced (i.e. stratified)[TODO: ref].
This is well justified when aiming at unbiased accuracy estimation. 
This also simplifies matter when aiming at signal detection, as can be seen from the above discussion of the appropriate test statistic. 
Then again, it may complicate matters, as can be seen from the above discussion on label versus feature permutation. 
In general , it is not imperative in general, and we will indeed be comparing the effect of balanced foldings versus unbalanced. 


\paragraph{How many folds?}
Different authors suggest different rules for the number of folds. 
We will be varying the number of folds, since it will affect the concentration of the estimated accuracy, which will have a crucial effect on the conservativeness of the permutation test. 
Our intuition suggests that since more folds imply a less concentrated estimate, then leave-one-out should be the less conservative, and 2-fold should be the most conservative. 

\end{description}



\section{Controlling the False Positive Rate}
In the first of our battery of simulations we verify that various test statistics and permutation schemes control the type I error. 
Figure~\ref{fig:simulation_1} demonstrates that this is indeed the case. 
All our candidate tests control the type I error, with varying degrees of conservativeness. 
In particular:
(a) if the folds are balanced or not,
(b) if the labels are permuted or the features, 
(c) if the test statistic is varied, 
(d) if the regularization level of the support vector machine classifier (SVM) is varied,
(e) if the number of folds is varied.



\begin{figure}
\centering
%\includegraphics[width=0.7\linewidth]{}
\missingfigure[figwidth=6cm]{TODO}
\caption{
	\label{fig:simulation_1}
	TODO}
\end{figure}





\section{Power}
fff





\section{Discussion}
% Not all problems are signal detection.
% Implementation difficulty with discrete test statistics.


fff



%\begin{acknowledgments}
%TODO
%\end{acknowledgments}













\bibliography{Permuting_Accuracy.bib}
\bibliographystyle{PNAS.bst}
%\bibliographystyle{abbvrnat}

\end{article}





\end{document}


